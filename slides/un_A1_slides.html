<!DOCTYPE html>
<html lang="en"><head>
<script src="un_A1_files/libs/clipboard/clipboard.min.js"></script>
<script src="un_A1_files/libs/quarto-html/tabby.min.js"></script>
<script src="un_A1_files/libs/quarto-html/popper.min.js"></script>
<script src="un_A1_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="un_A1_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="un_A1_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="un_A1_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.2.335">

  <meta name="author" content="Tommaso Rigon">
  <title>A-B-C</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="un_A1_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="un_A1_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="un_A1_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
  <link href="un_A1_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="un_A1_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="un_A1_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="un_A1_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">A-B-C</h1>
  <p class="subtitle">Data Mining - CdL CLAMSES</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<span class="orange">Tommaso Rigon</span> 
</div>
        <p class="quarto-title-affiliation">
            <em>Università degli Studi di Milano-Bicocca</em>
          </p>
    </div>
</div>

</section>
<section id="about-this-unit" class="slide level2">
<h2>About this unit</h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="img/ABC.png"> <em>“Everything should be made as simple as possible, but not simpler”</em> Attributed to Albert Einstein</p>
</div><div class="column" style="width:60%;">
<ul>
<li><p>In this unit we will cover the following <span class="orange">topics</span>:</p>
<ul>
<li>Linear models and the modelling process</li>
<li>Cholesky factorization</li>
<li>Orthogonalization and QR decomposition</li>
<li>Iterative methods</li>
</ul></li>
<li><p>The <span class="blue">computational aspects</span> of linear models will be novel to most of you…</p></li>
<li><p>… but you should be already <span class="orange">very familiar</span> with linear models!</p></li>
<li><p>If do not remember much about them, use this first week of lectures to catch up (or study) from the material of previous courses.</p></li>
</ul>
<!-- -   A [short introduction]{.blue} to the topic is also offered in -->
<!--     Azzalini & Scarpa (2011), Chapter 2 and Appendix A.3. -->
</div>
</div>
</section>
<section>
<section id="old-friends-linear-models" class="title-slide slide level1 center">
<h1>Old friends: linear models</h1>

</section>
<section id="car-data" class="slide level2">
<h2>Car data</h2>
<ul>
<li>We consider data for <span class="math inline">n = 203</span> models of cars in circulation in 1985 in the USA.</li>
<li>We want to identify a relationship that allows to <span class="blue">predict</span> the distance covered per unit of fuel, as a function of the vehicle characteristics.</li>
<li>We consider the following <span class="orange">continuous variables</span>:
<ul>
<li>The city distance per unit of fuel (km/L, <code>city.distance</code>)</li>
<li>The engine size (L, <code>engine.size</code>)</li>
<li>The number of cylinders (<code>n.cylinders</code>)</li>
<li>The curb weight (kg, <code>curb.weight</code>)</li>
</ul></li>
<li>We also considered the <span class="orange">categorical variable</span> fuel type (gasoline or diesel, <code>fuel</code>).</li>
</ul>
</section>
<section id="car-data-diesel-or-gas" class="slide level2">
<h2>Car data (<span class="blue">diesel</span> or <span class="orange">gas</span>)</h2>
<div class="cell" data-hash="un_A1_cache/revealjs/unnamed-chunk-1_3b2f496c61142123593d1b884f17fc20">
<div class="cell-output-display">
<p><img data-src="un_A1_files/figure-revealjs/unnamed-chunk-1-1.png" width="3000"></p>
</div>
</div>
</section>
<section id="linear-regression" class="slide level2">
<h2>Linear regression</h2>
<ul>
<li><p>At the moment, let us focus on <code>city.distance</code> (<span class="math inline">y</span>), <code>engine.size</code> (<span class="math inline">x</span>) and <code>fuel</code> (<span class="math inline">z</span>).</p></li>
<li><p>The simplest model we can come up with is a <span class="orange">linear regression</span> line: <span class="math display">
y_i = \beta_1 + \beta_2 x_i + \epsilon_i, \qquad i=1,\dots,n,
</span> where the “errors” <span class="math inline">\epsilon_i</span> are iid random variables, having zero mean and variance <span class="math inline">\sigma^2</span>.</p></li>
</ul>
<div class="fragment">
<ul>
<li><p>We are looking for an estimate for the <span class="blue">unknown regression parameters</span> <span class="math inline">\beta_0</span> and <span class="math inline">\beta_1</span>.</p></li>
<li><p>Such an estimate could be obtained by ordinary least squares (OLS)…</p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li><p>… but the next plot clearly suggests that the relationship between <code>city.distance</code> and <code>engine.size</code> is <span class="orange">not</span> well approximated by a <span class="orange">linear</span> function.</p></li>
<li><p>… and also that <code>fuel</code> has an non-negligible effect on the response.</p></li>
</ul>
</div>
</section>
<section id="scatterplot-of-the-data" class="slide level2">
<h2>Scatterplot of the data</h2>
<div class="cell" data-hash="un_A1_cache/revealjs/unnamed-chunk-2_726a4f0a79fbf488abcdf2fb42c0441b">
<div class="cell-output-display">
<p><img data-src="un_A1_files/figure-revealjs/unnamed-chunk-2-1.png" width="3000"></p>
</div>
</div>
</section>
<section id="regression-models" class="slide level2">
<h2>Regression models</h2>
<div>
<ul>
<li class="fragment"><p>A more <span class="orange">general formulation</span> for modeling the relationship between a vector of covariates <span class="math inline">(x_{i1},\dots,x_{ip})^T \in \mathbb{R}^p</span> and a response <span class="math inline">y \in \mathbb{R}</span> is <span class="math display">
y_i = f(x_{i1},\dots,x_{ip}; \beta) + \epsilon_i, \qquad i=1,\dots,n.
</span></p></li>
<li class="fragment"><p>To estimate the unknown parameters <span class="math inline">\beta</span>, a possibility is to rely on the <span class="blue">least squares criterion</span>: we seek the <span class="orange">minimum</span> of the objective function <span class="math display">
D(\beta) = \sum_{i=1}^n\{y_i - f(x_{i1},\dots, x_{ip}; \beta)\}^2,
</span> using <span class="math inline">n</span> pairs of observations <span class="math inline">(x_{i1},\dots,x_{ip})^T</span> and <span class="math inline">y_i</span>, for <span class="math inline">i = 1,\dots,n</span>.</p></li>
<li class="fragment"><p>The solution to this minimization problem is denoted by <span class="math inline">\hat{\beta}</span>.</p></li>
<li class="fragment"><p>The <span class="blue">predicted values</span> <span class="math inline">\hat{y}_i</span> are then obtained as <span class="math inline">\hat{y}_i = f(x_{i1},\dots,x_{ip}; \hat{\beta})</span>, for <span class="math inline">i=1,\dots,n.</span></p></li>
</ul>
</div>
</section>
<section id="linear-models" class="slide level2">
<h2>Linear models</h2>
<ul>
<li><p>We can more flexibly model <code>city.distance</code> (<span class="math inline">y</span>), <code>engine.size</code> (<span class="math inline">x</span>) and <code>fuel</code> (<span class="math inline">z</span>).</p></li>
<li><p>For instance, we could consider a <span class="orange">polynomial term</span> combined with a <span class="blue">dummy variable</span> <span class="math display">
f(x, z; \beta) = \beta_1 + \beta_2 x + \beta_3 x^2 + \beta_4 x^3 + \beta_5 I(z = \texttt{gas}).
</span></p></li>
<li><p><span class="orange">Remark</span>. This model is <span class="blue">linear in the parameters</span>, but it can capture non-linear patterns!</p></li>
</ul>
<div class="fragment">
<div class="callout callout-note no-icon callout-captioned callout-style-simple">
<div class="callout-body">
<div class="callout-caption">
<p><strong>Definition (Linear model)</strong></p>
</div>
<div class="callout-content">
<p>In a <span class="blue">linear model</span> the response variable <span class="math inline">y</span> is related to the covariates through the function<span class="math display">
    f(x_1,\dots,x_p; \beta) = \beta_1 x_1 + \cdots + \beta_p x_p = \tilde{\bm{x}}^T\beta,
    </span> where <span class="math inline">\tilde{\bm{x}} = (x_1, \dots,x_p)^T</span> is a vector of <span class="orange">covariates</span> and <span class="math inline">\beta = (\beta_1,\dots,\beta_p)^T</span> is the corresponding vector of <span class="orange">coefficients</span>.</p>
</div>
</div>
</div>
</div>
</section>
<section id="matrix-notation" class="slide level2">
<h2>Matrix notation</h2>
<div>
<ul>
<li class="fragment"><p>The <span class="orange">response values</span> are collected in the vector <span class="math inline">\bm{y} = (y_1,\dots,y_n)^T</span>.</p></li>
<li class="fragment"><p>The <span class="blue">design matrix</span> is a <span class="math inline">n \times p</span> matrix, comprising the covariate’s values, defined by <span class="math display">
\bm{X} =
\begin{bmatrix}
x_{11} &amp; \cdots &amp; x_{1p}\\
\vdots &amp; \ddots &amp; \vdots \\
x_{n1} &amp; \cdots &amp; x_{np}
\end{bmatrix}.
</span></p></li>
<li class="fragment"><p>The linear model can be written using the <span class="orange">compact notation</span>: <span class="math display">
\bm{y} = \bm{X}\beta + \bm{\epsilon},
</span> where <span class="math inline">\bm{\epsilon} = (\epsilon_1,\dots,\epsilon_n)^T</span> is a vector of iid error terms with zero mean and variance <span class="math inline">\sigma^2</span>.</p></li>
<li class="fragment"><p>The <span class="math inline">j</span>th variable (column) is denoted with <span class="math inline">\bm{x}_j</span>, whereas the <span class="math inline">i</span>th observation (row) is <span class="math inline">\tilde{\bm{x}}_i</span>: <span class="math display">
\bm{X} = (\bm{x}_1,\dots,\bm{x}_p) = (\tilde{\bm{x}}_1, \dots,\tilde{\bm{x}}_n)^T.
</span></p></li>
</ul>
</div>
</section>
<section id="linear-regression-estimation-i" class="slide level2">
<h2>Linear regression: estimation I</h2>
<ul>
<li>The optimal set of coefficients <span class="math inline">\hat{\beta}</span> is the minimizer of the least squared criterion <span class="math display">
D(\beta) = (\bm{y} - \bm{X}\beta)^T(\bm{y} - \bm{X}\beta) = ||\bm{y} - \bm{X}\beta||^2,
</span> known also as <span class="orange">residual sum of squares (RSS)</span>, where <span class="math display">
||\bm{y}|| = \sqrt{y_1^2 + \cdots + y_n^2}.</span></li>
</ul>
<div class="callout callout-note no-icon callout-captioned callout-style-simple">
<div class="callout-body">
<div class="callout-caption">
<p><strong>Least square estimate (OLS)</strong></p>
</div>
<div class="callout-content">
<p>If the design matrix has <span class="blue">full rank</span>, that is if <span class="math inline">\text{rk}(\bm{X}^T\bm{X}) = p</span>, then the <span class="orange">least square estimate</span> has an explicit solution: <span class="math display">
    \hat{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T \bm{y}.
    </span></p>
</div>
</div>
</div>
</section>
<section id="linear-regression-estimation-ii" class="slide level2">
<h2>Linear regression: estimation II</h2>
<div>
<ul>
<li class="fragment"><p>Consequently, the predicted values are <span class="math display">
\hat{\bm{y}} = \bm{X}\hat{\beta} = \bm{H}\bm{y}, \qquad \bm{H} = \bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T.
</span></p></li>
<li class="fragment"><p><span class="math inline">\bm{H}</span> is a <span class="math inline">n \times n</span> <span class="orange">projection matrix</span> matrix sometimes called <span class="blue">hat matrix</span>.</p></li>
<li class="fragment"><p>It can be shown that <span class="math inline">\text{tr}(\bm{H}) = \text{rk}(\bm{H}) = p</span>. Moreover, it holds <span class="math inline">\bm{H} = \bm{H}^T</span> and <span class="math inline">\bm{H}^2 = \bm{H}</span>.</p></li>
<li class="fragment"><p>The quantity <span class="math inline">D(\hat{\beta})</span> is the <span class="blue">deviance</span>, which is equal to <span class="math display">
D(\hat{\beta}) = ||\bm{y} - \hat{\bm{y}}||^2 = \bm{y}^T(I_n - \bm{H})\bm{y}.
</span></p></li>
<li class="fragment"><p>A typical estimate for the <span class="orange">residual variance</span> <span class="math inline">\sigma^2</span> is then given by <span class="math display">
s^2 = \frac{D(\hat{\beta})}{n - p} = \frac{1}{n-p}\sum_{i=1}^n(y_i - \tilde{\bm{x}}_i^T\hat{\beta})^2.
</span></p></li>
</ul>
</div>
</section>
<section id="linear-regression-inference" class="slide level2">
<h2>Linear regression: inference</h2>
<div>
<ul>
<li class="fragment"><p>Let us additionally assume that the errors follow a Gaussian distribution: <span class="math inline">\epsilon_i \overset{\text{iid}}{\sim} N(0, \sigma^2)</span>.</p></li>
<li class="fragment"><p>This implies that the distribution of the estimator <span class="math inline">\hat{\beta}</span> is <span class="math display">
\hat{\beta} \sim N_p(\beta, \sigma^2 (\bm{X}^T\bm{X})^{-1}).
</span></p></li>
<li class="fragment"><p>Hence, the estimator <span class="math inline">\hat{\beta}</span> is <span class="orange">unbiased</span> and its <span class="blue">variance</span> can be estimated by <span class="math display">
\widehat{\text{var}}(\hat{\beta}) = s^2 (\bm{X}^T\bm{X})^{-1}.
</span></p></li>
<li class="fragment"><p>The <span class="orange">standard errors</span> of the components of beta correspond to the square root of the diagonal of the above covariance matrix.</p></li>
<li class="fragment"><p>Confidence interval and Wald’s tests can be obtained through classical inferential theory.</p></li>
</ul>
</div>
</section>
<section id="car-data-a-preliminary-model" class="slide level2">
<h2>Car data, a preliminary model</h2>
<div>
<ul>
<li class="fragment"><p>A first model to predict <code>city.distance</code> (<span class="math inline">y</span>) via <code>engine.size</code> (<span class="math inline">x</span>) and <code>fuel</code> (<span class="math inline">z</span>) is <span class="math display">
  f(x, z; \beta) = \beta_1 + \beta_2 x + \beta_3 x^2 + \beta_4 x^3 + \beta_5 I(z = \texttt{gas}).
  </span></p></li>
<li class="fragment"><p>By looking at the plot of the data, it is plausible that we need a <span class="orange">polynomial</span> of degree <span class="math inline">3</span> or <span class="math inline">4</span></p></li>
<li class="fragment"><p>It is also clear from the plot that <code>fuel</code> is a relevant variable. Categorical variable should be <span class="orange">encoded</span> using <span class="blue">indicator variables</span>.</p></li>
<li class="fragment"><p>To evaluate the goodness of fit, we can calculate the <span class="orange">coefficient of determination</span>: <span class="math display">
R^2 = 1 - \frac{\text{(``Residual deviance'')}}{\text{(``Total deviance'')}} = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n(y_i - \bar{y})^2}.
</span></p></li>
</ul>
</div>
</section>
<section id="a-first-model-estimated-coefficients" class="slide level2">
<h2>A first model: estimated coefficients</h2>
<ul>
<li>We obtain the following <span class="orange">summary</span> for the regression coefficients <span class="math inline">\hat{\beta}</span>.</li>
</ul>
<div class="cell" data-hash="un_A1_cache/revealjs/unnamed-chunk-3_21c65b81ccc6d1134f6a4f4de54e1000">
<div class="cell-output-display">
<table style="width:100%;">
<colgroup>
<col style="width: 23%">
<col style="width: 12%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 10%">
<col style="width: 12%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
<th style="text-align: right;">conf.low</th>
<th style="text-align: right;">conf.high</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">28.045</td>
<td style="text-align: right;">3.076</td>
<td style="text-align: right;">9.119</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">21.980</td>
<td style="text-align: right;">34.110</td>
</tr>
<tr class="even">
<td style="text-align: left;">engine.size</td>
<td style="text-align: right;">-10.980</td>
<td style="text-align: right;">3.531</td>
<td style="text-align: right;">-3.109</td>
<td style="text-align: right;">0.002</td>
<td style="text-align: right;">-17.943</td>
<td style="text-align: right;">-4.016</td>
</tr>
<tr class="odd">
<td style="text-align: left;">I(engine.size^2)</td>
<td style="text-align: right;">2.098</td>
<td style="text-align: right;">1.271</td>
<td style="text-align: right;">1.651</td>
<td style="text-align: right;">0.100</td>
<td style="text-align: right;">-0.409</td>
<td style="text-align: right;">4.604</td>
</tr>
<tr class="even">
<td style="text-align: left;">I(engine.size^3)</td>
<td style="text-align: right;">-0.131</td>
<td style="text-align: right;">0.139</td>
<td style="text-align: right;">-0.939</td>
<td style="text-align: right;">0.349</td>
<td style="text-align: right;">-0.406</td>
<td style="text-align: right;">0.144</td>
</tr>
<tr class="odd">
<td style="text-align: left;">fuelgas</td>
<td style="text-align: right;">-3.214</td>
<td style="text-align: right;">0.427</td>
<td style="text-align: right;">-7.523</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">-4.057</td>
<td style="text-align: right;">-2.372</td>
</tr>
</tbody>
</table>
</div>
</div>
<ul>
<li>The coefficient <span class="math inline">R^2</span> and <span class="math inline">s</span> are estimated as follows:</li>
</ul>
<div class="cell" data-hash="un_A1_cache/revealjs/unnamed-chunk-4_2d932bd0b2b42c1063fcc10c11347040">
<div class="cell-output-display">
<table>
<thead>
<tr class="header">
<th style="text-align: right;">r.squared</th>
<th style="text-align: right;">sigma</th>
<th style="text-align: right;">deviance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.5973454</td>
<td style="text-align: right;">1.790362</td>
<td style="text-align: right;">634.6687</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="a-first-model-fitted-values" class="slide level2">
<h2>A first model: fitted values</h2>
<div class="cell" data-hash="un_A1_cache/revealjs/unnamed-chunk-5_73ff6b250013fb7c5e07d6819797a8f5">
<div class="cell-output-display">
<p><img data-src="un_A1_files/figure-revealjs/unnamed-chunk-5-1.png" width="3000"></p>
</div>
</div>
</section>
<section id="a-first-model-graphical-diagnostics" class="slide level2">
<h2>A first model: graphical diagnostics</h2>
<div class="cell" data-hash="un_A1_cache/revealjs/unnamed-chunk-6_e265984980255ab314cda4def7bff5fc">
<div class="cell-output-display">
<p><img data-src="un_A1_files/figure-revealjs/unnamed-chunk-6-1.png" width="3000"></p>
</div>
</div>
</section>
<section id="comments-and-criticisms" class="slide level2">
<h2>Comments and criticisms</h2>
<div>
<ul>
<li class="fragment"><p>The overall fit <span class="blue">seems satisfactory</span> at first glance, especially if we aim at predicting the urban distance of cars when average engine size (i.e., between <span class="math inline">1.5L</span> and <span class="math inline">3L</span>).</p></li>
<li class="fragment"><p>However, the plot of the <span class="orange">residuals</span> <span class="math inline">r_i = y_i - \hat{y}_i</span> suggests that there the homoschedasticity assumption, i.e.&nbsp;<span class="math inline">\text{var}(\epsilon_i) = \sigma^2</span>, might be violated.</p></li>
<li class="fragment"><p>Also, this model in not suitable for <span class="orange">extrapolation</span>. Indeed:</p></li>
<li class="fragment"><p>It has no grounding in physics or engineering, which leads to difficulties in the interpretation of the trend and/or paradoxical situations.</p></li>
<li class="fragment"><p>For example, the curve of the set of gasoline cars shows a local minimum around <span class="math inline">4.6 L</span> and then rises again!</p></li>
</ul>
</div>
</section>
<section id="variable-transformation" class="slide level2">
<h2>Variable transformation</h2>
<ul>
<li><p>A major advantage of linear models is that they can exploit non-linear relationship via <span class="blue">variable transformations</span>.</p></li>
<li><p>This gives the statistician a lot of modelling flexibility, for instance:</p></li>
</ul>
<p><span class="math display">
\log{y_i} = \beta_1 + \beta_2 \log{x_i} + \beta_3 I(z_i = \texttt{gas}) + \epsilon_i, \qquad i=1,\dots,n.
</span></p>
<div class="fragment">
<ul>
<li>This specification is <span class="orange">linear in the parameters</span>, it fixes the domain issues, and imposes a monotone relationship between engine size and consumption.</li>
</ul>
<div class="cell" data-hash="un_A1_cache/revealjs/unnamed-chunk-7_e785d5596eb06f10cdd5c6d7fc36fd24">
<div class="cell-output-display">
<table style="width:100%;">
<colgroup>
<col style="width: 26%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 10%">
<col style="width: 11%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
<th style="text-align: right;">conf.low</th>
<th style="text-align: right;">conf.high</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">3.060</td>
<td style="text-align: right;">0.047</td>
<td style="text-align: right;">64.865</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">2.967</td>
<td style="text-align: right;">3.153</td>
</tr>
<tr class="even">
<td style="text-align: left;">I(log(engine.size))</td>
<td style="text-align: right;">-0.682</td>
<td style="text-align: right;">0.040</td>
<td style="text-align: right;">-17.129</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">-0.760</td>
<td style="text-align: right;">-0.603</td>
</tr>
<tr class="odd">
<td style="text-align: left;">fuelgas</td>
<td style="text-align: right;">-0.278</td>
<td style="text-align: right;">0.038</td>
<td style="text-align: right;">-7.344</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">-0.353</td>
<td style="text-align: right;">-0.203</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="second-model-fitted-values" class="slide level2">
<h2>Second model: fitted values</h2>
<div class="cell" data-hash="un_A1_cache/revealjs/unnamed-chunk-8_6c77cea7d2c362b49059b5eb447ce54e">
<div class="cell-output-display">
<p><img data-src="un_A1_files/figure-revealjs/unnamed-chunk-8-1.png" width="3000"></p>
</div>
</div>
</section>
<section id="second-model-graphical-diagnostics" class="slide level2">
<h2>Second model: graphical diagnostics</h2>
<div class="cell" data-hash="un_A1_cache/revealjs/unnamed-chunk-9_21d0707b5935c472a7bb75af0aefc1d4">
<div class="cell-output-display">
<p><img data-src="un_A1_files/figure-revealjs/unnamed-chunk-9-1.png" width="3000"></p>
</div>
</div>
</section>
<section id="comments-and-criticisms-1" class="slide level2">
<h2>Comments and criticisms</h2>
<ul>
<li>The <span class="blue">goodness of fit</span> indices are the following:</li>
</ul>
<div class="cell" data-hash="un_A1_cache/revealjs/unnamed-chunk-10_b7f4b9d92665f73fb788030de6eefbea">
<div class="cell-output-display">
<table>
<thead>
<tr class="header">
<th style="text-align: right;">r.squared.original</th>
<th style="text-align: right;">r.squared</th>
<th style="text-align: right;">sigma</th>
<th style="text-align: right;">deviance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.5847555</td>
<td style="text-align: right;">0.6196093</td>
<td style="text-align: right;">0.1600278</td>
<td style="text-align: right;">5.121777</td>
</tr>
</tbody>
</table>
</div>
</div>
<ul>
<li><span class="blue">Apple</span> and <span class="orange">oranges</span>: the <span class="math inline">R^2</span> must be computed and compared using the same scale!</li>
</ul>
<div class="fragment">
<ul>
<li><p>This second model is <span class="blue">more parsimonious</span> and yet it reaches similar predictive performance.</p></li>
<li><p>It is also more coherent with the nature of the data: the predictions cannot be negative and the relationship between engine size and the consumption is monotone.</p></li>
<li><p>There is still some heteroschedasticity in the residuals — is this due to a missing covariate that has not been included into the model?</p></li>
</ul>
</div>
</section>
<section id="a-third-model-additional-variables" class="slide level2">
<h2>A third model: additional variables</h2>
<ul>
<li>Let us consider <span class="orange">two additional variables</span>: <code>curb.weight</code> (<span class="math inline">w</span>) and <code>n.cylinders</code> (<span class="math inline">v</span>). A modified model could be: <span class="math display">
\log{y_i} = \beta_1 + \beta_2 \log{x_i} +  \beta_3 \log{w_i} + \beta_4 I(z_i = \texttt{gas}) + \beta_5 I(v_i = 2) + \epsilon_i,
</span> for <span class="math inline">i=1,\dots,n</span>.</li>
</ul>
<div class="fragment">
<div class="cell" data-hash="un_A1_cache/revealjs/unnamed-chunk-11_a4e6dd8611316de7f79748cadbcd730f">
<div class="cell-output-display">
<table style="width:100%;">
<colgroup>
<col style="width: 26%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 10%">
<col style="width: 11%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
<th style="text-align: right;">conf.low</th>
<th style="text-align: right;">conf.high</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: right;">9.423</td>
<td style="text-align: right;">0.482</td>
<td style="text-align: right;">19.549</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">8.472</td>
<td style="text-align: right;">10.373</td>
</tr>
<tr class="even">
<td style="text-align: left;">I(log(engine.size))</td>
<td style="text-align: right;">-0.180</td>
<td style="text-align: right;">0.051</td>
<td style="text-align: right;">-3.504</td>
<td style="text-align: right;">0.001</td>
<td style="text-align: right;">-0.281</td>
<td style="text-align: right;">-0.079</td>
</tr>
<tr class="odd">
<td style="text-align: left;">I(log(curb.weight))</td>
<td style="text-align: right;">-0.943</td>
<td style="text-align: right;">0.072</td>
<td style="text-align: right;">-13.066</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">-1.085</td>
<td style="text-align: right;">-0.800</td>
</tr>
<tr class="even">
<td style="text-align: left;">fuelgas</td>
<td style="text-align: right;">-0.353</td>
<td style="text-align: right;">0.022</td>
<td style="text-align: right;">-15.934</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">-0.396</td>
<td style="text-align: right;">-0.309</td>
</tr>
<tr class="odd">
<td style="text-align: left;">cylinders2TRUE</td>
<td style="text-align: right;">-0.481</td>
<td style="text-align: right;">0.052</td>
<td style="text-align: right;">-9.301</td>
<td style="text-align: right;">0.000</td>
<td style="text-align: right;">-0.584</td>
<td style="text-align: right;">-0.379</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</section>
<section id="a-third-model-graphical-diagnostics" class="slide level2">
<h2>A third model: graphical diagnostics</h2>
<div class="cell" data-hash="un_A1_cache/revealjs/unnamed-chunk-12_c986a7a9640cebfc3fb357790e566d3d">
<div class="cell-output-display">
<p><img data-src="un_A1_files/figure-revealjs/unnamed-chunk-12-1.png" width="3000"></p>
</div>
</div>
</section>
<section id="comments-and-criticisms-2" class="slide level2">
<h2>Comments and criticisms</h2>
<ul>
<li>The goodness of fit greatly <span class="blue">improved</span>:</li>
</ul>
<div class="cell" data-hash="un_A1_cache/revealjs/unnamed-chunk-13_7871f5e868d8b6f8b5347127c10c8cc4">
<div class="cell-output-display">
<table>
<thead>
<tr class="header">
<th style="text-align: right;">r.squared.original</th>
<th style="text-align: right;">r.squared</th>
<th style="text-align: right;">sigma</th>
<th style="text-align: right;">deviance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.869048</td>
<td style="text-align: right;">0.8819199</td>
<td style="text-align: right;">0.0896089</td>
<td style="text-align: right;">1.589891</td>
</tr>
</tbody>
</table>
</div>
</div>
<ul>
<li><p>We took care of the <span class="orange">outliers</span>, which it turns out are identified by the group of cars having 2 cylinders.</p></li>
<li><p>The diagnostic plots are also very much improved, although still not perfect.</p></li>
<li><p>The estimates are coherent with our expectations, based on common knowledge. Have a look at the textbook (A&amp;S) for a detailed explaination about <span class="math inline">\beta_4</span>!</p></li>
</ul>
</section></section>
<section>
<section id="cholesky-factorization" class="title-slide slide level1 center">
<h1>Cholesky factorization</h1>

</section>
<section id="how-to-obtain-the-least-squares-estimate" class="slide level2">
<h2>How to obtain the least squares estimate?</h2>
<ul>
<li><p>In undergraduate courses it is often suggested that the least square estimate should be computed using the formula <span class="math display">
\hat{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T \bm{y},
</span> that is, using the R code <code>solve(t(X) %*% X) %*% t(X) %*% y</code>.</p></li>
<li><p>This is <span class="orange">theoretically correct</span> and it works reasonably well in many simple cases.</p></li>
</ul>
<div class="fragment">
<ul>
<li><p>However, in realistic problems, when we have a lot of data (large <span class="math inline">n</span>) and correlated variables, the above code is <span class="orange">computationally inefficient</span> and <span class="blue">numerically inaccurate</span>.</p></li>
<li><p>The main computational bottleneck is about obtaining the inverse of <span class="math inline">\bm{X}^T\bm{X}</span>, which is very costly and often numerically unstable, especially when the predictors are almost collinear.</p></li>
</ul>
</div>
</section>
<section id="the-normal-equations" class="slide level2">
<h2>The normal equations</h2>
<ul>
<li><p>Consider the following system of equations (<span class="orange">normal equations</span>):<span class="math display">
\bm{X}^T\bm{X} \beta = \bm{X}^T \bm{y}.
</span></p></li>
<li><p>This system could be solved using <code>solve(crossprod(X), crossprod(X, y))</code>.</p></li>
<li><p>The above R command avoids the computation of <span class="math inline">(\bm{X}^T\bm{X})^{-1}</span> and it is preferable compared to the “direct solution”. However, it does not exploit the properties of the matrix <span class="math inline">\bm{X}^T\bm{X}</span>.</p></li>
</ul>
<div class="fragment">
<div class="callout callout-note no-icon callout-captioned callout-style-simple">
<div class="callout-body">
<div class="callout-caption">
<p><strong>Proposition A.1</strong></p>
</div>
<div class="callout-content">
<p>Suppose <span class="math inline">\bm{X} \in \mathbb{R}^{n \times p}</span> with <span class="math inline">n \ge p</span> has full rank, that is <span class="math inline">\text{rk}(\bm{X}) = p</span>. Then, the matrix <span class="math display">
\bm{X}^T\bm{X}
</span> is <span class="blue">symmetric</span> and <span class="orange">positive definite</span>.</p>
</div>
</div>
</div>
</div>
</section>
<section id="cholesky-factorization-1" class="slide level2">
<h2>Cholesky factorization</h2>
<ul>
<li>Recall (from your favorite linear algebra textbook) that a <span class="blue">symmetric</span> matrix <span class="math inline">\bm{A} \in \mathbb{R}^{p \times p}</span> is <span class="orange">positive definite</span> if and only if one the following properties is satisfied
<ul>
<li>The quadratic form <span class="math inline">\bm{x}^T \bm{A} \bm{x} &gt; 0</span> for all <span class="math inline">\bm{x} \in \mathbb{R}^p</span> such that <span class="math inline">\bm{x} \neq 0</span>.</li>
<li>The eigenvalues <span class="math inline">\lambda_1,\dots,\lambda_p</span> of <span class="math inline">\bm{A}</span> are all strictly positive.</li>
</ul></li>
</ul>
<div class="callout callout-note no-icon callout-captioned callout-style-simple">
<div class="callout-body">
<div class="callout-caption">
<p><strong>Theorem (Cholesky factorization)</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\bm{A} \in \mathbb{R}^{p \times p}</span> be a symmetric and positive definite matrix. Then, there exists a unique <span class="orange">upper triangular</span> <span class="math inline">p \times p</span> matrix <span class="math inline">\bm{R}</span> with positive entries such that <span class="math display">
\bm{A} = \bm{R}^T\bm{R}.
</span></p>
</div>
</div>
</div>
</section>
<section id="cholesky-factorization-and-least-squares" class="slide level2">
<h2>Cholesky factorization and least squares</h2>
<div>
<ul>
<li class="fragment"><p>Let <span class="math inline">\bm{R}^T\bm{R}</span> be the Cholesky factorization of the matrix <span class="math inline">\bm{X}^T\bm{X}</span>. Then, the <span class="orange">normal equations</span> can be written as <span class="math display">
\bm{R}^T\bm{R} \beta = \bm{X}^T \bm{y},
</span> which can be can be solved in two steps.</p></li>
<li class="fragment"><p><span class="blue">Step 1 (Forwardsolve)</span>. Solve with respect to <span class="math inline">z</span> the system of equations <span class="math display">
\bm{R}^T z = \bm{X}^T \bm{y}.
</span></p></li>
<li class="fragment"><p><span class="blue">Step 2 (Backsolve)</span>. Given <span class="math inline">z</span>, now solve with respect to <span class="math inline">\beta</span> the system of equations <span class="math display">
\bm{R} \beta = z.
</span></p></li>
</ul>
</div>
</section>
<section id="forward-and-backward-substitutions" class="slide level2">
<h2>Forward and backward substitutions</h2>
<div>
<ul>
<li class="fragment"><p>The solution of <span class="blue">triangular systems</span> is <span class="orange">straightforward</span>.</p></li>
<li class="fragment"><p>As an example, consider the following <span class="math inline">3 \times 3</span> lower triangular system: <span class="math display">
\begin{bmatrix}
l_{11} &amp; 0 &amp; 0 \\
l_{21} &amp; l_{22} &amp; 0 \\
l_{31} &amp; l_{32} &amp; l_{33} \\
\end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix}
b_1 \\ b_2 \\ b_3
\end{bmatrix}.
</span></p></li>
<li class="fragment"><p>The solution for <span class="math inline">x_1,x_2,x_3</span> can be found sequentially: <span class="math display">
x_1 = \frac{b_1}{l_{11}}, \qquad x_2 = \frac{b_2 -  l_{21}x_1}{l_{22}}, \qquad x_3 = \frac{b_3 - l_{31}x_1 - l_{32}x_2}{l_{33}}.
</span></p></li>
<li class="fragment"><p>Finding the inverse <span class="math inline">\bm{R}^{-1}</span> is simple, again because <span class="math inline">\bm{R}</span> is upper triangular. Also note that <span class="math display">
(\bm{X}^T \bm{X})^{-1} = (\bm{R}^T \bm{R})^{-1} = \bm{R}^{-1} (\bm{R}^{-1})^T.
</span></p></li>
</ul>
</div>
</section>
<section id="computational-complexity" class="slide level2">
<h2>Computational complexity</h2>
<div>
<ul>
<li class="fragment"><p>The solution via Cholesky factorization is a <span class="blue">fast direct approach</span> for finding <span class="math inline">\hat{\beta}</span>.</p></li>
<li class="fragment"><p>The expensive steps are:</p>
<ul>
<li class="fragment">The formation of the matrix <span class="math inline">\bm{X}^T\bm{X}</span> requires <span class="math inline">\sim n p^2</span> elementary operations</li>
<li class="fragment">The Cholesky factorization of <span class="math inline">\bm{X}^T\bm{X}</span> requires <span class="math inline">\sim p^3 / 3</span> elementary operations.</li>
</ul></li>
<li class="fragment"><p>This gives an overall computational complexity of order <span class="math display">
\sim n p^2 + p^3 /3,
</span> which corrects the typographical error of the A&amp;S textbook.</p></li>
<li class="fragment"><p>This means that in <span class="orange">high-dimensional</span> settings (large <span class="math inline">p</span>) computations become rapidly <span class="orange">very costly</span>, since the complexity is cubic in <span class="math inline">p</span>.</p></li>
</ul>
</div>
</section>
<section id="error-propagation-in-normal-equations" class="slide level2">
<h2>☠️ - Error propagation in normal equations</h2>
<div>
<ul>
<li class="fragment"><p>The normal equations method is typically <span class="blue">quicker</span> than other algorithms, as it removes the dependency on <span class="math inline">n</span>, but it is in general numerically more <span class="orange">unstable</span>.</p></li>
<li class="fragment"><p>Consider for example the following matrix: <span class="math display">
\bm{X} = \begin{bmatrix}1 &amp; 1 \\
\epsilon &amp; 0 \\
0 &amp; \epsilon \end{bmatrix},
</span> for a small value <span class="math inline">\epsilon &gt; 0</span>. Then, we obtain that <span class="math display">\bm{X}^T \bm{X} = \begin{bmatrix}1 + \epsilon^2&amp; 1 \\
1 &amp; 1 + \epsilon^2 \end{bmatrix}.
</span></p></li>
<li class="fragment"><p>The numerical computation of <span class="math inline">\epsilon^2</span> in <span class="math inline">\bm{X}^T\bm{X}</span> requires a higher precision compared to <span class="math inline">\epsilon</span>, leading to numerical instabilities and/or a <span class="orange">loss in accuracy</span>.</p></li>
</ul>
</div>
</section>
<section id="condition-numbers-and-normal-equations" class="slide level2">
<h2>☠️ - Condition numbers and normal equations</h2>
<div>
<ul>
<li class="fragment"><p>Suppose <span class="math inline">\bm{X} \in \mathbb{R}^{n \times p}</span> with <span class="math inline">n \ge p</span> has full rank and singular values <span class="math inline">d_1 \ge d_2 \ge \dots \ge d_p</span>. Then its <span class="orange">condition number</span> is <span class="math display">
\kappa(\bm{X}) = ||\bm{X}|| \cdot ||\bm{X}^+|| = \frac{d_1}{d_p},
</span> where <span class="math inline">\bm{X}^+</span> is the Moore-Penrose pseudo-inverse. Note that <span class="math inline">\kappa(\bm{X}) \ge 1</span>.</p></li>
<li class="fragment"><p>If <span class="math inline">\kappa(\bm{X})</span> is small, the matrix <span class="math inline">\bm{X}</span> is <span class="blue">well conditioned</span>. Otherwise, we say it is <span class="orange">ill conditioned</span>.</p></li>
<li class="fragment"><p>The condition number determines how accurately we can solve linear systems.</p></li>
<li class="fragment"><p>An important fact is: <span class="math display">
\kappa(\bm{X}^T\bm{X}) = \kappa(\bm{X})^2,
</span> implying that there is a clear loss of numerical accuracy when using normal equations.</p></li>
</ul>
</div>
</section></section>
<section>
<section id="the-qr-decomposition" class="title-slide slide level1 center">
<h1>The QR decomposition</h1>

</section>
<section id="orthogonal-predictors" class="slide level2">
<h2>Orthogonal predictors</h2>
<ul>
<li><p>When the <span class="blue">predictors</span> are mutually <span class="orange">orthogonal</span>, the problem is significantly simpler.</p></li>
<li><p>In other words, consider a linear model of the form <span class="math display">
\bm{y} = \bm{Z}\bm{\beta} + \bm{\epsilon},
</span> where <span class="math inline">\bm{Z} = (\bm{z}_1,\dots,\bm{z}_p)</span>. <span class="orange">Orthogonality</span> means that <span class="math inline">\bm{Z}^T\bm{Z} = \text{diag}(\bm{z}_1^T\bm{z}_1,\dots,\bm{z}_p^T\bm{z}_p)</span>.</p></li>
</ul>
<div class="callout callout-note no-icon callout-captioned callout-style-simple">
<div class="callout-body">
<div class="callout-caption">
<p><strong>Proposition A.2. OLS with orthogonal predictors</strong></p>
</div>
<div class="callout-content">
<p>The least square estimate <span class="math inline">\hat{\bm{\beta}} = (\hat{\beta}_1,\dots,\hat{\beta}_p)^T</span> of a linear model with orthogonal predictors is</p>
<p><span class="math display">
\hat{\gamma}_j = \frac{\bm{z}_j^T\bm{y}}{\bm{z}_j^T\bm{z}_j}, \qquad j=1,\dots,p.
</span></p>
</div>
</div>
</div>
</section>
<section id="regression-by-successive-orthogonalization" class="slide level2">
<h2>Regression by successive orthogonalization</h2>
<ul>
<li>The predictors of <span class="math inline">\bm{X}</span> are generally not orthogonal. We want to find a suitable transformation <span class="math inline">\bm{Z} = \bm{X} \bm{\Gamma}^{-1}</span> that <span class="blue">orthogonalize</span> the <span class="orange">predictors</span>.</li>
</ul>
<div class="fragment">
<ul>
<li><p>Suppose for example that <span class="math inline">p = 2</span>. We set <span class="orange">first orthogonal predictor</span> <span class="math inline">\bm{z}_1 = \bm{x}_1</span>.</p></li>
<li><p>We then consider the following <span class="blue">univariate</span> regression problem <span class="math display">
\bm{x}_2 = \gamma \bm{z}_1 + \bm{\epsilon}, \qquad \text{which leads} \qquad \hat{\gamma} = \frac{\bm{z}_1^T\bm{x}_2}{\bm{z}_1^T\bm{z}_1}.
</span></p></li>
<li><p>The <span class="orange">second orthogonal predictor</span> is obtained as the <span class="blue">residual term</span>: <span class="math display">
\bm{z}_2 = \bm{x}_2 - \hat{\gamma}\bm{z}_1.
</span></p></li>
<li><p>The geometry of linear models guarantees that <span class="math inline">\bm{z}_1^T\bm{z}_2 = 0</span>.</p></li>
</ul>
</div>
</section>
<section id="gram-schmidt-algorithm" class="slide level2">
<h2>Gram-Schmidt algorithm</h2>
<ul>
<li><p>Let us now consider the <span class="orange">general case</span>, for any value of <span class="math inline">p</span>.</p></li>
<li><p><span class="blue">Initialization</span>. Set <span class="math inline">\bm{z}_1 = \bm{x}_1</span>.</p></li>
<li><p><span class="blue">For <span class="math inline">j= 2,\dots,p</span></span>. Consider the regression problem with <span class="math inline">j-1</span> orthogonal predictors <span class="math display">
\bm{x}_j = \sum_{k=1}^{j-1}\gamma_{kj} \bm{z}_k + \bm{\epsilon}_j, \quad \text{which leads} \quad \hat{\gamma}_{kj} = \frac{\bm{z}_k^T\bm{x}_j}{\bm{z}_k^T\bm{z}_k}, \quad k=1,\dots,j-1,
</span> Then, compute the new vector <span class="math inline">\bm{z}_j</span> as the <span class="orange">residual</span> term <span class="math display">
\bm{z}_j = \bm{x}_j - \sum_{k=1}^{j-1}\hat{\gamma}_{kj} \bm{z}_k
</span></p></li>
<li><p>The geometry of linear models guarantees <span class="orange">orthogonality</span>, that is <span class="math inline">\bm{z}_j^T \bm{z}_{j'} = 0</span> for any <span class="math inline">j \neq j'</span>.</p></li>
</ul>
</section>
<section id="the-qr-decomposition-i" class="slide level2">
<h2>The QR decomposition I</h2>
<div>
<ul>
<li class="fragment"><p>By construction, the Gram-Schmidt algorithm produces the following decomposition <span class="math display">
\bm{X} = \bm{Z} \bm{\Gamma}, \qquad \bm{\Gamma} =
\begin{bmatrix}
1 &amp; \hat{\gamma}_{12} &amp; \hat{\gamma}_{13} &amp;\cdots &amp; \hat{\gamma}_{1p} \\
0 &amp;  1 &amp; \hat{\gamma}_{23} &amp;\cdots &amp; \hat{\gamma}_{2p} \\
\vdots &amp; \vdots &amp; \vdots &amp;\ddots &amp; \vdots\\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1
\end{bmatrix}, \qquad \bm{Z} = (\bm{z}_1,\dots,\bm{z}_p).
</span></p></li>
<li class="fragment"><p>The <span class="math inline">p \times p</span> matrix <span class="math inline">\bm{\Gamma}</span> is <span class="orange">upper triangular</span>, whereas the columns of the <span class="math inline">n \times p</span> matrix <span class="math inline">\bm{Z}</span> are <span class="blue">mutually orthogonal</span>.</p></li>
<li class="fragment"><p>It is often convenient to <span class="orange">standardize</span> the columns of <span class="math inline">\bm{Z}</span>, dividing them by their norm <span class="math inline">||\bm{z}_j||</span>. Let <span class="math inline">\bm{D} = \text{diag}(||\bm{z}_1||, \dots, ||\bm{z}_p||)</span>, then in matrix notation: <span class="math display">
\bm{X} = \bm{Z} \bm{\Gamma} = \bm{Z} \bm{D}^{-1} \bm{D} \bm{\Gamma} = \bm{Q} \bm{R}, \quad \text{with} \quad \bm{Q} = \bm{Z}\bm{D}^{-1}.
</span></p></li>
<li class="fragment"><p><span class="orange">Remark</span>. Note that <span class="math inline">\bm{Q}^T \bm{Q} = I_p</span>, i.e.&nbsp;the columns of <span class="math inline">\bm{Q}</span> are <span class="blue">orthonormal</span>.</p></li>
</ul>
</div>
</section>
<section id="the-qr-decomposition-ii" class="slide level2">
<h2>The QR decomposition II</h2>
<div class="callout callout-note no-icon callout-captioned callout-style-simple">
<div class="callout-body">
<div class="callout-caption">
<p><strong>Theorem (QR factorization)</strong></p>
</div>
<div class="callout-content">
<p>Suppose <span class="math inline">\bm{X} \in \mathbb{R}^{n \times p}</span> with <span class="math inline">n \ge p</span> has full rank, that is <span class="math inline">\text{rk}(\bm{X}) = p</span>. Then there exists a factorization of the form <span class="math display">
\bm{X} = \bm{Q} \bm{R},
</span> where <span class="math inline">\bm{Q} \in \mathbb{R}^{n \times p}</span> has <span class="orange">orthonormal columns</span> and <span class="math inline">\bm{R} \in \mathbb{R}^{p \times p}</span> is an <span class="blue">upper triangular</span> matrix.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-captioned callout-style-simple">
<div class="callout-body">
<div class="callout-caption">
<p><strong>Corollary (QR factorization)</strong></p>
</div>
<div class="callout-content">
<p>The QR decomposition is <span class="blue">unique</span> up to <span class="blue">sign flips</span> of the columns of <span class="math inline">\bm{Q}</span> and the rows of <span class="math inline">\bm{R}</span>. Moreover, if <span class="math inline">\bm{R}</span> has positive diagonal entries, as the one obtained using Gram-Schmidt, then it coincides with the <span class="orange">Cholesky factor</span> of <span class="math inline">\bm{X}^T\bm{X}</span>.</p>
</div>
</div>
</div>
</section>
<section id="the-qr-decomposition-and-least-squares" class="slide level2">
<h2>The QR decomposition and least squares</h2>
<div>
<ul>
<li class="fragment"><p>The QR decomposition greatly facilitates computations for linear models. Indeed: <span class="math display">
\begin{aligned}
\hat{\beta} &amp;= (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y} = [(\bm{Q}\bm{R})^T\bm{Q}\bm{R}]^{-1}(\bm{Q}\bm{R})^T\bm{y}\\
&amp;= (\bm{R}^T\bm{Q}^T\bm{Q}\bm{R})^{-1}\bm{R}^T\bm{Q}^T\bm{y} \\
&amp;= \bm{R}^{-1} (\bm{R}^T)^{-1} \bm{R}^T\bm{Q}^T\bm{y} \\
&amp;= \bm{R}^{-1}\bm{Q}^T \bm{y}.
\end{aligned}
</span></p></li>
<li class="fragment"><p>Hence, the least square estimate is obtained as the solution of the <span class="blue">triangular system</span> <span class="math display">
\bm{R}\beta = \bm{Q}^T\bm{y},
</span> which can be easily solved via <span class="orange">backward substitution</span>.</p></li>
<li class="fragment"><p>As a special case of the above equation, one gets <span class="math inline">\hat{\beta}_p = (\bm{z}_p^T\bm{y}) / (\bm{z}_p^T \bm{z}_p)</span>.</p></li>
</ul>
</div>
</section>
<section id="the-qr-decomposition-and-linear-models" class="slide level2">
<h2>The QR decomposition and linear models</h2>
<div>
<ul>
<li class="fragment"><p>An important advantage of the QR factorization is that many other useful quantities can be readily computed. For example, the <span class="orange">covariance matrix</span> is obtained as: <span class="math display">
  s^2 (\bm{X}^T \bm{X})^{-1} = s^2 \bm{R}^{-1} (\bm{R}^{-1})^T.
  </span></p></li>
<li class="fragment"><p>The <span class="orange">predicted values</span> and the <span class="blue">projection matrix</span> are also easily obtained as <span class="math display">
\hat{\bm{y}} = \bm{H}\bm{y} = \bm{Q}\bm{Q}^T\bm{y}.
</span></p></li>
<li class="fragment"><p>The diagonal elements <span class="math inline">h_i = [\bm{H}]_{ii}</span> of the hat matrix <span class="math inline">\bm{H}</span> are called <span class="orange">leverages</span> and one may want to compute them without evaluating the full <span class="math inline">n \times n</span> matrix, using <span class="math display">
h_i = \sum_{j=1}^p q_{ij}^2, \qquad i=1,\dots,n,
</span> where <span class="math inline">q_{ij}</span> are the entries of <span class="math inline">\bm{Q}</span>.</p></li>
</ul>
</div>
</section>
<section id="computational-complexity-1" class="slide level2">
<h2>Computational complexity</h2>
<div>
<ul>
<li class="fragment"><p>The solution via QR factorization is <span class="orange">numerically reliable</span> and it facilitates the computation of other quantities of interest.</p></li>
<li class="fragment"><p>In practice, the QR is computed via a <span class="blue">modified Gram-Schmidt</span>, that fixes the instabilities of the naïve Gram-Schmidt algorithm, or via <span class="orange">Householder reflections</span>.</p></li>
<li class="fragment"><p>The expensive step is the QR factorization itself. This gives an overall computational complexity of order <span class="math display">
\sim 2 n p^2</span> which is about twice that of the Cholesky, when <span class="math inline">n</span> is much larger than <span class="math inline">p</span>, and about the same when <span class="math inline">p \approx n</span>.</p></li>
<li class="fragment"><p>Depending on the context, we may prefer the Cholesky or the QR. The default approach in R, i.e.&nbsp;the one implemented in the <code>lm</code> function, is the QR factorization.<br>
</p></li>
</ul>
</div>
</section>
<section id="pivoting-and-rank-deficiencies" class="slide level2">
<h2>☠️ - Pivoting and rank deficiencies</h2>
<div>
<ul>
<li class="fragment"><p>If <span class="math inline">\text{rk}(\bm{X}) = k &lt; p</span> (<span class="orange">rank deficiency</span>) then it is still possibile to obtain a “QR” factorization of the form <span class="math display">
\bm{X}\bm{P} = \bm{Q}\begin{bmatrix}\bm{R}_{11} &amp; \bm{R}_{12} \\
0 &amp; 0\end{bmatrix},
</span> where <span class="math inline">\bm{P}</span> is a <span class="math inline">p × p</span> permutation matrix and <span class="math inline">\bm{R}_{11}</span> is an <span class="math inline">k \times k</span> upper triangular and non-singular matrix.</p></li>
<li class="fragment"><p>This operation is sometimes called <span class="blue">pivoting</span> and it is particularly important even when <span class="math inline">\text{rk}(\bm{X}) = p</span> to prevent numerical issues when the condition number <span class="math inline">\kappa(\bm{X})</span> is high.</p></li>
<li class="fragment"><p>In presence of perfect collinarity, the implementation of the QR decomposition in R (<code>qr</code>) relies on pivoting. This is why the <code>lm</code> function is able to automatically “omit” a predictor.</p></li>
</ul>
</div>
</section></section>
<section>
<section id="iterative-methods" class="title-slide slide level1 center">
<h1>Iterative methods</h1>

</section>
<section id="when-n-is-very-large" class="slide level2">
<h2>When <span class="math inline">n</span> is very large…</h2>
<div>
<ul>
<li class="fragment"><p>When the sample size <span class="math inline">n</span> is extremely large, as it is common in data mining problems, then the QR factorization cannot be computed.</p></li>
<li class="fragment"><p>Indeed, even <span class="orange">loading</span> <span class="math inline">\bm{X}</span> into <span class="orange">memory</span> could be problematic.</p></li>
<li class="fragment"><p>In the normal equations approach, we only need to compute the <span class="blue">sufficient statistics</span>: <span class="math display">
\bm{W} = \bm{X}^T\bm{X}, \qquad \bm{u} = \bm{X}^T\bm{y},
</span> which are of dimension <span class="math inline">p\times p</span> and <span class="math inline">p \times 1</span>, respectively.</p></li>
<li class="fragment"><p>Using <span class="math inline">\bm{W}</span> and <span class="math inline">\bm{u}</span> one can obtain the least square estimate <span class="math inline">\hat{\beta}</span> using the Cholesky factorization.</p></li>
<li class="fragment"><p>However, when <span class="math inline">n</span> is extremely large, the difficult part is indeed computing <span class="math inline">\bm{W}</span> and <span class="math inline">\bm{u}</span>!</p></li>
</ul>
</div>
</section>
<section id="recursive-data-import" class="slide level2">
<h2>Recursive data import</h2>
<div>
<ul>
<li class="fragment"><p>Using matrix notation, we express <span class="math inline">\bm{W} = \bm{W}_{(n)}</span> and <span class="math inline">\bm{u} = \bm{u}_{(n)}</span> as follows <span class="math display">
\bm{W}_{(n)} = \sum_{i=1}^n \tilde{\bm{x}}_i \tilde{\bm{x}}_i^T, \qquad \bm{u}_{(n)} = \sum_{i=1}^n\tilde{\bm{x}}_i y_i.
</span></p></li>
<li class="fragment"><p>Let us define the <span class="blue">initial conditions</span> <span class="math inline">\bm{W}_{(1)} = \tilde{\bm{x}}_1 \tilde{\bm{x}}_1^T</span> and <span class="math inline">\bm{u}_{(1)} = \tilde{\bm{x}}_1 y_1</span>.</p></li>
<li class="fragment"><p>Then, the following <span class="orange">recursive relationship</span> holds: <span class="math display">
\bm{W}_{(i)} = \bm{W}_{(i-1)} +  \tilde{\bm{x}}_i \tilde{\bm{x}}_i^T, \qquad \bm{u}_{(i)} = \bm{u}_{(i-1)} + \tilde{\bm{x}}_i y_i, \qquad i=2,\dots,n,
</span> where <span class="math inline">\bm{W}_{(i)}</span> is the matrix formed by the first <span class="math inline">i</span> summands of <span class="math inline">\bm{W}_{(n)}</span> and analogously <span class="math inline">\bm{u}_{(i)}</span>.</p></li>
<li class="fragment"><p>Hence <span class="math inline">\bm{W}_{(n)}</span> and <span class="math inline">\bm{u}_{(n)}</span> can be calculated by <span class="blue">importing a single record</span> at a time, which does not create memory issues.</p></li>
</ul>
</div>
</section>
<section id="recursive-estimates" class="slide level2">
<h2>Recursive estimates</h2>
<div>
<ul>
<li class="fragment"><p>In many occasions, the data flow continuously, meaning that we get a new pair of observations <span class="math inline">(\tilde{\bm{x}}_{n+1}, y_{n+1})</span> every minute, or even every second.</p></li>
<li class="fragment"><p>We would like to <span class="orange">update</span> the current least square estimate <span class="math inline">\hat{\beta}_{(n)}</span> with the new information <span class="math inline">(\tilde{\bm{x}}_{n+1}, y_{n+1})</span>, but ideally without re-doing all the calculations.</p></li>
<li class="fragment"><p>The <span class="blue">recursive data import</span> of the <a href="#/recursive-data-import">previous slide</a> is partially unsatisfactory, because one would need to invert (or factorize) a <span class="math inline">p \times p</span> matrix every time, which could be costly.</p></li>
<li class="fragment"><p>Let us define some useful quantity: <span class="math display">
\bm{V}_{(n)} = \bm{W}_{(n)}^{-1} = (\bm{X}_{(n)}^T\bm{X}_{(n)})^{-1},
</span> where <span class="math inline">\bm{X}_{(n)}</span> denotes the design matrix with <span class="math inline">n</span> observations and analogously <span class="math inline">\bm{y}_{(n)}</span>.</p></li>
</ul>
</div>
</section>
<section id="sherman-morrison-formula" class="slide level2">
<h2>Sherman-Morrison formula</h2>
<ul>
<li><p>When the new data points arrive, we can write the updated quantities <span class="math display">
\bm{X}_{(n+1)} = (\bm{X}_{(n)}, \tilde{\bm{x}}_{(n + 1)})^T, \quad \bm{W}_{(n + 1)} = (\bm{X}_{(n+1)}^T\bm{X}_{(n + 1)}) = (\bm{X}_{(n)}^T\bm{X}_{(n)} +  \tilde{\bm{x}}_{(n + 1)} \tilde{\bm{x}}_{(n + 1)}^T).
</span></p></li>
<li><p>The difficult part is to <span class="orange">efficiently</span> compute <span class="math inline">\bm{V}_{(n+1)} = \bm{W}_{(n + 1)}^{-1}</span>. The following result of linear algebra is of incredible help in this regard.</p></li>
</ul>
<div class="callout callout-note no-icon callout-captioned callout-style-simple">
<div class="callout-body">
<div class="callout-caption">
<p><strong>Sherman-Morrison formula</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\bm{A} \in \mathbb{R}^{p\times p}</span> be an invertible matrix and let <span class="math inline">\bm{b},\bm{d}</span> be <span class="math inline">p</span>-dimensional vectors. Then <span class="math display">
(\bm{A} + \bm{b} \bm{d}^T)^{-1} = \bm{A}^{-1} - \frac{1}{1 + \bm{d}^T \bm{A}^{-1}\bm{b}}\bm{A}^{-1}\bm{b}\bm{d}^T\bm{A}^{-1}.
</span></p>
</div>
</div>
</div>
</section>
<section id="the-recursive-least-squares-algorithm-i" class="slide level2">
<h2>The recursive least squares algorithm I</h2>
<div>
<ul>
<li class="fragment"><p>Using the Sherman-Morrison formula, then we can express the new matrix <span class="math inline">\bm{V}_{(n + 1)}</span> as a function of previously computed quantities: <span class="math display">
\bm{V}_{(n + 1)} = \bm{V}_{(n)} - v_{(n)}\bm{V}_{(n)} \tilde{\bm{x}}_{(n + 1)}\tilde{\bm{x}}_{(n + 1)}^T \bm{V}_{(n)}, \quad v_{(n)} = \frac{1}{(1 + \tilde{\bm{x}}_{(n + 1)}^T \bm{V}_{(n)}\tilde{\bm{x}}_{(n + 1)})}.
</span></p></li>
<li class="fragment"><p>The <span class="orange">updated least square estimate</span> therefore becomes <span class="math display">
\begin{aligned}
\hat{\beta}_{(n+1)} &amp;= \bm{V}_{(n + 1)}(\bm{X}_{(n)}^T\bm{y}_{(n)} +  \tilde{\bm{x}}_{(n + 1)} y_{n+1}) \\
&amp;=\hat{\beta}_{(n)} +  \underbrace{v_{(n)} \bm{V}_{(n)}\tilde{\bm{x}}_{(n + 1)}}_{k_n}\underbrace{(y_{n+1} - \tilde{\bm{x}}_{(n + 1)}^T\hat{\beta}_{(n)})}_{e_{n + 1}} \\
&amp;= \hat{\beta}_{(n)} + k_n e_{n+1}.
\end{aligned}
</span></p></li>
<li class="fragment"><p>The quantity <span class="math inline">e_{n+1}</span> is the <span class="blue">prediction error</span> of <span class="math inline">y_{n+1}</span> based on the previous estimate <span class="math inline">\hat{\beta}_{(n)}</span>.</p></li>
</ul>
</div>
</section>
<section id="the-recursive-least-squares-algorithm-ii" class="slide level2">
<h2>The recursive least squares algorithm II</h2>
<ul>
<li><p>The recursive estimation <span class="math inline">\hat{\beta}_{(n+1)} = \hat{\beta}_{(n)} + k_n e_{n+1}</span> takes the form of a <span class="orange">linear filter</span>, in which the new estimate <span class="math inline">\hat{\beta}_{(n+1)}</span> is obtained by modifying the old one <span class="math inline">\hat{\beta}_{(n+1)}</span>.</p></li>
<li><p>This is performed according to the <span class="blue">prediction error</span> <span class="math inline">\epsilon_{n+1}</span> and the <span class="orange">gain</span> <span class="math inline">k_n</span> of the filter.</p></li>
<li><p>Using a terminology typical of the machine learning field, we say that the estimator “learns from its errors”.</p></li>
<li><p>If <span class="math inline">n</span> is sufficiently high, it is also possible to get an <span class="orange">approximate</span> solution by initializing the algorithm by setting <span class="math inline">\bm{V}_{(0)} = I_p</span>, to avoid any matrix inversion / factorization.</p></li>
</ul>
<div class="fragment">
<ul>
<li>With further algebraic steps, we also obtain a <span class="blue">recursive formula</span> for the <span class="blue">deviance</span></li>
</ul>
<p><span class="math display">
||\bm{y}_{(n + 1)} - \bm{X}_{(n+1)}\hat{\beta}_{(n+1)}||^2 = ||\bm{y}_{(n + 1)} - \bm{X}_{(n)}\hat{\beta}_{(n)}||^2 + v_{(n)} e_{n+1}^2.
</span></p>
<ul>
<li>The full algorithm is provided in A&amp;S, Algorithm 2.2.</li>
</ul>
</div>
</section></section>
<section>
<section id="references" class="title-slide slide level1 center">
<h1>References</h1>

</section>
<section id="references-1" class="slide level2">
<h2>References</h2>
<ul>
<li><span class="blue">Main references</span>
<ul>
<li><strong>Chapter 2</strong> of Azzalini, A. and Scarpa, B. (2011), <a href="http://azzalini.stat.unipd.it/Book-DM/"><em>Data Analysis and Data Mining</em></a>, Oxford University Press.</li>
<li><strong>Chapter 3</strong> of Hastie, T., Tibshirani, R. and Friedman, J. (2009), <a href="https://hastie.su.domains/ElemStatLearn/"><em>The Elements of Statistical Learning</em></a>, Second Edition, Springer.</li>
</ul></li>
<li><span class="orange">Specific references</span>
<ul>
<li><strong>Chapters 1–3</strong> of Quarteroni, A., Sacco, R., and Saleri F. (2007). <em>Numerical mathematics</em>. Second Edition, Springer.</li>
<li><strong>Chapters 1–5</strong> of Golub, G.H., and Van Loan, C.F. (1983). <em>Matrix computations</em>. Hopkins University Press.</li>
</ul></li>
</ul>
<p><img src="img/logoB.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://tommasorigon.github.io/datamining">Home page</a></p>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="un_A1_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="un_A1_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="un_A1_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="un_A1_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="un_A1_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="un_A1_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="un_A1_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="un_A1_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="un_A1_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>