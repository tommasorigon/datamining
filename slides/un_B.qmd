---
title: "Optimism, Conflicts, and Trade-offs"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_B_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 200
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
    highlight-style: github
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 150
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_B.qmd", output = "../code/un_B.R")
styler:::style_file("../code/un_B.R")
```

::: columns
::: {.column width="40%"}
![](img/razor.jpg) *"Pluralitas non est ponenda sine necessitate."*
William of Ockham
:::

::: {.column width="60%"}
-   In this unit we will cover the following [topics]{.orange}:

    -   Bias-variance trade-off
    -   Cross-validation
    -   Information criteria
    -   Optimism

-   You may have seen these notions before...

-   ...but it is worth discussing the [details]{.orange} of these ideas
    once again, with the maturity you now have in an M.Sc.

-   They are the [foundations]{.blue} of [statistical
    learning]{.orange}.
:::
:::

# Yesterday's and tomorrow's data

## Yesterday's data

::: columns
::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
#| warning: false
library(tidyverse)
library(ggplot2)
library(ggthemes)

rm(list = ls())
# The dataset can be downloaded here: https://tommasorigon.github.io/datamining/data/yesterday.txt
dataset <- read.table("../data/yesterday.txt", header = TRUE)
ggplot(data = dataset, aes(x = x, y = y.yesterday)) +
  geom_point() +
  geom_point(aes(x=2,y=0.51),colour="#fc7d0b") +
  geom_segment(aes(x = 2, xend = 2, y=0.42, yend = 0.51), col = "#fc7d0b", linetype = "dotted") +
  geom_segment(aes(x = 0.48, xend = 2, y=0.51, yend = 0.51), col = "#fc7d0b", linetype = "dotted") +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("x") +
  ylab("y")
```
:::

::: {.column width="50%"}
-   Let us presume that [yesterday]{.orange} we observed $n = 30$ pairs
    of data $(x_i, y_i)$.

-   Data were generated according to $$
      Y_i = f(x_i) + \epsilon_i, \quad i=1,\dots,n,
      $$ with each $y_i$ being the realization of $Y_i$.

-   The $\epsilon_1,\dots,\epsilon_n$ are iid "[error]{.orange}" terms,
    such that $\mathbb{E}(\epsilon_i)=0$ and
    $\text{var}(\epsilon_i)=\sigma^2 = 10^{-4}$.

-   Here $f(x)$ is a regression function ([signal]{.blue}) that we leave
    unspecified.

-   [Tomorrow]{.blue} we will get a new $x$. We wish to
    [predict]{.orange} $Y$ using $\mathbb{E}(Y) = f(x)$.
:::
:::

## Polynomial regression

::: incremental
-   The function $f(x)$ is unknown, therefore, it should be estimated.

-   A simple approach is using the tools of [Unit
    A](unit_A_slides.html), such as [polynomial regression]{.orange}:
    $$
    f(x; \beta) = \beta_1 + \beta_2 x + \beta_3 x^2 + \cdots + \beta_p x^{p-1},
    $$ namely $f(x)$ is [approximated]{.orange} with a polynomial of
    degree $p-1$ (i.e., Taylor expansions).

-   This model is linear in the parameters: ordinary least squares can
    be applied.

-   How do we choose the [degree of the polynomial]{.blue} $p - 1$?

-   Without clear guidance, in principle, any value of
    $p \in \{1,\dots,n\}$ could be appropriate.

-   Let us compare the [mean squared error]{.blue} (MSE) on yesterday's
    data ([training]{.orange}) $$
    \text{MSE}_{\text{train}} = \frac{1}{n}\sum_{i=1}^n\{y_i -f(x_i; \hat{\beta})\}^2,
    $$ or alternatively $R^2_\text{train}$, for different values of
    $p$...
:::

## Yesterday's data, polynomial regression

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center

# Degrees of the polynomials
degree_list <- c(1, 3, 5, 11, 17, 23)

# I am using 30.000 obs to improve the quality of the graph
x_seq <- seq(from = min(dataset$x), to = max(dataset$x), length = 30000) 

# Actual fitting procedure
data_pred <- NULL
for (degree in degree_list) {
  # Fitting a polynomial of degree p - 1
  fit <- lm(y.yesterday ~ poly(x, degree = degree, raw = FALSE), data = dataset)
  # Fitted values
  y_hat <- predict(fit, newdata = data.frame(x = x_seq))
  data_pred <- rbind(data_pred, data.frame(x = x_seq, y_hat = y_hat, degree = paste("Number of parameters p:", degree + 1)))
}

# Graphical adjustment to get the plots in the right order
data_pred$degree <- factor(data_pred$degree)
data_pred$degree <- factor(data_pred$degree, levels = levels(data_pred$degree)[c(3, 5, 6, 1, 2, 4)])

# Final plot
ggplot(data = data_pred) +
  geom_line(aes(x = x, y = y_hat, col = degree)) +
  geom_point(data = dataset, aes(x = x, y = y.yesterday), size = 0.8) +
  theme_light() +
  theme(legend.position = "none") +
  facet_wrap(. ~ degree, ncol = 3) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("x") +
  ylab("y") +
  ylim(c(0.42, 0.56)) # Manual identification of an "interesting" region
```

## Yesterday's data, goodness of fit

```{r}
# Main chunk of code; fitting several models and storing some relevant quantities
degree_list <- 1:23

# Tomorrow's mean-squared error
MSE_tot <- mean((dataset$y.tomorrow - mean(dataset$y.tomorrow))^2)

# Initialization
data_goodness <- data.frame(degree = degree_list, MSE = NA, R_squared = NA, MSE_test = NA, R_squared_test = NA)

# Code execution
for (degree in degree_list) {
  # Fitting a polynomial of degree p -1
  fit <- lm(y.yesterday ~ poly(x, degree = degree, raw = FALSE), data = dataset)
  y_hat <- fitted(fit)
  
  # Training goodness of fit
  data_goodness$MSE[degree] <- mean((dataset$y.yesterday - y_hat)^2)
  data_goodness$R_squared[degree] <- summary(fit)$r.squared
  
  # Test goodness of fit
  data_goodness$MSE_test[degree] <- mean((dataset$y.tomorrow - y_hat)^2)
  data_goodness$R_squared_test[degree] <- 1 - data_goodness$MSE_test[degree] / MSE_tot
}
```

::: columns
::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
ggplot(data = data_goodness, aes(x = degree + 1, y = MSE)) +
  geom_line() +
  geom_point() +
  theme_light() +
  xlab("# of parameters p") +
  ylab("MSE")
```
:::

::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
ggplot(data = data_goodness, aes(x = degree + 1, y = R_squared)) +
  geom_line() +
  geom_point() +
  theme_light() +
  xlab("# of parameters p") +
  ylab(expression(R^2))
```
:::
:::

## Yesterday's data, polynomial interpolation ($p = n$) {#yesterdays-data-polynomial-interpolation-p-n}

```{r}
#| fig-width: 9
#| fig-height: 6
#| fig-align: center
lagrange <- function(x0, y0) {
  f <- function(x) {
    sum(y0 * sapply(seq_along(x0), function(j) {prod(x - x0[-j]) / prod(x0[j] - x0[-j])}))
  }
  Vectorize(f, "x")
}
f <- lagrange(dataset$x, dataset$y.yesterday)

plot(dataset$x, dataset$y.yesterday, pch = 16, xlab = "x", ylab = "y", main = "Degree of the polynomial: n-1")
curve(f(x), n = 300, add = TRUE)
```

## Yesterday's data, tomorrow's prediction

::: incremental
-   The [MSE]{.blue} decreases as the number of parameter increases;
    similarly, the [$R^2$]{.blue} increases as a function of $p$.
    It can be [proved]{.orange} that this [always happens]{.orange} using ordinary least squares.

-   One might be tempted to let $p$ as large as possible to make the
    model more flexible...

-   Taking this reasoning to the extreme would lead to the choice
    $p = n$, so that $$
    \text{MSE}_\text{train} = 0, \qquad R^2_\text{train} = 1,
    $$ i.e., a perfect fit. This procedure is called
    [interpolation]{.blue}.

-   However, we are [not]{.orange} interested in predicting
    [yesterday]{.orange} data. Our goal is to predict
    [tomorrow]{.blue}'s data, i.e. a [new set]{.blue} of $n = 30$
    points: $$
    (x_1, \tilde{y}_1), \dots, (x_n, \tilde{y}_n), 
    $$ using $\hat{y}_i = f(x_i; \hat{\beta})$, where $\hat{\beta}$ is
    obtained using yesterday's data.

-   [Remark]{.orange}. Tomorrow's r.v. $\tilde{Y}_1,\dots, \tilde{Y}_n$
    follow the same scheme as yesterday's data.
:::

## Tomorrow's data, polynomial regression

```{r}
#| fig-width: 8
#| fig-height: 4.5
#| fig-align: center
ggplot(data = data_pred) +
  geom_line(aes(x = x, y = y_hat, col = degree)) +
  geom_point(data = dataset, aes(x = x, y = y.tomorrow), size = 0.8) +
  theme_light() +
  theme(legend.position = "none") +
  facet_wrap(. ~ degree, ncol = 3) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("x") +
  ylab("y") +
  ylim(c(0.42, 0.56))
```

## Tomorrow's data, goodness of fit

::: columns
::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
ggplot(data = data_goodness, aes(x = degree + 1, y = MSE_test)) +
  geom_line() +
  geom_point() +
  theme_light() +
  xlab("# of parameters p") +
  ylab("MSE")
```
:::

::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
ggplot(data = data_goodness, aes(x = degree + 1, y = R_squared_test)) +
  geom_line() +
  geom_point() +
  theme_light() +
  xlab("# of parameters p") +
  ylab(expression(R^2))
```
:::
:::

## Comments and remarks

::: incremental
-   The mean squared error on tomorrow's data ([test]{.blue}) is defined
    as $$
    \text{MSE}_{\text{test}} = \frac{1}{n}\sum_{i=1}^n\{\tilde{y}_i -f(x_i; \hat{\beta})\}^2,
    $$ and similarly the $R^2_\text{test}$. We would like the
    $\text{MSE}_{\text{test}}$ to be [as small as possible]{.orange}.

-   For [small values]{.blue} of $p$, an increase in the degree of the
    polynomial [improves the fit]{.blue}. In other words, at the
    beginning, both the $\text{MSE}_{\text{train}}$ and the
    $\text{MSE}_{\text{test}}$ decrease.

-   For [larger values]{.orange} of $p$, the improvement gradually
    ceases, and the polynomial follows [random fluctuations]{.orange} in
    yesterday's data which are [not observed]{.blue} in the [new
    sample]{.blue}.

-   An over-adaptation to yesterday's data is called
    [overfitting]{.orange}, which occurs when the training
    $\text{MSE}_{\text{train}}$ is low but the test
    $\text{MSE}_{\text{test}}$ is high.

- Yesterday's dataset is available from the textbook (A&S)
    website:

    -   Dataset <http://azzalini.stat.unipd.it/Book-DM/yesterday.dat>
    -   True $f(\bm{x})$
        <http://azzalini.stat.unipd.it/Book-DM/f_true.R>
:::

## ☠️ - Orthogonal polynomials

-   When performing polynomial regression, the `poly` command computes
    an [orthogonal basis]{.orange} of the original covariates
    $(1, x, x^2,\dots,x^{p-1})$ through the QR decomposition:

```{r}
#| echo: true
fit <- lm(y.yesterday ~ poly(x, degree = 3, raw = FALSE), data = dataset)
X <- model.matrix(fit)
colnames(X) = c("Intercept","x1","x2","x3")
round(t(X) %*% X, 8)
```

-   Polynomial regression becomes numerically unstable when $p \ge 13$
    (`raw = TRUE`, i.e., using the original polynomials) and $p \ge 25$
    (`raw = FALSE`, i.e., using orthogonal polynomials).

## ☠️ - Lagrange interpolating polynomials

-   If the previous code does not work for $p \ge 25$, how was the plot
    of [this slide](#yesterdays-data-polynomial-interpolation-p-n)
    computed?

-   It turns out that for $p = n$ there exists an alternative way of
    finding the ordinary least square solution, based on Lagrange
    interpolating polynomials, namely:

$$
\hat{f}(x) = \sum_{i=1}^n\ell_i(x) y_i, \qquad \ell_i(x) = \prod_{k \neq i}\frac{x - x_k}{x_i - x_k}.
$$

-   Interpolating polynomials are clearly [unsuitable]{.orange} for
    regression purposes, but may have interesting applications in other
    contexts.

# Errors, trade-offs, and optimism

## Summary and notation (fixed-$X$)

::: incremental

-   In the previous example, we consider two sets of [random
    variables]{.blue}:

    -   The [training set]{.orange} (yesterday) $Y_1,\dots, Y_n$,
        whose realization is $y_1,\dots,y_n$.
    -   The [test set]{.blue} (tomorrow)
        $\tilde{Y}_1,\dots,\tilde{Y}_n$, whose realization is
        $\tilde{y}_1, \dots, \tilde{y}_n$.

-   The [covariates]{.blue} $\bm{x}_i = (x_{i1},\dots,x_{ip})^T$ in this
    scenario are [deterministic]{.blue}. This is the so-called
    [fixed-$X$]{.blue} design, which is a common assumption in
    regression models.

-   We also assume that the random variables $Y_i$ and $\tilde{Y}_i$
    are [independent]{.blue}. 
    
- In [regression]{.orange} problems we customarily assume that $$
      Y_i = f(\bm{x}_i) + \epsilon_i, \qquad \tilde{Y}_i = f(\bm{x}_i) + \tilde{\epsilon}_i, \quad i=1,\dots,n,
    $$ where $\epsilon_i$ and $\tilde{\epsilon}_i$ are iid
    "[error]{.orange}" terms, with $\mathbb{E}(\epsilon_i)=0$ and
    $\text{var}(\epsilon_i)=\sigma^2$.
    
- In [classification]{.blue} problems the relationship between $\bm{x}_i$ and the [Bernoulli]{.orange} r.v. $Y_i \in \{0, 1\}$ is 
$$
\mathbb{P}(Y_i = 1) = p(\bm{x}_i) =  g\{f(\bm{x}_i)\}, \qquad i=1,\dots,n,
$$
where $g(x) :\mathbb{R} \rightarrow (0,1)$ is monotone transformation, such as the inverse logit.    

:::

## The in-sample prediction error

::: incremental

-   The [training data]{.orange} is used to estimate a function of the
    covariates $\hat{f}(\bm{x}_i)$. We hope our predictions work well on the [test set]{.blue}.


-   A measure of quality for the predictions is the [in-sample
    prediction error]{.blue}: $$
    \text{ErrF} =  \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n \mathscr{L}\{\tilde{Y}_i; \hat{f}(\bm{x}_i)\}\right],
    $$ where $\mathscr{L}\{\tilde{Y}_i; \hat{f}(\bm{x}_i)\}$ is a [loss
    function]{.orange}. The "[F]{.blue}" is a reminder of the
    [f]{.blue}ixed-$X$ design.

-   The expectation is taken with respect to training random variable
    $Y_1,\dots,Y_n$, implicitly appearing in $\hat{f}(\bm{x})$, and the
    new data points $\tilde{Y}_1,\dots,\tilde{Y}_n$.

-   The in-sample prediction error is measuring the [average]{.blue}
    "discrepancy" between the [new data points]{.orange} and the
    corresponding predictions based on the training.

:::

## Loss functions

::: incremental

-   Examples of loss functions for [regression problems]{.blue}
    $Y \in \mathbb{R}$ are:

    -   The [quadratic loss]{.orange}
        $\mathscr{L}\{\tilde{Y}_i; \hat{f}(\bm{x}_i)\} = \{\tilde{Y}_i - \hat{f}(\bm{x}_i)\}^2$,
        leading to the MSE.
    -   The [absolute loss]{.orange}
        $\mathscr{L}\{\tilde{Y}_i; \hat{f}(\bm{x}_i)\} = |\tilde{Y}_i - \hat{f}(\bm{x}_i)|$,
        leading to the MAE.
        
-   Examples of loss functions for [binary classification problems]{.blue}
    $Y \in \{0, 1\}$ are:
    -   The [misclassification loss]{.orange}, which is defined as
        $$
        \mathscr{L}\{\tilde{Y}_i; \hat{f}(\bm{x}_i)\} = \mathbb{I}(\tilde{Y}_i  \neq \hat{y}_i).
        $$
        The predictions are obtained dichotomizing the probabilities $\hat{y}_i = \mathbb{I}(\hat{p}(\bm{x}_i) > 1/2)$.
    -   The [deviance]{.orange} or [cross-entropy]{.orange} loss functions are defined as
        $$
        \mathscr{L}\{\tilde{Y}_i; \hat{f}(\bm{x}_i)\} = -2\left[ \mathbb{I}(Y_i = 1)\log{\hat{p}(\bm{x}_i)} + \mathbb{I}(Y_i = 0)\log{\{1 - \hat{p}(\bm{x}_i)}\}\right].
        $$

:::

## Regression under quadratic loss I

::: callout-note
#### Error decomposition (reducible and irreducible)

In a regression problem, under a quadratic loss, [each element]{.orange} of the [in-sample
prediction error]{.blue} admits the following decomposition $$
\begin{aligned}
\mathbb{E}\left[\{\tilde{Y}_i - \hat{f}(\bm{x}_i)\}^2\right] &= \mathbb{E}\left[\{f(\bm{x}_i) + \tilde{\epsilon}_i - \hat{f}(\bm{x}_i)\}^2\right] \\
& = \mathbb{E}\left[\{f(\bm{x}_i) - \hat{f}(\bm{x}_i)\}^2\right] + \mathbb{E}(\tilde{\epsilon}_i^2) + 2 \: \mathbb{E}\left[\tilde{\epsilon}_i \: \{f(\bm{x}_i) - \hat{f}(\bm{x}_i)\}\right]\\
& = \underbrace{\mathbb{E}\left[\{\hat{f}(\bm{x}_i) - f(\bm{x}_i)\}^2\right]}_{\text{reducible}} + \underbrace{\sigma^2}_{\text{irreducible}},
\end{aligned}
$$ recalling that
$\mathbb{E}(\tilde{\epsilon}_i^2) = \text{var}(\tilde{\epsilon}_i) = \sigma^2$
and for any $i = 1,\dots,n$.
:::

## Regression under quadratic loss II

::: incremental
-   We would like to make the [mean squared error]{.orange} as
    [small]{.orange} as possible, e.g., by choosing an "optimal" degree
    of the polynomial $p-1$ that minimizes it.

-   Let us recall the previous decomposition $$
    \mathbb{E}\left[\{\tilde{Y}_i - \hat{f}(\bm{x}_i)\}^2\right] =  \underbrace{\mathbb{E}\left[\{\hat{f}(\bm{x}_i) - f(\bm{x}_i)\}^2\right]}_{\text{reducible}} + \underbrace{\sigma^2}_{\text{irreducible}}, \quad i=1\dots,n.
    $$

-   The [best case scenario]{.blue} is when the estimated function
    coincides with the mean of $\tilde{Y}_i$, i.e. $$
    \hat{f}(\bm{x}_i) =  f(\bm{x}_i) = \mathbb{E}(\tilde{Y}_i),
    $$ but even in this (overly optimistic) situation, we would still
    commit mistakes, due to the presence of $\tilde{\epsilon}_i$ (unless
    $\sigma^2 = 0$). Hence, the variance $\sigma^2$ is called the
    [irreducible error]{.orange}.

-   Since we do not know $f(\bm{x}_i)$, we seek for an estimate
    $\hat{f}(\bm{x}_i) \approx f(\bm{x}_i)$, in the attempt of
    minimizing the [reducible error]{.blue}.
:::

##  Classification under misclassification loss

::: incremental

- In [classification problems]{.orange}, under a [misclassification loss]{.blue}, the in sample prediction error is
$$
\text{ErrF} = \mathbb{E}\left[\frac{1}{n}\sum_{i=1}^n  \mathscr{L}\{\tilde{Y}_i; \hat{f}(\bm{x}_i)\}\right] = \frac{1}{n}\sum_{i=1}^n  \mathbb{E}\{\mathbb{I}(\tilde{Y}_i \neq \hat{y}_i)\}= \frac{1}{n}\sum_{i=1}^n\mathbb{P}(\tilde{Y}_i \neq \hat{y}_i).
$$
- The above error is [minimized]{.orange} whenever $\hat{y}_i$ corresponds to [Bayes classifier]{.orange}
$$
\hat{y}_{i, \text{bayes}} = \arg\max_{y \in \{0, 1\}} \mathbb{P}(\tilde{Y}_i = y) = \mathbb{I}(p(\bm{x}_i) > 0.5),
$$
which depends on the unknown probabilities $p(\bm{x}_i)$.

- We call the [Bayes rate]{.blue} the optimal in-sample prediction error:
$$
\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^n  \mathscr{L}\{\tilde{Y}_i; p(\bm{x}_i)\}\right] = \frac{1}{n}\sum_{i=1}^n\min\{p(\bm{x}_i), 1 - p(\bm{x}_i)\}.
$$

- The [Bayes rate]{.blue} is the error rate we would get if we knew the true $p(\bm{x})$ and can be regarded as the [irreducible error]{.orange} for classification problems.  

:::

## Bias-variance trade-off

-   In many textbooks, including A&S, the starting point of the analysis
    is the [reducible error]{.blue}, because it is the only one we can
    control and has a transparent interpretation.

-   The reducible error measures the [discrepancy]{.orange} between the
    unknown function $f(\bm{x})$ and its estimate $\hat{f}(\bm{x})$ and
    therefore it is a [natural measure]{.orange} of the goodness of fit.

-   What follows  holds both for [regression]{.blue} and [classification]{.orange} problems.

. . .

::: callout-note
#### Bias-variance decomposition

For any covariate value $\bm{x}$, it holds the following bias-variance
decomposition: $$
\mathbb{E}\left[\{\hat{f}(\bm{x}) - f(\bm{x})\}^2\right] = \underbrace{\mathbb{E}\left[\{\hat{f}(\bm{x}) - f(\bm{x})\}\right]^2}_{\text{Bias}^2} + \underbrace{\text{var}\{\hat{f}(\bm{x})\}}_{\text{variance}}.
$$
:::

## Example: bias-variance in linear regression models

::: incremental
-   In [regression problems]{.blue} the [in-sample prediction error]{.orange} under
    [squared loss]{.blue} is $$
    \begin{aligned}
    \text{ErrF} = \sigma^2 + \frac{1}{n}\sum_{i=1}^n\mathbb{E}\left[\{\hat{f}(\bm{x}_i) - f(\bm{x}_i)\}\right]^2 + \frac{1}{n}\sum_{i=1}^n\text{var}\{\hat{f}(\bm{x}_i)\}.
    \end{aligned}
    $$

-   In [ordinary least squares]{.blue} the above quantity can be
    computed in closed form, since each element of the [bias]{.orange}
    term equals $$
    \mathbb{E}\left[\{\hat{f}(\bm{x}_i) - f(\bm{x}_i)\}\right] = \bm{x}_i^T(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{f} - f(\bm{x}_i).
    $$ where $\bm{f} = (f(\bm{x}_1),\dots,f(\bm{x}_n))^T$. Note that if
    $f(\bm{x}) = \bm{x}^T\beta$, then the bias is zero.

-   Moreover, in [ordinary least squares]{.blue} the [variance]{.orange}
    term equals $$
    \frac{1}{n}\sum_{i=1}^n\text{var}\{\hat{f}(\bm{x}_i)\} = \frac{\sigma^2}{n}\sum_{i=1}^n  \bm{x}_i^T (\bm{X}^T\bm{X})^{-1}\bm{x}_i = \frac{\sigma^2}{n}\text{tr}\{(\bm{X}^T\bm{X})(\bm{X}^T\bm{X})^{-1}\} = \sigma^2 \frac{p}{n}.
    $$
:::

## If we knew $f(x)$...

```{r}
# I am storing this information for simplicity
x <- dataset$x
n <- nrow(dataset)

# The standard deviation of the data generative process is sigma2true = 0.01, declared in the book and the slides
sigmatrue <- 0.01

# The true values have been downloaded from the A&S textbook; see here: http://azzalini.stat.unipd.it/Book-DM/f_true.R
ftrue <- c(
  0.4342, 0.4780, 0.5072, 0.5258, 0.5369,
  0.5426, 0.5447, 0.5444, 0.5425, 0.5397,
  0.5364, 0.5329, 0.5294, 0.5260, 0.5229,
  0.5200, 0.5174, 0.5151, 0.5131, 0.5113,
  0.5097, 0.5083, 0.5071, 0.5061, 0.5052,
  0.5044, 0.5037, 0.5032, 0.5027, 0.5023
)
```

```{r}
# Number of degrees of the polynomial
degree_list <- 1:23
# Number of parameters in the model
p_list <- degree_list + 1

# Squared bias
Bias2s <- sapply(p_list, function(p) {
  mean((ftrue - fitted(lm(ftrue ~ poly(x, degree = p - 1))))^2)
})

# Variance
Vars <- p_list * (sigmatrue^2) / n 

# Reducible errors
MSEs <- Bias2s + Vars

# Organize the data to 
data_bv <- data.frame(p = p_list, Bias = Bias2s, Variance = Vars, MSE = MSEs)
data_bv <- reshape2::melt(data_bv, id = "p")
levels(data_bv$variable) <- c("Squared Bias", "Variance", "Reducible error")
colnames(data_bv) <- c("p", "Error term", "value")
```

```{r}
#| fig-width: 8
#| fig-height: 4.5
#| #| fig-align: center
ggplot(data = data_bv, aes(x = p, y = value, col = `Error term`)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = 6, linetype = "dotted") +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Model complexity (p)") +
  ylab("Error")
```

## Bias-variance trade-off

::: incremental
-   When $p$ grows, the mean squared error first decreases and then it
    increases. In the example, the [theoretical optimum]{.orange} is
    $p = 6$ (5th degree polynomial).

-   The [bias]{.orange} measures the ability of $\hat{f}(\bm{x})$ to
    reconstruct the true $f(\bm{x})$. The bias is due to [lack of
    knowledge]{.blue} of the data-generating mechanism. It equals zero
    when $\mathbb{E}\{\hat{f}(\bm{x})\} = f(\bm{x})$.

-   The [bias]{.orange} term can be reduced by increasing the
    flexibility of the model (e.g., by considering a high value for
    $p$).

-   The [variance]{.blue} measures the variability of the estimator
    $\hat{f}(\bm{x})$ and its tendency to follow random fluctuations of
    the data.

-   The [variance]{.blue} increases with the model complexity.

-   It is not possible to minimize both the bias and the variance, there
    is a [trade-off]{.orange}.

-   We say that an estimator is [overfitting]{.orange} the data if an
    increase in variance comes without important gains in terms of bias.

<!-- - **Summary**. Low model complexity: [high bias]{.orange}, [low variance]{.blue}. High model complexity: [low bias]{.blue}, [high variance]{.orange}. -->
:::

## But since we do not know $f(x)$...

::: incremental
-   We just concluded that we must expect a trade-off between error and
    variance components. In practice, however, we cannot do this
    because, of course, $f(x)$ is [unknown]{.orange}.

-   A simple solution consists indeed in [splitting]{.orange} the
    observations in two parts: a [training set]{.orange}
    $(y_1,\dots,y_n)$ and a [test set]{.blue}
    $(\tilde{y}_1,\dots,\tilde{y}_n)$, having the same covariates
    $x_1,\dots,x_n$.

-   We fit the model $\hat{f}$ using $n$ observations of the training
    and we use it to predict the $n$ observations on the test set.

-   This leads to an [unbiased estimate]{.orange} of the [in-sample
    prediction error]{.blue}, i.e.: $$
    \widehat{\mathrm{ErrF}} =  \frac{1}{n}\sum_{i=1}^n\mathscr{L}\{\tilde{y}_i; \hat{f}(\bm{x}_i)\}.
    $$

-   This is precisely what we already did with yesterday's and tomorrow's
    data!
:::

## MSE on training and test set (recap)

```{r}
#| fig-width: 8
#| fig-height: 4.5
#| fig-align: center

data_bv <- data.frame(p = p_list, #MSE = sigmatrue^2 + Bias2s + Vars, 
                      MSE_train = data_goodness$MSE, MSE_test = data_goodness$MSE_test)
data_bv <- reshape2::melt(data_bv, id = "p")
levels(data_bv$variable) <- c("MSE train (yesterday's data)", "MSE test (tomorrow's data)")
colnames(data_bv) <- c("p", "Error term", "value")

ggplot(data = data_bv, aes(x = p, y = value, col = `Error term`)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = 5, linetype = "dotted") +
  theme_light() +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Model complexity (p)") +
  ylab("Error")
```

## Optimism I

::: incremental

- Let us investigate this discrepancy between training and test more in-depth.

-   In [regression problems]{.blue}, under a [squared loss function]{.blue}, the [in-sample prediction
    error]{.orange} is $$
    \text{ErrF} = \mathbb{E}(\text{MSE}_\text{test}) = \frac{1}{n}\sum_{i=1}^n\mathbb{E}\left[\{\tilde{Y}_i - \hat{f}(\bm{x}_i)\}^2\right]
    $$

-   Similarly, the [in-sample training error]{.orange} can be defined as
    follows $$
    \mathbb{E}(\text{MSE}_\text{train}) = \frac{1}{n}\sum_{i=1}^n\mathbb{E}\left[\{Y_i - \hat{f}(\bm{x}_i)\}^2\right].
    $$

-   We already know that $\mathbb{E}(\text{MSE}_\text{train})$ provides
    a very optimistic assessment of the model performance. For example
    when $p = n$ then $\mathbb{E}(\text{MSE}_\text{train}) = 0$.

-   We call [optimism]{.orange} the difference between these two
    quantities: $$
    \text{Opt} = \mathbb{E}(\text{MSE}_\text{test}) - \mathbb{E}(\text{MSE}_\text{train}).
    $$
:::

## Optimism II

::: incremental
-   It can be proved (see Exercises) that the [optimism]{.orange} has a
    very simple form: $$
    \text{Opt} = \frac{2}{n}\sum_{i=1}^n\text{cov}(Y_i, \hat{f}(\bm{x}_i)) = \frac{2}{n} \text{tr}\{\text{cov}(\bm{Y}, \hat{\bm{f}})\}.
    $$

-   If the predicted values are obtained using [ordinary least
    squares]{.blue}, then $\hat{\bm{f}} = \bm{H}\bm{Y}$, therefore $$
    \text{Opt}_\text{ols} = \frac{2}{n} \text{tr}\{\text{cov}(\bm{Y}, \bm{H}\bm{Y})\} = \frac{2}{n} \text{tr}\{\text{cov}(\bm{Y}, \bm{Y}) \bm{H}^T\} = \frac{2 \sigma^2}{n}\text{tr} \{\bm{H}\} = \frac{2 \sigma^2 p}{n}.
    $$

-   This leads to an estimate for the in-sample prediction error, known
    as [$C_p$ of Mallows]{.blue}: $$
    \widehat{\mathrm{ErrF}} = \text{MSE}_\text{train} + \text{Opt}_\text{ols} = \frac{1}{n}\sum_{i=1}^n\{y_i - \hat{f}(\bm{x}_i)\}^2 + \frac{2 \sigma^2 p}{n}.
    $$

-   If $\sigma^2$ is unknown, then it must be [estimated]{.orange} using
    for instance $s^2$.
:::

## Optimism III

```{r}
# Code execution and storage of the interesting quantities
for (degree in degree_list) {
  # Fitting a polynomial of degree p -1
  fit <- lm(y.yesterday ~ poly(x, degree = degree, raw = FALSE), data = dataset)
  data_goodness$C_p[degree] <- mean((dataset$y.yesterday - fitted(fit))^2) + 2 * summary(fit)$sigma^2 * (degree +  1) / nrow(dataset)
}

# Organization of the results for graphical purposes
data_bv <- data.frame(p = p_list, MSE_train = data_goodness$MSE, C_p = data_goodness$C_p)
data_bv <- reshape2::melt(data_bv, id = "p")
levels(data_bv$variable) <- c("MSE train (yesterday's data)", "C_p")
colnames(data_bv) <- c("p", "Error term", "value")
```

```{r}
#| fig-width: 8
#| fig-height: 4.5
#| fig-align: center
#| warning: false
ggplot(data = data_bv, aes(x = p, y = value, col = `Error term`)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = 6, linetype = "dotted") +
  theme_light() +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Model complexity (p)") +
  ylab(expression(C[p])) + xlim(c(2, 15))
```

# Cross-validation

## Another example: `cholesterol` data

::: columns
::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
#| warning: false
# rm(list = ls())
# The dataset can be downloaded here: https://tommasorigon.github.io/datamining/data/cholesterol.txt
dataset <- read.table("../data/cholesterol.txt", header = TRUE)
ggplot(data = dataset, aes(x = compliance, y = cholesterol.decrease)) +
  geom_point() +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Compliance") +
  ylab("Cholesterol Decrease")
```
:::

::: {.column width="50%"}
-   A drug called "cholestyramine" is administered to $n = 164$ men.

-   For each man, we observe the pair $(x_i, y_i)$.

-   The response $y_i$ is the [decrease in cholesterol level]{.orange}
    over the experiment.

-   The covariate $x_i$ is a measure of [compliance]{.blue}.

-   We assume, as before, that the data are generated according to $$
        Y_i = f(x_i) + \epsilon_i, \quad i=1,\dots,n.
        $$

-   The original data can be [found
    here](https://hastie.su.domains/CASI_files/DATA/cholesterol.txt).
:::
:::

## Summary and notation (random-$X$)

::: incremental
-   A slight change to the previous setup is necessary. In fact, there
    are no reasons to believe that the [compliance]{.blue} is a fixed
    covariate.

-   We consider a set of iid [random variables]{.blue}
    $(X_1, Y_1),\dots, (X_n, Y_n)$, whose realization is
    $(\bm{x}_1,y_1),\dots,(\bm{x}_n, y_n)$. This time the covariates are
    [random]{.orange}.

-   The main assumption is that these pairs are [iid]{.orange}, namely:
    $$(X_i, Y_i) \overset{\text{iid}}{\sim} \mathcal{P}, \quad  i=1,\dots,n.$$

-   Conditionally on $X_i = \bm{x}_i$, in [regression problems]{.orange}
    we let as before $$
      Y_i = f(\bm{x}_i) + \epsilon_i, \quad i=1,\dots,n,
    $$ where $\epsilon_i$ are iid "[error]{.orange}" terms with
    $\mathbb{E}(\epsilon_i)=0$ and $\text{var}(\epsilon_i)=\sigma^2$.
:::

## Expected prediction error

::: incremental
-   In this setting with random covariates, we want to minimize the
    [expected prediction error]{.orange}: $$
    \text{Err} =  \mathbb{E}\left[\mathscr{L}\{\tilde{Y}; \hat{f}(\tilde{X})\}\right],
    $$ where $(\tilde{X},\tilde{Y}) \sim \mathcal{P}$ is a [new data
    point]{.blue} and $\hat{f}$ is an estimate using $n$ observations.

-   We can [randomly]{.orange} split the original set of data
    $\{1,\dots,n\}$ into two groups $V_\text{train}$ and
    $V_\text{test}$.

-   We call $\hat{f}_\text{train}$ the estimate based on the data in
    $V_\text{train}$.

-   Then, we obtain a (slightly biased) estimate of $\text{Err}$ by
    using the empirical quantity: $$
      \widehat{\mathrm{Err}} =  \frac{1}{|V_\text{test}|}\sum_{i \in V_\text{test}}\mathscr{L}\{\tilde{y}_i; \hat{f}_\text{train}(\bm{x}_i)\}.
      $$

-   The data-splitting strategy we used before is an effective tool for
    assessing the error. However, its [interpretation]{.blue} is
    changed: we are now estimating $\text{Err}$ and not $\text{ErrF}$.
:::

## MSE on training and test (`cholesterol` data)

```{r}
# Main chunk of code; fitting several models and storing some relevant quantities
degree_list <- 1:14
p_list <- degree_list + 1

# Data splitting
set.seed(123)
id_train <- sample(1:nrow(dataset), size = floor(0.75 * nrow(dataset)), replace = FALSE)
id_test <- setdiff(1:nrow(dataset), id_train)
dataset_train <- dataset[id_train, ]
dataset_test <- dataset[id_test, ]

# Initialization
data_goodness <- data.frame(degree = degree_list, MSE = NA, MSE_test = NA)

# Code execution
for (degree in degree_list) {
  # Fitting a polynomial of degree p -1
  fit <- lm(cholesterol.decrease ~ poly(compliance, degree = degree, raw = FALSE), 
            data = dataset_train)
  y_hat_train <- fitted(fit)
  y_hat_test <- predict(fit, newdata = dataset_test)
  
  # Training goodness of fit
  data_goodness$MSE[degree] <- mean((dataset_train$cholesterol.decrease - y_hat_train)^2)

  # Test goodness of fit
  data_goodness$MSE_test[degree] <- mean((dataset_test$cholesterol.decrease - y_hat_test)^2)
}
```

```{r}
#| fig-width: 8
#| fig-height: 4.5
#| fig-align: center

data_bv <- data.frame(p = p_list, #MSE = sigmatrue^2 + Bias2s + Vars, 
                      MSE_train = data_goodness$MSE, MSE_test = data_goodness$MSE_test)
data_bv <- reshape2::melt(data_bv, id = "p")
levels(data_bv$variable) <- c("MSE train", "MSE test")
colnames(data_bv) <- c("p", "Error term", "value")

ggplot(data = data_bv, aes(x = p, y = value, col = `Error term`)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = 3, linetype = "dotted") +
  theme_light() +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Model complexity (p)") +
  ylab("Error")
```

## Training, validation, and test I

::: incremental
- On many occasions, we may need to select several complexity
    parameters and compare hundreds of models.

-   If the same test set is used for such a task, the
    final assessment of the error is somewhat biased and [too
    optimistic]{.orange}, because we are "learning" from the test set.

-   If we are in a data-rich situation, the best approach is to divide the dataset into three parts randomly:

    -   a [training set]{.orange}, used for [fitting]{.orange} the
        models;
    -   a [validation set]{.blue}, used to estimate prediction error and
        perform [model selection]{.blue};
    -   a [test set]{.grey}, for [assessment of the error]{.grey} of the
        final chosen model.

-   Ideally, the test set should be kept in a "vault" and be brought
    out only at the end of the data analysis.
:::

## Training, validation, and test II

-   There is no precise rule on how to select the size of these sets;
    a rule of thumb is given in the picture below.

::: {.flourish-embed .flourish-chart data-src="visualisation/14246499"}
```{=html}
<script src="https://public.flourish.studio/resources/embed.js"></script>
```
:::

-   The training, validation, and test setup [reduces]{.orange} the
    [number of observations]{.orange} we can use to fit the
    models. It could be problematic if the sample size is relatively
    small.

## Cross-validation I

-   A way to partially overcome the loss of efficiency of the training /
    test paradigm consists in [randomly splitting the data]{.orange}
    $\{1,\dots,n\}$ in equal parts, say $V_1,\ldots,V_K$.

-   In the [$K$-fold cross-validation]{.blue} method we use the
    observations $i \notin V_k$ to train the model and the remaining
    observations $i \in V_k$ to assess to perform model selection.

-   In the following scheme we let $K = 5$.

::: {.flourish-embed .flourish-hierarchy data-src="visualisation/14247663"}
```{=html}
<script src="https://public.flourish.studio/resources/embed.js"></script>
```
:::

## Cross-validation II

::: incremental
-   In the [$K$-fold cross validation]{.blue} we compute for each fold
    $k$ we fit a model $\hat{f}_{-V_k}(\bm{x})$ without using the
    observations of $V_k$.

-   Hence, the model must be estimated [$K$ times]{.orange}, which could
    be computationally challenging.

-   The error of each on the $k$th folds is computed as
    $$\widehat{\text{Err}}_{V_k} = \frac{1}{|V_k|} \sum_{i \in V_k} \mathscr{L}\{y_i; \hat{f}_{-V_k}(\bm{x}_i)\},
    $$ where $|V_k|$ is the cardinality of $V_k$, i.e.
    $V_k \approx n / K$.

-   We summarize the above errors using the mean, obtaining
    the following [estimate]{.orange} for the [expected prediction
    error]{.orange}:
    $$\widehat{\mathrm{Err}} = \frac{1}{K} \sum_{k=1}^{K} \widehat{\text{Err}}_{V_k}= \frac{1}{K} \sum_{k=1}^{K}\left[ \frac{1}{|V_k|} \sum_{i \in V_k} \mathscr{L}\{y_i; \hat{f}_{-V_k}(\bm{x}_i)\} \right].$$
:::

## Cross-validation III

::: incremental

- An advantage of CV is that [variance]{.blue} of the Monte Carlo estimate $\widehat{\mathrm{Err}}$ can be quantified.

- Let us define cross-validated "[residuals]{.orange}" of our procedure as follows
$$
r_i = \mathscr{L}\{y_i; \hat{f}_{-V_k}(\bm{x}_i)\}, \qquad i=1,\dots,n.
$$
so that $\widehat{\text{Err}} = \bar{r}$. 

- Then, a simple estimate for the standard error of $\widehat{\text{Err}}$ is
$$
\widehat{\text{se}} = \frac{1}{\sqrt{n}} \text{sd}(r) = \frac{1}{\sqrt{n}} \sqrt{\frac{1}{n-1}\sum_{i=1}^n (r_i - \bar{r})^2}.
$$

- The above formula is often criticized for producing intervals that are [too narrow]{.orange}!

- Indeed, the estimate $\widehat{\text{se}}$ of the standard deviation of $\widehat{\text{Err}}$ assumes that the observed errors $r_1, \dots, r_n$ are independent, but this is not true! 

:::

## Cross-validation IV (`cholesterol` data)

```{r}
#| message: false
#| warning: false

library(tidymodels)
source("../code/mse_yardstick.R")

control_settings <- control_grid(save_pred = TRUE, verbose = TRUE)
rec <- recipe(cholesterol.decrease ~ compliance, data = dataset) %>% step_poly(compliance, degree = tune())
m_lm_grid <- tibble(degree = degree_list)

m_lin <- linear_reg() %>% set_engine("lm")
wf_lin <- workflow() %>% add_model(m_lin) %>% add_recipe(rec)

# Define the cross-validation setup
set.seed(123)
CV_splits <- vfold_cv(dataset, v = 10)

# Fitting the various models - this takes some time!
fit_lin_cv <- wf_lin %>% tune_grid(resamples = CV_splits, 
                                metrics = metric_set(mse),
                                grid = m_lm_grid, control = control_settings)
```

```{r}
#| fig-width: 8
#| fig-height: 4.5
#| fig-align: center
data_bv <- collect_metrics(fit_lin_cv) %>% select(degree, mean, std_err) %>% mutate(p = degree + 1, `Error term` = "10-fold MSE")
colnames(data_bv) <- c("degree","value", "se", "p", "Error term")
ggplot(data = data_bv, aes(x = p, y = value, col = `Error term`)) +
  geom_linerange(aes(ymin = value - se, ymax = value + se)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = 2, linetype = "dotted") +
  theme_light() +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Model complexity (p)") +
  ylab("Mean Squared Error (MSE)")
```

## Leave-one-out cross-validation

-   The maximum possible value for $K$ is $n$, the
    [leave-one-out]{.orange} cross-validation (LOO-CV).

-   The LOO-CV is hard to implement because it requires the estimation
    of $n$ different models.

-   However, in [linear models]{.blue} there is a brilliant
    [computational shortcut]{.orange} that we can exploit.

. . .

::: callout-note
#### LOO-CV (Linear models)

Let $\hat{y}_{-i} = \bm{x}_i^T\hat{\beta}_{-i}$ be the leave-one-out
predictions of a [linear model]{.blue} and let $h_i = [\bm{H}]_{ii}$ and
$\hat{y}_i$ be the leverages and the predictions of the
full model. Then: $$
y_i - \hat{y}_{-i} = \frac{y_i - \hat{y}_i}{1 - h_i}, \qquad i=1,\dots,n.
$$ Therefore, the leave-one-out mean squared error is
$$\widehat{\mathrm{Err}} = \frac{1}{n} \sum_{i=1}^{n} \widehat{\text{Err}}_{V_i}= \frac{1}{n}\sum_{i=1}^n \left(\frac{y_i - \hat{y}_i}{1 - h_i}\right)^2.$$
:::

## Generalized cross-validation

::: incremental
-   An alternative to LOO-CV, sometimes used in more complex scenarios,
    is the so-called [generalized cross validation]{.orange} (GCV),
    defined as $$
    \text{GCV} = \widehat{\mathrm{Err}} = \frac{1}{n}\sum_{i=1}^n \left(\frac{y_i - \hat{y}_i}{1 - p/n}\right)^2.
    $$
-   The GCV is an approximate LOO-CV, in which the leverages $h_i$ are
    replaced by their mean: $$
    \frac{1}{n}\sum_{i=1}^n h_i = \frac{p}{n}. 
    $$
-   For small $x >0$ it holds that $(1 - x)^{-2} \approx 1 + 2x$. Then,
    we will write $$
    \text{GCV} \approx \frac{1}{n}\sum_{i=1}^n(y_i - \hat{f}(\bm{x}_i))^2 + 2 \hat{\sigma}^2 p, \qquad \hat{\sigma}^2 =\frac{1}{n}\sum_{i=1}^n(y_i - \hat{f}(\bm{x}_i))^2,
    $$ revealing a sharp connection with the $C_p$ of Mallows.
:::

## LOO-CV and GCV (`cholesterol` data)

```{r}
# Code execution and storage of the interesting quantities
data_goodness <- data.frame(degree = degree_list)
for (degree in degree_list) {
  # Fitting a polynomial of degree p -1
  fit <- lm(cholesterol.decrease ~ poly(compliance, degree = degree, raw = FALSE), data = dataset)
  # Computation of the leverages h_i efficiently (using QR)
  lev <- influence(fit)$hat
  res_loo <- (dataset$cholesterol.decrease - fitted(fit)) / (1 - lev)
  data_goodness$LOO_CV[degree] <- mean(res_loo^2)
  data_goodness$LOO_CV_SE[degree] <- sd(res_loo) / sqrt(nrow(dataset))
  p <- degree + 1
  data_goodness$GCV[degree] <- mean(((dataset$cholesterol.decrease - fitted(fit)) / (1 - p / nrow(dataset)))^2)
  data_goodness$AICc[degree] <- AIC(fit) + 2 * p * (p + 1) / (nrow(dataset) - p - 1)
  data_goodness$AIC[degree] <- AIC(fit)
  data_goodness$BIC[degree] <- BIC(fit)
}

# Organization of the results for graphical purposes
data_bv <- data.frame(p = p_list, GCV = data_goodness$GCV, LOO_CV = data_goodness$LOO_CV, SE = data_goodness$LOO_CV_SE)
data_bv <- reshape2::melt(data_bv, id = c("p", "SE"))
data_bv$SE[data_bv$variable == "GCV"] <- NA
levels(data_bv$variable) <- c("GCV", "LOO-CV")
colnames(data_bv) <- c("p", "SE", "Error term", "value")
```

```{r}
#| fig-width: 8
#| fig-height: 4.5
#| fig-align: center
ggplot(data = data_bv, aes(x = p, y = value, col = `Error term`)) +
  geom_line() +
  # geom_linerange(aes(ymin = value - SE, ymax = value + SE)) +
  geom_point() +
  geom_vline(xintercept = 4, linetype = "dotted") +
  theme_light() +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Model complexity (p)") +
  ylab("Mean Squared Error (MSE)") #+ ylim(c(9e-05, 6e-4))
```

## On the choice of $K$

::: incremental
-   Common choices are $K = 5$ or $K = 10$. It is quite evident that a
    [larger $K$]{.blue} requires more [computations]{.blue}.

-   A $K$-fold CV with $K=5$ or $K = 10$ is a (upwords) [biased
    estimate]{.orange} of $\text{Err}$ because it uses less observations
    than those available (either $4/5$ or $9/10$).

-   The LOO-CV has a very [small bias]{.blue}, since each fit uses $n-1$
    observations, but it has [high variance]{.orange}, being the average
    of $n$ highly positively correlated quantities.

-   Indeed, the estimates $\hat{f}_{-i}$ and $\hat{f}_{-i'}$ have
    $n - 2$ observations in common. Recall that the variance of the sum
    is:
    $$\text{var}(X + Y) = \text{var}(X) + \text{var}(Y) + 2\text{cov}(X, Y).$$

-   Overall, the choice is very much context-dependent.
:::

<!-- ## ☠️ - Optimism, random-$X$ -->

<!-- -   What is a good definition and estimate of the [optimism]{.orange} in -->

<!--     the very much realistic scenario where the covariates are random? -->

<!-- ![](img/randomX.png){fig-align="center"} -->

# Information criteria

## Goodness of fit with a penalty term

::: incremental
-   The main statistical method for estimating unknown parameters of a
    model is the [maximize the log-likelihood]{.blue}
    $\ell(\theta) = \ell(\theta; y_1,\dots,y_n)$.

-   However, we cannot pick the value of $p$ that maximizes the
    log-likelihood (why not?)

-   We must consider the different number of parameters,
    introducing a [penalty]{.orange}: $$
    \text{IC}(p) = -2 \ell(\hat{\theta}) + \text{penalty}(p),
    $$

-   The $\text{IC}$ is called an [information criterion]{.orange}. We
    select the number of parameters that minimizes the $\text{IC}$.

-   The choice of the specific penalty identifies a particular
    criterion.

-   An advantage of $\text{IC}$ is that they are based on the full dataset.
:::

## The Akaike information criterion I

::: incremental

-   Akaike suggested minimizing over $p$ the [expectation]{.blue} of the [Kullback-Leibler
    divergence]{.orange}: $$
    \text{KL}(p(\cdot; \theta_0) \mid\mid p(\cdot;\hat{\theta})) = \int p(\tilde{\bm{Y}};\theta_0) \log{p(\tilde{\bm{Y}};\theta_0)}\mathrm{d}\tilde{\bm{Y}} - \int p(\tilde{\bm{Y}};\theta_0)\log{p(\tilde{\bm{Y}};\hat{\theta})\mathrm{d}\tilde{\bm{Y}}},
    $$ between the "true" model $p(\bm{Y};\theta_0)$ with parameter $\theta_0$ and the estimated model $p(\bm{Y};\hat{\theta})$.

-   In the above Kullback-Leibler, for any fixed $p$, the parameter $\theta$ is replaced with its [maximum likelihood estimator]{.blue} $\hat{\theta} = \hat{\theta}(\bm{Y})$, using the data $\bm{Y} = (Y_1,\dots,Y_n)$.
    
- [Equivalently]{.orange}, we can select $p$ such that the expectation w.r.t. $p(\bm{Y}; \theta_0)$
$$
\begin{aligned}
\Delta(p) &= 2 \: \mathbb{E}_{\theta_0}\left[\text{KL}(p(\cdot; \theta_0) \mid\mid p(\cdot;\hat{\theta}))\right] - \underbrace{2 \int p(\tilde{\bm{Y}};\theta_0) \log{p(\tilde{\bm{Y}};\theta_0)}\mathrm{d}\tilde{\bm{Y}}}_{\text{Does not depend on } p} \\
& = - 2 \: \mathbb{E}_{\theta_0}\left[\int p(\tilde{\bm{Y}};\theta_0)\log{p(\tilde{\bm{Y}};\hat{\theta})\mathrm{d}\tilde{\bm{Y}}} \right] 
\end{aligned}
$$
is [minimized]{.orange}. Unfortunately, we cannot compute nor minimize $\Delta(p)$ because $\theta_0$ is unknown. 

:::

## The Akaike information criterion II

::: incremental

- The theoretical quantity $\Delta(p)$ cannot be obtained. However, the quantity $$
\text{AIC} = -2 \ell(\hat{\theta}) + 2 p,
$$
namely the [Akaike information criterion]{.orange}, is a good estimator of $\Delta(p)$.

- More formally, it can be proved that, under technical conditions:
$$
\mathbb{E}_{\theta_0}(\text{AIC}) + o(1) = \Delta(p),
$$
for $n \rightarrow \infty$. 


- In practice, we will select the value of $p$ minimizing the $\text{AIC}$, which is typically quite easy. 

- The factor $2$ is just a [convention]{.blue}, introduced to match the quantities of the usual asymptotic theory. 


:::

## The AIC for Gaussian linear models

::: incremental

- Let us assume that $\sigma^2$ is [known]{.orange}. Then the $\text{AIC}$ for a Gaussian linear model is
$$
\begin{aligned}
\text{AIC} &= -2 \ell(\hat{\beta}) + 2 p = -2 \left\{-\frac{n}{2}\log{(2\pi\sigma^2)} - \frac{1}{2
\sigma^2}\sum_{i=1}^n (y_i - \bm{x}_i^T\hat{\beta})^2\right\} + 2p \\
& = n\log{(2\pi\sigma^2)} + \frac{n}{
\sigma^2}\left\{ \frac{1}{n}\sum_{i=1}^n (y_i - \bm{x}_i^T\hat{\beta})^2 + \frac{2p\sigma^2}{n} \right\} \\
& = n\log{(2\pi\sigma^2)} + \frac{n}{
\sigma^2} C_p,
\end{aligned}
$$
implying that for fixed values $\sigma^2$ the $C_p$ of Mallows and the Akaike's $\text{AIC}$ are [equivalent]{.blue}, i.e. they lead to the same [minimum]{.orange}. 

- When $\sigma^2$ is unknown, then it is estimated and the $C_p$ and $\text{AIC}$ may be slightly different.

:::

## AIC, AICc, BIC

- Several other proposals followed Akaike's original work, differing in their assumptions and the way they approximate certain quantities.


| Criterion      | Author                | Penalty                           |
|----------------|-----------------------|-----------------------------------|
| $\text{AIC}$   | Akaike                | $2p$                              |
| $\text{AIC}_c$ | Sugiura, Hurvich-Tsay | $2p + \frac{2p(p+1)}{n - (p +1)}$ |
| $\text{BIC}$   | Akaike, Schwarz       | $p \log{n}$                       |


-   The $\text{AIC}_c$ is an [higher order correction]{.orange} of the
    $\text{AIC}$ and the differences tend to be negligible for high
    values of $n$.


- The justification of $\text{BIC}$ is comes from [Bayesian statistics]{.blue}.

-   Since $\log{n} > 2$ for any $n > 7$, it means that the $\text{BIC}$
    [penalty]{.blue} is typically [stronger]{.orange} than the one of
    $\text{AIC}$ and it favors more parsimonious models. 

## AIC and BIC (`cholesterol` data)



::: columns
::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5

# Organization of the results for graphical purposes
data_bv <- data.frame(p = p_list, AIC = data_goodness$AIC, AICc = data_goodness$AICc)
data_bv <- reshape2::melt(data_bv, id = "p")
levels(data_bv$variable) <- c("AIC", "AICc")
colnames(data_bv) <- c("p", "Criterion", "value")

ggplot(data = data_bv, aes(x = p, y = value, col = Criterion)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = 4, linetype = "dotted") +
  theme_light() +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Model complexity (p)") +
  ylab("Information Criterion (IC)")
```
:::

::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
# Organization of the results for graphical purposes
data_bv <- data.frame(p = p_list, BIC = data_goodness$BIC)
data_bv <- reshape2::melt(data_bv, id = "p")
levels(data_bv$variable) <- c("BIC")
colnames(data_bv) <- c("p", "Criterion", "value")

ggplot(data = data_bv, aes(x = p, y = value, col = Criterion)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = 2, linetype = "dotted") +
  theme_light() +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Model complexity (p)") +
  ylab("Information Criterion (IC)")
```
:::
:::


## An optimistic summary

::: incremental

- In the `cholesterol` dataset, the various indices produced [different results]{.orange}!
  - The BIC and  and the $10-$fold cross-validation selected $p = 2$ (linear model); 
  - The the training / test split suggested $p = 3$ (quadratic model); 
  - All the others (LOO-CV, $\text{GCV}$, $\text{AIC}$ and $\text{AIC}_c$) concluded that $p = 4$ (cubic model). 

- The good news is that all the above methods produced [similar findings]{.blue}. For example, we are quite sure that we should choose $p \le 6$. 

- On the other hand, there is obviously some [uncertainty]{.orange}, which is a quite common situation. 

- In this specific case, we may prefer $p = 4$, since it is based on the less-biased estimates of $\text{Err}$, such as the LOO-CV. 

- However, this choice is [debatable]{.blue}: another statistician may prefer the simpler linear model with $p =2$. 

:::

## The `cholesterol` data: final model ($p = 4$)

```{r}
#| fig-width: 8
#| fig-height: 4.5
#| fig-align: center
fit <- lm(cholesterol.decrease ~ poly(compliance, degree = 3, raw = FALSE), data = dataset)
dataset$fitted <- fitted(fit)

ggplot(data = dataset, aes(x = compliance, y = cholesterol.decrease)) +
  geom_point(size = 0.8) + geom_line(aes(x = compliance, y = fitted), col = "black") + 
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Compliance") +
  ylab("Cholesterol Decrease")
```

# References

-   [Main references]{.blue}
    -   **Chapter 3** of Azzalini, A. and Scarpa, B. (2011), [*Data
        Analysis and Data
        Mining*](http://azzalini.stat.unipd.it/Book-DM/), Oxford
        University Press.
    -   **Chapter 7** of Hastie, T., Tibshirani, R. and Friedman, J.
        (2009), [*The Elements of Statistical
        Learning*](https://hastie.su.domains/ElemStatLearn/), Second
        Edition, Springer.
-   [Advanced references]{.orange}
    -   Rosset, S., and R. J. Tibshirani (2020). "[From fixed-X to
        random-X regression: bias-variance decompositions, covariance
        penalties, and prediction error
        Estimation](https://doi.org/10.1080/01621459.2018.1424632)."
        *Journal of the American Statistical Association* **115** (529):
        138--51.
    -   Bates, S., Hastie, T., and R. Tibshirani (2023).
        "[Cross-validation: what does it estimate and how well does it
        do it?](https://doi.org/10.1080/01621459.2023.2197686)" *Journal of the American Statistical Association*, in press.
