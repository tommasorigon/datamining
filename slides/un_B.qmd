---
title: "Optimism, Conflicts, and Trade-offs"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
execute:
  cache: true
format:
  revealjs:
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_B_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 150
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
    highlight-style: ayu
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: true
    code-summary: "Show the code"
    fig-dpi: 150
    highlight-style: ayu
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

::: columns
::: {.column width="40%"}
![](img/razor.jpg) *"Pluralitas non est ponenda sine necessitate."* William of Ockham
:::

::: {.column width="60%"}
-   In this unit we will cover the following [topics]{.orange}:

    - Bias-variance trade-off
    - Cross-validation
    - Information criteria
    - Optimism

-   You may have seem some of these notions before... 
- ...but it is worth discussing the details of these ideas once again, with the maturity you now have in a M.Sc. 

- They are the [foundations]{.blue} of [statistical learning]{.orange}.
:::
:::

## Yesterday's data


:::columns

::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
library(tidyverse)
library(ggplot2)
library(ggthemes)
dataset <- read.table("../data/yesterday.txt", header = TRUE)
ggplot(data = dataset, aes(x = x, y = y.yesterday)) +
  geom_point() +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("x") +
  ylab("y")
```
:::
::: {.column width="50%"}
- Let us presume that [yesterday]{.orange} we observed $n = 30$ pairs of data $(x_i, y_i)$.

- Data were generated according to $$
    y_i = f(x_i) + \epsilon_i, \quad i=1,\dots,n.
    $$
- The $\epsilon_1,\dots,\epsilon_n$ are iid "[error]{.orange}" terms, such that $\mathbb{E}(\epsilon_i)=0$ and $\text{var}(\epsilon_i)=\sigma^2 = 10^{-2}$. 

- Here $f(x)$ is a regression function ([signal]{.blue}) that we leave unspecified.

- Suppose [tomorrow]{.blue} we get a new $x_{n+1}$. We wish to [predict]{.orange} $y_{n+1}$ using $f(x_{n+1}; \beta)$.

:::
::: 

## Polynomial regression

::: incremental

- The function $f(x)$ is unknown, therefore it should be estimated. 

- A simple approach is using the tools of [Unit A.1](unit_A1_slides.html), such as [polynomial regression]{.orange}:
$$
f(x) = f(x; \beta) = \beta_1 + \beta_2 x + \beta_3 x^2 + \cdots + \beta_p x^{p-1},
$$
i.e. $f(x; \beta)$ is [approximated]{.orange} with a polynomial of degree $p-1$ (Taylor expansions).

- This model is linear in the parameters, therefore we can use ordinary least squares.

- How do we choose the [degree of the polynomial]{.blue} $p - 1$?

- Without have any clear guideline, at least in principle any value of $p \in \{1,\dots,n\}$ could be appropriate. 

- Let us see what happens, in practice, when we select different values of $p$...

:::

## Yesterday's data, polynomial regression

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
degree_list <- c(1, 3, 6, 12, 18, 23)
data_pred <- NULL
x_seq <- seq(from = min(dataset$x), to = max(dataset$x), length = 30000)
for (degree in degree_list) {
  # Fitting a polynomial of degree p -1
  fit <- lm(y.yesterday ~ poly(x, degree = degree, raw = F), data = dataset)
  y_hat <- predict(fit, newdata = data.frame(x = x_seq))
  data_pred <- rbind(data_pred, data.frame(x = x_seq, y_hat = y_hat, degree = paste("Degree of the polynomial:", degree)))
}
data_pred$degree <- factor(data_pred$degree)
data_pred$degree <- factor(data_pred$degree, levels = levels(data_pred$degree)[c(1, 5, 6, 2, 3, 4)])

ggplot(data = data_pred) +
  geom_line(aes(x = x, y = y_hat, col = degree)) +
  geom_point(data = dataset, aes(x = x, y = y.yesterday), size = 0.8) +
  theme_light() +
  theme(legend.position = "none") +
  facet_wrap(. ~ degree, ncol = 3) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("x") +
  ylab("y") +
  ylim(c(0.42, 0.56))
```

## Yesterday's data, goodness of fit

```{r}
degree_list <- 1:23
data_goodness <- data.frame(degree = degree_list, Deviance = NA, R_squared = NA, Deviance_test = NA, R_squared_test = NA)
deviance_tot <- sum((dataset$y.tomorrow - mean(dataset$y.tomorrow))^2)
for (degree in degree_list) {
  # Fitting a polynomial of degree p -1
  fit <- lm(y.yesterday ~ poly(x, degree = degree, raw = F), data = dataset)
  data_goodness$Deviance[degree] <- sum((dataset$y.yesterday - fitted(fit))^2)
  data_goodness$Deviance_test[degree] <- sum((dataset$y.tomorrow - fitted(fit))^2)
  data_goodness$R_squared[degree] <- summary(fit)$r.squared
  data_goodness$R_squared_test[degree] <- 1 - data_goodness$Deviance_test[degree] / deviance_tot
}
```


:::columns

::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
ggplot(data = data_goodness, aes(x = degree + 1, y = Deviance)) +
  geom_line() +
  geom_point(size = 0.8) +
  theme_light() +
  xlab("# of parameters p") +
  ylab("Deviance")
```
:::

::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
ggplot(data = data_goodness, aes(x = degree + 1, y = R_squared)) +
  geom_line() +
  geom_point(size = 0.8) +
  theme_light() +
  xlab("# of parameters p") +
  ylab(expression(R^2))
```
:::

:::

## Yesterday's data, polynomial interpolation ($p = n$)

```{r}
#| fig-width: 9
#| fig-height: 6
#| fig-align: center
lagrange <- function(x0, y0) {
  f <- function(x) {
    sum(y0 * sapply(seq_along(x0), \(j) {
      prod(x - x0[-j]) / prod(x0[j] - x0[-j])
    }))
  }
  Vectorize(f, "x")
}
f <- lagrange(dataset$x, dataset$y.yesterday)

plot(dataset$x, dataset$y.yesterday, pch = 16, xlab = "x", ylab = "y", main = "Degree of the polynomial: n-1")
curve(f(x), n = 300, add = TRUE)
```

## Yesterday's data, tomorrow's prediction

::: incremental

- The [deviance]{.blue} decreases as the number of parameter increases; similarly, the [$R^2$]{.blue} always increases as a function of $p$. It can be [proved]{.orange} that this [always happens]{.orange} in linear models. 

- One might be tempted to let $p$ as large as possible...

- Taking this reasoning to the extreme, this would lead to the choice $p = n$, so that $$
D(\hat{\beta}) = 0, \qquad R^2 = 1,
$$
i.e. an apparently perfect fit. This procedure is called [interpolation]{.blue}.

- However, we are [not]{.orange} interested in predicting [yesterday]{.orange} data. Our goal is to predict [tomorrow]{.blue}'s data, i.e., given $x_{n + i}$, to predict $y_{n + i}$, for a new set of $m = 30$ points: $$
(x_{n+1}, y_{n+1}), \dots, (x_{n+m}, y_{n + m}) \quad \longrightarrow \quad \hat{y}_{n+i} = f(x_{n+i}; \hat{\beta}), \quad i=1,\dots,m,
$$
where $\hat{\beta}$ is obtained using yesterday data. 

- [Remark]{.orange}. Tomorrow's data are generated according to the same scheme of yesterday's data.

:::

## Tomorrow's data, polynomial regression

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
ggplot(data = data_pred) +
  geom_line(aes(x = x, y = y_hat, col = degree)) +
  geom_point(data = dataset, aes(x = x, y = y.tomorrow), size = 0.8) +
  theme_light() +
  theme(legend.position = "none") +
  facet_wrap(. ~ degree, ncol = 3) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("x") +
  ylab("y") +
  ylim(c(0.42, 0.56))
```

## Tomorrow's data, goodness of fit

:::columns

::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
ggplot(data = data_goodness, aes(x = degree + 1, y = Deviance_test)) +
  geom_line() +
  geom_point(size = 0.8) +
  theme_light() +
  xlab("# of parameters p") +
  ylab("Deviance")
```
:::

::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
ggplot(data = data_goodness, aes(x = degree + 1, y = R_squared_test)) +
  geom_line() +
  geom_point(size = 0.8) +
  theme_light() +
  xlab("# of parameters p") +
  ylab(expression(R^2))
```
:::

::: 

## Comments and remarks

::: incremental

- The deviance on yesterday's data ([training]{.orange}) or tomorrow's data ([test]{.blue}) are defined as
$$
D_{\text{train}}(\hat{\beta}) = \sum_{i=1}^n\{y_i -f(x_i; \hat{\beta})\}^2,\quad D_{\text{test}}(\hat{\beta}) = \sum_{i=1}^m\{y_{n +i} -f(x_{n + i}; \hat{\beta})\}^2,
$$
and similarly for $R^2_\text{train}$ and $R^2_\text{test}$.

- For small values of $p$, an increase in the degree of the polynomial [improves the fit]{.blue}. In other words, at the beginning both the deviances $D_{\text{train}}(\hat{\beta})$ and $D_{\text{test}}(\hat{\beta})$ decrease.

- This improvement gradually ceases as the increase in degree causes the polynomial to follow [random fluctuations]{.orange} in yesterday’s data [not observed]{.blue} in the [new sample]{.blue}.

- An over-adaptation to yesterday data is called [overfitting]{.orange}, which occurs when the deviance $D_{\text{train}}(\hat{\beta})$ is low but the new deviance $D_{\text{test}}(\hat{\beta})$ is high.

:::

# Bias-variance tradeoff

## If we knew $f(x)$...

## Bias-variance trade-off

## Prediction error

## ...but since we do not know $f(x)$

# Cross-validation

# Information criteria