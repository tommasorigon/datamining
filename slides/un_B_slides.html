<!DOCTYPE html>
<html lang="en"><head>
<script src="un_B_files/libs/clipboard/clipboard.min.js"></script>
<script src="un_B_files/libs/quarto-html/tabby.min.js"></script>
<script src="un_B_files/libs/quarto-html/popper.min.js"></script>
<script src="un_B_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="un_B_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="un_B_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="un_B_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.2.335">

  <meta name="author" content="Tommaso Rigon">
  <title>Optimism, Conflicts, and Trade-offs</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="un_B_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="un_B_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
      }
    pre.numberSource { margin-left: 3em;  padding-left: 4px; }
    div.sourceCode
      { color: #24292e;  }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #24292e; } /* Normal */
    code span.al { color: #ff5555; font-weight: bold; } /* Alert */
    code span.an { color: #6a737d; } /* Annotation */
    code span.at { color: #d73a49; } /* Attribute */
    code span.bn { color: #005cc5; } /* BaseN */
    code span.bu { color: #d73a49; } /* BuiltIn */
    code span.cf { color: #d73a49; } /* ControlFlow */
    code span.ch { color: #032f62; } /* Char */
    code span.cn { color: #005cc5; } /* Constant */
    code span.co { color: #6a737d; } /* Comment */
    code span.cv { color: #6a737d; } /* CommentVar */
    code span.do { color: #6a737d; } /* Documentation */
    code span.dt { color: #d73a49; } /* DataType */
    code span.dv { color: #005cc5; } /* DecVal */
    code span.er { color: #ff5555; text-decoration: underline; } /* Error */
    code span.ex { color: #d73a49; font-weight: bold; } /* Extension */
    code span.fl { color: #005cc5; } /* Float */
    code span.fu { color: #6f42c1; } /* Function */
    code span.im { color: #032f62; } /* Import */
    code span.in { color: #6a737d; } /* Information */
    code span.kw { color: #d73a49; } /* Keyword */
    code span.op { color: #24292e; } /* Operator */
    code span.ot { color: #6f42c1; } /* Other */
    code span.pp { color: #d73a49; } /* Preprocessor */
    code span.re { color: #6a737d; } /* RegionMarker */
    code span.sc { color: #005cc5; } /* SpecialChar */
    code span.ss { color: #032f62; } /* SpecialString */
    code span.st { color: #032f62; } /* String */
    code span.va { color: #e36209; } /* Variable */
    code span.vs { color: #032f62; } /* VerbatimString */
    code span.wa { color: #ff5555; } /* Warning */
  </style>
  <link rel="stylesheet" href="un_B_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
  <link href="un_B_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="un_B_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="un_B_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="un_B_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Optimism, Conflicts, and Trade-offs</h1>
  <p class="subtitle">Data Mining - CdL CLAMSES</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<span class="orange">Tommaso Rigon</span> 
</div>
        <p class="quarto-title-affiliation">
            <em>Università degli Studi di Milano-Bicocca</em>
          </p>
    </div>
</div>

</section>
<section id="homepage" class="slide level2 center">
<h2><a href="../index.html">Homepage</a></h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><img data-src="img/razor.jpg"> <em>“Pluralitas non est ponenda sine necessitate.”</em> William of Ockham</p>
</div><div class="column" style="width:60%;">
<ul>
<li><p>In this unit we will cover the following <span class="orange">topics</span>:</p>
<ul>
<li>Bias-variance trade-off</li>
<li>Cross-validation</li>
<li>Information criteria</li>
<li>Optimism</li>
</ul></li>
<li><p>You may have seen these notions before…</p></li>
<li><p>…but it is worth discussing the <span class="orange">details</span> of these ideas once again, with the maturity you now have in a M.Sc.</p></li>
<li><p>They are the <span class="blue">foundations</span> of <span class="orange">statistical learning</span>.</p></li>
</ul>
</div>
</div>
</section>
<section>
<section id="yesterdays-and-tomorrows-data" class="title-slide slide level1 center">
<h1>Yesterday’s and tomorrow’s data</h1>

</section>
<section id="yesterdays-data" class="slide level2 center">
<h2>Yesterday’s data</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="cell" data-hash="un_B_cache/revealjs/unnamed-chunk-2_3013eb589d75c45da1175a2d29f495f0">
<div class="cell-output-display">
<p><img data-src="un_B_files/figure-revealjs/unnamed-chunk-2-1.png" width="750"></p>
</div>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>Let us presume that <span class="orange">yesterday</span> we observed <span class="math inline">n = 30</span> pairs of data <span class="math inline">(x_i, y_i)</span>.</p></li>
<li><p>Data were generated according to <span class="math display">
  Y_i = f(x_i) + \epsilon_i, \quad i=1,\dots,n,
  </span> with each <span class="math inline">y_i</span> being the realization of <span class="math inline">Y_i</span>.</p></li>
<li><p>The <span class="math inline">\epsilon_1,\dots,\epsilon_n</span> are iid “<span class="orange">error</span>” terms, such that <span class="math inline">\mathbb{E}(\epsilon_i)=0</span> and <span class="math inline">\text{var}(\epsilon_i)=\sigma^2 = 10^{-4}</span>.</p></li>
<li><p>Here <span class="math inline">f(x)</span> is a regression function (<span class="blue">signal</span>) that we leave unspecified.</p></li>
<li><p><span class="blue">Tomorrow</span> we will get a new <span class="math inline">x</span>. We wish to <span class="orange">predict</span> <span class="math inline">Y</span> using <span class="math inline">\mathbb{E}(Y) = f(x)</span>.</p></li>
</ul>
</div>
</div>
</section>
<section id="polynomial-regression" class="slide level2 center">
<h2>Polynomial regression</h2>
<div>
<ul>
<li class="fragment"><p>The function <span class="math inline">f(x)</span> is unknown, therefore it should be estimated.</p></li>
<li class="fragment"><p>A simple approach is using the tools of <a href="unit_A1_slides.html">Unit A.1</a>, such as <span class="orange">polynomial regression</span>: <span class="math display">
f(x) = f(x; \beta) = \beta_1 + \beta_2 x + \beta_3 x^2 + \cdots + \beta_p x^{p-1},
</span> namely <span class="math inline">f(x)</span> is <span class="orange">approximated</span> with a polynomial of degree <span class="math inline">p-1</span> (i.e.&nbsp;Taylor expansions).</p></li>
<li class="fragment"><p>This model is linear in the parameters: ordinary least squares can be applied.</p></li>
<li class="fragment"><p>How do we choose the <span class="blue">degree of the polynomial</span> <span class="math inline">p - 1</span>?</p></li>
<li class="fragment"><p>Without a clear guidance, in principle any value of <span class="math inline">p \in \{1,\dots,n\}</span> could be appropriate.</p></li>
<li class="fragment"><p>Let us compare the <span class="blue">mean squared error</span> (MSE) on yesterday’s data (<span class="orange">training</span>) <span class="math display">
\text{MSE}_{\text{train}} = \frac{1}{n}\sum_{i=1}^n\{y_i -f(x_i; \hat{\beta})\}^2,
</span> or alternatively <span class="math inline">R^2_\text{train}</span>, for different values of <span class="math inline">p</span>…</p></li>
</ul>
</div>
</section>
<section id="yesterdays-data-polynomial-regression" class="slide level2 center">
<h2>Yesterday’s data, polynomial regression</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-3-1.png" width="1170" class="r-stretch quarto-figure-center"></section>
<section id="yesterdays-data-goodness-of-fit" class="slide level2 center">
<h2>Yesterday’s data, goodness of fit</h2>
<div class="cell" data-hash="un_B_cache/revealjs/unnamed-chunk-4_a19f8118ca192ba03a1bfe8b92a9ddad">

</div>
<div class="columns">
<div class="column" style="width:50%;">
<div class="cell" data-hash="un_B_cache/revealjs/unnamed-chunk-5_7f6eafebab3340267f36309c76a4334d">
<div class="cell-output-display">
<p><img data-src="un_B_files/figure-revealjs/unnamed-chunk-5-1.png" width="750"></p>
</div>
</div>
</div><div class="column" style="width:50%;">
<div class="cell" data-hash="un_B_cache/revealjs/unnamed-chunk-6_f682da34d188ed3c269ec47d06c8f46f">
<div class="cell-output-display">
<p><img data-src="un_B_files/figure-revealjs/unnamed-chunk-6-1.png" width="750"></p>
</div>
</div>
</div>
</div>
</section>
<section id="yesterdays-data-polynomial-interpolation-p-n" class="slide level2 center">
<h2>Yesterday’s data, polynomial interpolation (<span class="math inline">p = n</span>)</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-7-1.png" width="1350" class="r-stretch quarto-figure-center"></section>
<section id="yesterdays-data-tomorrows-prediction" class="slide level2 center">
<h2>Yesterday’s data, tomorrow’s prediction</h2>
<div>
<ul>
<li class="fragment"><p>The <span class="blue">MSE</span> decreases as the number of parameter increases; similarly, the <span class="blue"><span class="math inline">R^2</span></span> increases as a function of <span class="math inline">p</span>. It can be <span class="orange">proved</span> that this <span class="orange">always happens</span> using ordinary least squares.</p></li>
<li class="fragment"><p>One might be tempted to let <span class="math inline">p</span> as large as possible to make the model more flexible…</p></li>
<li class="fragment"><p>Taking this reasoning to the extreme would lead to the choice <span class="math inline">p = n</span>, so that <span class="math display">
\text{MSE}_\text{train} = 0, \qquad R^2_\text{train} = 1,
</span> i.e.&nbsp;an apparently perfect fit. This procedure is called <span class="blue">interpolation</span>.</p></li>
<li class="fragment"><p>However, we are <span class="orange">not</span> interested in predicting <span class="orange">yesterday</span> data. Our goal is to predict <span class="blue">tomorrow</span>’s data, i.e.&nbsp;a <span class="blue">new set</span> of <span class="math inline">n = 30</span> points: <span class="math display">
(x_1, \tilde{y}_1), \dots, (x_n, \tilde{y}_n),
</span> using <span class="math inline">\hat{y}_i = f(x_i; \hat{\beta})</span>, where <span class="math inline">\hat{\beta}</span> is obtained using yesterday data.</p></li>
<li class="fragment"><p><span class="orange">Remark</span>. Tomorrow’s r.v. <span class="math inline">\tilde{Y}_1,\dots, \tilde{Y}_n</span> follow the same scheme of yesterday’s data.</p></li>
</ul>
</div>
</section>
<section id="tomorrows-data-polynomial-regression" class="slide level2 center">
<h2>Tomorrow’s data, polynomial regression</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-8-1.png" width="1200" class="r-stretch quarto-figure-center"></section>
<section id="tomorrows-data-goodness-of-fit" class="slide level2 center">
<h2>Tomorrow’s data, goodness of fit</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="cell" data-hash="un_B_cache/revealjs/unnamed-chunk-9_38ec2d5b8e8c3e0e48096398c5aeb998">
<div class="cell-output-display">
<p><img data-src="un_B_files/figure-revealjs/unnamed-chunk-9-1.png" width="750"></p>
</div>
</div>
</div><div class="column" style="width:50%;">
<div class="cell" data-hash="un_B_cache/revealjs/unnamed-chunk-10_d68df3a629cde8b6e0db2cca63fbfed2">
<div class="cell-output-display">
<p><img data-src="un_B_files/figure-revealjs/unnamed-chunk-10-1.png" width="750"></p>
</div>
</div>
</div>
</div>
</section>
<section id="comments-and-remarks" class="slide level2 center">
<h2>Comments and remarks</h2>
<div>
<ul>
<li class="fragment"><p>The mean squared error on tomorrow’s data (<span class="blue">test</span>) is defined as <span class="math display">
\text{MSE}_{\text{test}} = \frac{1}{n}\sum_{i=1}^n\{\tilde{y}_i -f(x_i; \hat{\beta})\}^2,
</span> and similarly the <span class="math inline">R^2_\text{test}</span>. We would like the <span class="math inline">\text{MSE}_{\text{test}}</span> to be <span class="orange">as small as possible</span>.</p></li>
<li class="fragment"><p>For <span class="blue">small values</span> of <span class="math inline">p</span>, an increase in the degree of the polynomial <span class="blue">improves the fit</span>. In other words, at the beginning, both the <span class="math inline">\text{MSE}_{\text{train}}</span> and the <span class="math inline">\text{MSE}_{\text{test}}</span> decrease.</p></li>
<li class="fragment"><p>For <span class="orange">larger values</span> of <span class="math inline">p</span>, the improvement gradually ceases and the polynomial follows <span class="orange">random fluctuations</span> in yesterday’s data which are <span class="blue">not observed</span> in the <span class="blue">new sample</span>.</p></li>
<li class="fragment"><p>An over-adaptation to yesterday data is called <span class="orange">overfitting</span>, which occurs when the training <span class="math inline">\text{MSE}_{\text{train}}</span> is low but the test <span class="math inline">\text{MSE}_{\text{test}}</span> is high.</p></li>
<li class="fragment"><p>The yesterday’s dataset is available from the textbook (A&amp;S) website:</p>
<ul>
<li class="fragment">Dataset <a href="http://azzalini.stat.unipd.it/Book-DM/yesterday.dat" class="uri">http://azzalini.stat.unipd.it/Book-DM/yesterday.dat</a></li>
<li class="fragment">True <span class="math inline">f(\bm{x})</span> <a href="http://azzalini.stat.unipd.it/Book-DM/f_true.R" class="uri">http://azzalini.stat.unipd.it/Book-DM/f_true.R</a></li>
</ul></li>
</ul>
</div>
</section>
<section id="orthogonal-polynomials" class="slide level2 center">
<h2>☠️ - Orthogonal polynomials</h2>
<ul>
<li>When performing polynomial regression, the <code>poly</code> command computes an <span class="orange">orthogonal basis</span> of the original covariates <span class="math inline">(1, x, x^2,\dots,x^{p-1})</span> through the QR decomposition:</li>
</ul>
<div class="cell" data-hash="un_B_cache/revealjs/unnamed-chunk-11_32432e945a6e8d822b902ffa4d783904">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y.yesterday <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="at">degree =</span> <span class="dv">3</span>, <span class="at">raw =</span> <span class="cn">FALSE</span>), <span class="at">data =</span> dataset)</span>
<span id="cb1-2"><a href="#cb1-2"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(fit)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="fu">colnames</span>(X) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"Intercept"</span>,<span class="st">"x1"</span>,<span class="st">"x2"</span>,<span class="st">"x3"</span>)</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="fu">round</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X, <span class="dv">8</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          Intercept x1 x2 x3
Intercept        30  0  0  0
x1                0  1  0  0
x2                0  0  1  0
x3                0  0  0  1</code></pre>
</div>
</div>
<ul>
<li>Polynomial regression becomes numerically unstable when <span class="math inline">p \ge 13</span> (<code>raw = TRUE</code>, i.e.&nbsp;using the original polynomials) and <span class="math inline">p \ge 25</span> (<code>raw = FALSE</code>, i.e.&nbsp;using orthogonal polynomials).</li>
</ul>
</section>
<section id="lagrange-interpolating-polynomials" class="slide level2 center">
<h2>☠️ - Lagrange interpolating polynomials</h2>
<ul>
<li><p>If the previous code does not work for <span class="math inline">p \ge 25</span>, how was the plot of <a href="#/yesterdays-data-polynomial-interpolation-p-n">this slide</a> computed?</p></li>
<li><p>It turns out that for <span class="math inline">p = n</span> there exists an alternative way of finding the ordinary least square solution, based on Lagrange interpolating polynomials, namely:</p></li>
</ul>
<p><span class="math display">
\hat{f}(x) = \sum_{i=1}^n\ell_i(x) y_i, \qquad \ell_i(x) = \prod_{k \neq i}\frac{x - x_k}{x_i - x_k}.
</span></p>
<ul>
<li>Interpolating polynomials are clearly <span class="orange">unsuitable</span> for regression purposes, but may have interesting applications in other contexts.</li>
</ul>
</section></section>
<section>
<section id="errors-trade-offs-and-optimism" class="title-slide slide level1 center">
<h1>Errors, trade-offs and optimism</h1>

</section>
<section id="summary-and-notation-fixed-x" class="slide level2 center">
<h2>Summary and notation (fixed-<span class="math inline">X</span>)</h2>
<div>
<ul>
<li class="fragment"><p>Let us better formalize the analysis we did so far, using the appropriate mathematical tools.</p></li>
<li class="fragment"><p>In the previous example we consider two set of <span class="blue">random variables</span>:</p>
<ul>
<li class="fragment">The <span class="orange">training set</span> (yesterday) <span class="math inline">Y_1,\dots, Y_n</span>, whose realization is <span class="math inline">y_1,\dots,y_n</span>.</li>
<li class="fragment">The <span class="blue">test set</span> (tomorrow) <span class="math inline">\tilde{Y}_1,\dots,\tilde{Y}_n</span>, whose realization is <span class="math inline">\tilde{y}_1, \dots, \tilde{y}_n</span>.</li>
</ul></li>
<li class="fragment"><p>The <span class="blue">covariates</span> <span class="math inline">\bm{x}_i = (x_{i1},\dots,x_{ip})^T</span> in this scenario are <span class="blue">deterministic</span>. This is the so-called <span class="blue">fixed-<span class="math inline">X</span></span> design, which is a common assumption in regression models.</p></li>
<li class="fragment"><p>We also assume that the random variables <span class="math inline">Y_i</span> and <span class="math inline">\tilde{Y}_i</span> are <span class="blue">independent</span>.</p></li>
<li class="fragment"><p>In <span class="orange">regression</span> problems we customarily assume that <span class="math display">
    Y_i = f(\bm{x}_i) + \epsilon_i, \qquad \tilde{Y}_i = f(\bm{x}_i) + \tilde{\epsilon}_i, \quad i=1,\dots,n,
  </span> where <span class="math inline">\epsilon_i</span> and <span class="math inline">\tilde{\epsilon}_i</span> are iid “<span class="orange">error</span>” terms, with <span class="math inline">\mathbb{E}(\epsilon_i)=0</span> and <span class="math inline">\text{var}(\epsilon_i)=\sigma^2</span>.</p></li>
<li class="fragment"><p>The <span class="orange">training data</span> is used to estimate a function of the covariates <span class="math inline">\hat{f}(\bm{x}_i)</span>. We hope that our predictions works well on the <span class="blue">test set</span>.</p></li>
</ul>
</div>
</section>
<section id="the-in-sample-prediction-error" class="slide level2 center">
<h2>The in-sample prediction error</h2>
<div>
<ul>
<li class="fragment"><p>A measure of quality for the predictions is the <span class="blue">in-sample prediction error</span>: <span class="math display">
\text{ErrF} =  \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n \mathscr{L}\{\tilde{Y}_i; \hat{f}(\bm{x}_i)\}\right],
</span> where <span class="math inline">\mathscr{L}\{\tilde{Y}_i; \hat{f}(\bm{x}_i)\}</span> is a <span class="orange">loss function</span>. The “<span class="blue">F</span>” is a reminder of the <span class="blue">f</span>ixed-<span class="math inline">X</span> design.</p></li>
<li class="fragment"><p>The expectation is taken with respect to traning random variable <span class="math inline">Y_1,\dots,Y_n</span>, implicitly appearing in <span class="math inline">\hat{f}(\bm{x})</span>, and the new data points <span class="math inline">\tilde{Y}_1,\dots,\tilde{Y}_n</span>.</p></li>
<li class="fragment"><p>The in-sample prediction error is measuring the <span class="blue">average</span> “discrepancy” between the <span class="orange">new data points</span> and the corresponding predictions based on the training.</p></li>
<li class="fragment"><p>Examples of loss functions for <span class="blue">regression problems</span> <span class="math inline">Y \in \mathbb{R}</span> are:</p>
<ul>
<li class="fragment">The quadratic loss <span class="math inline">\mathscr{L}\{\tilde{Y}_i; \hat{f}(\bm{x}_i)\} = \{\tilde{Y}_i - \hat{f}(\bm{x}_i)\}^2</span>, leading to the MSE.</li>
<li class="fragment">The absolute loss <span class="math inline">\mathscr{L}\{\tilde{Y}_i; \hat{f}(\bm{x}_i)\} = |\tilde{Y}_i - \hat{f}(\bm{x}_i)|</span>, leading to the MAE.</li>
</ul></li>
</ul>
</div>
</section>
<section id="quadratic-loss-and-the-mean-squared-error" class="slide level2 center">
<h2>Quadratic loss and the mean squared error</h2>
<div class="callout callout-note no-icon callout-captioned callout-style-simple">
<div class="callout-body">
<div class="callout-caption">
<p><strong>Error decomposition (reducible and irreducible)</strong></p>
</div>
<div class="callout-content">
<p>In a regression problem, under a quadratic loss, <span class="orange">each element</span> of the <span class="blue">in-sample prediction error</span> admits the following decomposition <span class="math display">
\begin{aligned}
\mathbb{E}\left[\{\tilde{Y}_i - \hat{f}(\bm{x}_i)\}^2\right] &amp;= \mathbb{E}\left[\{f(\bm{x}_i) + \tilde{\epsilon}_i - \hat{f}(\bm{x}_i)\}^2\right] \\
&amp; = \mathbb{E}\left[\{f(\bm{x}_i) - \hat{f}(\bm{x}_i)\}^2\right] + \mathbb{E}(\tilde{\epsilon}_i^2) + 2 \: \mathbb{E}\left[\tilde{\epsilon}_i \: \{f(\bm{x}_i) - \hat{f}(\bm{x}_i)\}\right]\\
&amp; = \underbrace{\mathbb{E}\left[\{\hat{f}(\bm{x}_i) - f(\bm{x}_i)\}^2\right]}_{\text{reducible}} + \underbrace{\sigma^2}_{\text{irreducible}},
\end{aligned}
</span> recalling that <span class="math inline">\mathbb{E}(\tilde{\epsilon}_i^2) = \text{var}(\tilde{\epsilon}_i) = \sigma^2</span> and for any <span class="math inline">i = 1,\dots,n</span>.</p>
</div>
</div>
</div>
</section>
<section id="reducible-and-irreducible-errors" class="slide level2 center">
<h2>Reducible and irreducible errors</h2>
<div>
<ul>
<li class="fragment"><p>We would like to make the <span class="orange">mean squared error</span> as <span class="orange">small</span> as possible, e.g.&nbsp;by choosing an “optimal” degree of the polynomial <span class="math inline">p-1</span> that minimizes it.</p></li>
<li class="fragment"><p>Let us recall the previous decomposition <span class="math display">
\mathbb{E}\left[\{\tilde{Y}_i - \hat{f}(\bm{x}_i)\}^2\right] =  \underbrace{\mathbb{E}\left[\{\hat{f}(\bm{x}_i) - f(\bm{x}_i)\}^2\right]}_{\text{reducible}} + \underbrace{\sigma^2}_{\text{irreducible}}, \quad i=1\dots,n.
</span></p></li>
<li class="fragment"><p>The <span class="blue">best case scenario</span> is when the estimated function coincides with the mean of <span class="math inline">\tilde{Y}_i</span>, i.e.&nbsp;<span class="math display">
\hat{f}(\bm{x}_i) =  f(\bm{x}_i) = \mathbb{E}(\tilde{Y}_i),
</span> but even in this (overly optimistic) situation, we would still commit mistakes, due to the presence of <span class="math inline">\tilde{\epsilon}_i</span> (unless <span class="math inline">\sigma^2 = 0</span>). Hence, the variance <span class="math inline">\sigma^2</span> is called the <span class="orange">irreducible error</span>.</p></li>
<li class="fragment"><p>Since we do not know <span class="math inline">f(\bm{x}_i)</span>, we seek for an estimate <span class="math inline">\hat{f}(\bm{x}_i) \approx f(\bm{x}_i)</span>, in the attempt of minimizing the <span class="blue">reducible error</span>.</p></li>
</ul>
</div>
</section>
<section id="classification-problems" class="slide level2 center">
<h2>Classification problems</h2>
<div>
<ul>
<li class="fragment"><p>The principles described for regression settings also applies to <span class="blue">classification problems</span>, after the appropriate adjustments. We focus here on the binary case <span class="math inline">Y \in \{0, 1\}</span>.</p></li>
<li class="fragment"><p>In this context, the relationship between <span class="math inline">\bm{x}_i</span> and the <span class="orange">Bernoulli</span> r.v. <span class="math inline">Y_i</span> is such that <span class="math display">
\mathbb{P}(Y_i = 1) = p(\bm{x}_i) =  g\{f(\bm{x}_i)\}, \qquad i=1,\dots,n,
</span> where <span class="math inline">g(x) :\mathbb{R} \rightarrow (0,1)</span> is monotone transformation, such as the inverse logit.</p></li>
<li class="fragment"><p>Examples of loss functions for <span class="blue">binary classification problems</span> <span class="math inline">Y \in \{0, 1\}</span> are:</p>
<ul>
<li class="fragment">The <span class="orange">misclassification rate</span> <span class="math inline">\mathscr{L}\{\tilde{Y}_i; \hat{f}(\bm{x}_i)\} = \mathbb{I}(\tilde{Y}_i \neq \hat{y}_i)</span>, where the predictions are obtained dichotomizing the estimated probabilities <span class="math inline">\hat{y}_i = \mathbb{I}(\hat{p}(\bm{x}_i) &gt; 1/2)</span>.</li>
<li class="fragment">The <span class="orange">deviance</span> or <span class="orange">cross-entropy</span> loss functions is defined as <span class="math display">
\mathscr{L}\{\tilde{Y}_i; \hat{f}(\bm{x}_i)\} = -2\left[ \mathbb{I}(Y_i = 1)\log{\hat{p}(\bm{x}_i)} + \mathbb{I}(Y_i = 0)\log{\{1 - \hat{p}(\bm{x}_i)}\}\right].
</span></li>
</ul></li>
<li class="fragment"><p>Under a misclassification loss, the <span class="blue">Bayes rate</span> is the error rate we would get if we knew the true <span class="math inline">p(\bm{x}_i)</span>. This is the <span class="orange">irreducible error</span> for classification problems.</p></li>
</ul>
</div>
</section>
<section id="bias-variance-trade-off" class="slide level2 center">
<h2>Bias-variance trade-off</h2>
<ul>
<li><p>In many textbooks, including A&amp;S, the starting point of the analysis is the <span class="blue">reducible error</span>, because it is the only one we can control and it has a transparent interpretation.</p></li>
<li><p>The reducible error measures the <span class="orange">discrepancy</span> between the unknown function <span class="math inline">f(\bm{x})</span> and its estimate <span class="math inline">\hat{f}(\bm{x})</span> and therefore it is a <span class="orange">natural measure</span> of goodness of fit.</p></li>
<li><p>What follows is the <span class="blue">most important result</span> of this unit.</p></li>
</ul>
<div class="fragment">
<div class="callout callout-note no-icon callout-captioned callout-style-simple">
<div class="callout-body">
<div class="callout-caption">
<p><strong>Bias-variance decomposition</strong></p>
</div>
<div class="callout-content">
<p>For any covariate value <span class="math inline">\bm{x}</span>, it holds the following bias-variance decomposition: <span class="math display">
\mathbb{E}\left[\{\hat{f}(\bm{x}) - f(\bm{x})\}^2\right] = \underbrace{\mathbb{E}\left[\{\hat{f}(\bm{x}) - f(\bm{x})\}\right]^2}_{\text{Bias}^2} + \underbrace{\text{var}\{\hat{f}(\bm{x})\}}_{\text{variance}}.
</span></p>
</div>
</div>
</div>
</div>
</section>
<section id="in-sample-prediction-error-of-ordinary-least-squares" class="slide level2 center">
<h2>In-sample prediction error of ordinary least squares</h2>
<div>
<ul>
<li class="fragment"><p>Summarizing, the <span class="orange">in-sample prediction error</span> under <span class="blue">squared loss</span> can be decomposed as <span class="math display">
\begin{aligned}
\text{ErrF} = \sigma^2 + \frac{1}{n}\sum_{i=1}^n\mathbb{E}\left[\{\hat{f}(\bm{x}_i) - f(\bm{x}_i)\}\right]^2 + \frac{1}{n}\sum_{i=1}^n\text{var}\{\hat{f}(\bm{x}_i)\}.
\end{aligned}
</span></p></li>
<li class="fragment"><p>In <span class="blue">ordinary least squares</span> the above quantity can be computed in closed form, since each element of the <span class="orange">bias</span> term equals <span class="math display">
\mathbb{E}\left[\{\hat{f}(\bm{x}_i) - f(\bm{x}_i)\}\right] = \bm{x}_i^T(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{f} - f(\bm{x}_i).
</span> where <span class="math inline">\bm{f} = (f(\bm{x}_1),\dots,f(\bm{x}_n))^T</span>. Note that if <span class="math inline">f(\bm{x}) = \bm{x}^T\beta</span>, then the bias is zero.</p></li>
<li class="fragment"><p>Moreover, in <span class="blue">ordinary least squares</span> the <span class="orange">variance</span> term equals <span class="math display">
\frac{1}{n}\sum_{i=1}^n\text{var}\{\hat{f}(\bm{x}_i)\} = \frac{\sigma^2}{n}\sum_{i=1}^n  \bm{x}_i^T (\bm{X}^T\bm{X})^{-1}\bm{x}_i = \frac{\sigma^2}{n}\text{tr}\{(\bm{X}^T\bm{X})(\bm{X}^T\bm{X})^{-1}\} = \sigma^2 \frac{p}{n}.
</span></p></li>
</ul>
</div>
</section>
<section id="if-we-knew-fx" class="slide level2 center">
<h2>If we knew <span class="math inline">f(x)</span>…</h2>
<div class="cell" data-hash="un_B_cache/revealjs/unnamed-chunk-12_b6b42a44338830019296b590e2832953">

</div>
<div class="cell" data-hash="un_B_cache/revealjs/unnamed-chunk-13_b8f205e3685c8bd0e13338b9e1d6e924">

</div>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-14-1.png" width="1200" class="r-stretch"></section>
<section id="bias-variance-trade-off-1" class="slide level2 center">
<h2>Bias-variance trade-off</h2>
<div>
<ul>
<li class="fragment"><p>When <span class="math inline">p</span> grows, the mean squared error first decreases and then it increases. In the example, the <span class="orange">theoretical optimum</span> is <span class="math inline">p = 6</span> (5th degree polynomial).</p></li>
<li class="fragment"><p>The <span class="orange">bias</span> measures the ability of <span class="math inline">\hat{f}(\bm{x})</span> to reconstruct the true <span class="math inline">f(\bm{x})</span>. The bias is due to <span class="blue">lack of knowledge</span> of the data-generating mechanism. It equals zero when <span class="math inline">\mathbb{E}\{\hat{f}(\bm{x})\} = f(\bm{x})</span>.</p></li>
<li class="fragment"><p>The <span class="orange">bias</span> term can be reduced by increasing the flexibility of the model (e.g., by considering a high value for <span class="math inline">p</span>).</p></li>
<li class="fragment"><p>The <span class="blue">variance</span> measures the variability of the estimator <span class="math inline">\hat{f}(\bm{x})</span> and its tendency to follow random fluctuations of the data.</p></li>
<li class="fragment"><p>The <span class="blue">variance</span> increases together with the model complexity.</p></li>
<li class="fragment"><p>It is not possible to minimize both the bias and the variance, there is a <span class="orange">trade-off</span>.</p></li>
<li class="fragment"><p>We say that an estimator is <span class="orange">overfitting</span> the data if an increase in variance comes without important gains in terms of bias.</p></li>
</ul>
<!-- - **Summary**. Low model complexity: [high bias]{.orange}, [low variance]{.blue}. High model complexity: [low bias]{.blue}, [high variance]{.orange}. -->
</div>
</section>
<section id="but-since-we-do-not-know-fx" class="slide level2 center">
<h2>But since we do not know <span class="math inline">f(x)</span>…</h2>
<div>
<ul>
<li class="fragment"><p>We just concluded that we must expect a trade-off between error and variance components. In practice, however, we cannot do this because, of course, <span class="math inline">f(x)</span> is <span class="orange">unknown</span>.</p></li>
<li class="fragment"><p>A simple solution consists indeed in <span class="orange">splitting</span> the observations in two parts: a <span class="orange">training set</span> <span class="math inline">(y_1,\dots,y_n)</span> and a <span class="blue">test set</span> <span class="math inline">(\tilde{y}_1,\dots,\tilde{y}_n)</span>, having the same covariates <span class="math inline">x_1,\dots,x_n</span>.</p></li>
<li class="fragment"><p>We fit the model <span class="math inline">\hat{f}</span> using <span class="math inline">n</span> observations of the training and we use it to predict the <span class="math inline">n</span> observations on the test set.</p></li>
<li class="fragment"><p>This leads to an <span class="orange">unbiased estimate</span> of the <span class="blue">in-sample prediction error</span>, i.e.: <span class="math display">
\widehat{\mathrm{ErrF}} =  \frac{1}{n}\sum_{i=1}^n\mathscr{L}\{\tilde{y}_i; \hat{f}(\bm{x}_i)\}.
</span></p></li>
<li class="fragment"><p>This is exactly what we already did with yesterday’s and tomorrow’s data!</p></li>
</ul>
</div>
</section>
<section id="mse-on-training-and-test-set-recap" class="slide level2 center">
<h2>MSE on training and test set (recap)</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-15-1.png" width="1200" class="r-stretch quarto-figure-center"></section>
<section id="optimism-i" class="slide level2 center">
<h2>Optimism I</h2>
<div>
<ul>
<li class="fragment"><p>Let us investigate this discrepancy between training and test more in depth.</p></li>
<li class="fragment"><p>Under a <span class="blue">squared loss function</span>, the <span class="orange">in-sample prediction error</span> is <span class="math display">
\text{ErrF} = \mathbb{E}(\text{MSE}_\text{test}) = \frac{1}{n}\sum_{i=1}^n\mathbb{E}\left[\{\tilde{Y}_i - \hat{f}(\bm{x}_i)\}^2\right]
</span></p></li>
<li class="fragment"><p>Similarly, the <span class="orange">in-sample training error</span> can be defined as follows <span class="math display">
\mathbb{E}(\text{MSE}_\text{train}) = \frac{1}{n}\sum_{i=1}^n\mathbb{E}\left[\{Y_i - \hat{f}(\bm{x}_i)\}^2\right].
</span></p></li>
<li class="fragment"><p>We already know that <span class="math inline">\mathbb{E}(\text{MSE}_\text{train})</span> provides a very optimistic assessment of the model performance. For example when <span class="math inline">p = n</span> then <span class="math inline">\mathbb{E}(\text{MSE}_\text{train}) = 0</span>.</p></li>
<li class="fragment"><p>We call <span class="orange">optimism</span> the difference between these two quantities: <span class="math display">
\text{Opt} = \mathbb{E}(\text{MSE}_\text{test}) - \mathbb{E}(\text{MSE}_\text{train}).
</span></p></li>
</ul>
</div>
</section>
<section id="optimism-ii" class="slide level2 center">
<h2>Optimism II</h2>
<div>
<ul>
<li class="fragment"><p>It can be proved (see Exercises), that the <span class="orange">optimism</span> has a very simple form: <span class="math display">
\text{Opt} = \frac{2}{n}\sum_{i=1}^n\text{cov}(Y_i, \hat{f}(\bm{x}_i)) = \frac{2}{n} \text{tr}\{\text{cov}(\bm{Y}, \hat{\bm{f}})\}.
</span></p></li>
<li class="fragment"><p>If the predicted values are obtained using <span class="blue">ordinary least squares</span>, then <span class="math inline">\hat{\bm{f}} = \bm{H}\bm{Y}</span>, therefore <span class="math display">
\text{Opt} = \frac{2}{n} \text{tr}\{\text{cov}(\bm{Y}, \bm{H}\bm{Y})\} = \frac{2}{n} \text{tr}\{\text{cov}(\bm{Y}, \bm{Y}) \bm{H}^T\} = \frac{2 \sigma^2}{n}\text{tr} \{\text{cov}(\bm{H})\} = \frac{2 \sigma^2 p}{n}.
</span></p></li>
<li class="fragment"><p>This leads to an estimate for the in-sample prediction error, known as <span class="blue"><span class="math inline">C_p</span> of Mallows</span>: <span class="math display">
\widehat{\mathrm{ErrF}} = \text{MSE}_\text{train} + \text{Opt} = \frac{1}{n}\sum_{i=1}^n\{y_i - \hat{f}(\bm{x}_i)\}^2 + \frac{2 \sigma^2 p}{n}.
</span></p></li>
<li class="fragment"><p>If <span class="math inline">\sigma^2</span> is unknown, then it must be <span class="orange">estimated</span> using for instance <span class="math inline">s^2</span>.</p></li>
</ul>
</div>
</section>
<section id="optimism-iii" class="slide level2 center">
<h2>Optimism III</h2>
<div class="cell" data-hash="un_B_cache/revealjs/unnamed-chunk-16_e2b552cbef452dee76f84d6724cbdfff">

</div>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-17-1.png" width="1200" class="r-stretch quarto-figure-center"></section></section>
<section>
<section id="cross-validation" class="title-slide slide level1 center">
<h1>Cross-validation</h1>

</section>
<section id="another-example-cholesterol-data" class="slide level2 center">
<h2>Another example: <code>cholesterol</code> data</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="cell" data-hash="un_B_cache/revealjs/unnamed-chunk-18_fb1504b56ce04da5a6b634854229dcca">
<div class="cell-output-display">
<p><img data-src="un_B_files/figure-revealjs/unnamed-chunk-18-1.png" width="900"></p>
</div>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>A drug called “cholestyramine” is administered to <span class="math inline">n = 164</span> men.</p></li>
<li><p>For each man, we observe the pair <span class="math inline">(x_i, y_i)</span>.</p></li>
<li><p>The response <span class="math inline">y_i</span> is the <span class="orange">decrease in cholesterol level</span> over the experiment.</p></li>
<li><p>The covariate <span class="math inline">x_i</span> is a measure of <span class="blue">compliance</span>.</p></li>
<li><p>We assume, as before, that the data are generated according to <span class="math display">
    Y_i = f(x_i) + \epsilon_i, \quad i=1,\dots,n.
    </span></p></li>
<li><p>The original data can be <a href="https://hastie.su.domains/CASI_files/DATA/cholesterol.txt">found here</a>.</p></li>
</ul>
</div>
</div>
</section>
<section id="summary-and-notation-random-x" class="slide level2 center">
<h2>Summary and notation (random-<span class="math inline">X</span>)</h2>
<div>
<ul>
<li class="fragment"><p>A slight change to the previous setup is necessary. In fact, there are no reasons to believe that the <span class="blue">compliance</span> is a fixed covariate.</p></li>
<li class="fragment"><p>We consider a set of iid <span class="blue">random variables</span> <span class="math inline">(X_1, Y_1),\dots, (X_n, Y_n)</span>, whose realization is <span class="math inline">(\bm{x}_1,y_1),\dots,(\bm{x}_n, y_n)</span>. This time the covariates are <span class="orange">random</span>.</p></li>
<li class="fragment"><p>The main assumptions is that these pairs are <span class="orange">iid</span>, namely: <span class="math display">(X_i, Y_i) \overset{\text{iid}}{\sim} \mathcal{P}, \quad  i=1,\dots,n.</span></p></li>
<li class="fragment"><p>Conditionally on <span class="math inline">X_i = \bm{x}_i</span>, in <span class="orange">regression problems</span> we let as before <span class="math display">
  Y_i = f(\bm{x}_i) + \epsilon_i, \quad i=1,\dots,n,
</span> where <span class="math inline">\epsilon_i</span> are iid “<span class="orange">error</span>” terms with <span class="math inline">\mathbb{E}(\epsilon_i)=0</span> and <span class="math inline">\text{var}(\epsilon_i)=\sigma^2</span>.</p></li>
</ul>
</div>
</section>
<section id="expected-prediction-error" class="slide level2 center">
<h2>Expected prediction error</h2>
<div>
<ul>
<li class="fragment"><p>In this setting with random covariates, we want to minimize the <span class="orange">expected prediction error</span>: <span class="math display">
\text{Err} =  \mathbb{E}\left[\mathscr{L}\{\tilde{Y}; \hat{f}(\tilde{X})\}\right],
</span> where <span class="math inline">(\tilde{X},\tilde{Y}) \sim \mathcal{P}</span> is a <span class="blue">new data point</span> and <span class="math inline">\hat{f}</span> is an estimate using <span class="math inline">n</span> observations.</p></li>
<li class="fragment"><p>We can <span class="orange">randomly</span> split the original set data <span class="math inline">\{1,\dots,n\}</span> into two groups <span class="math inline">V_\text{train}</span> and <span class="math inline">V_\text{test}</span>.</p></li>
<li class="fragment"><p>We call <span class="math inline">\hat{f}_\text{train}</span> the estimate based on the data in <span class="math inline">V_\text{train}</span>.</p></li>
<li class="fragment"><p>Then, we obtain a (slightly biased) estimate of <span class="math inline">\text{Err}</span> by using the empirical quantity: <span class="math display">
  \widehat{\mathrm{Err}} =  \frac{1}{|V_\text{test}|}\sum_{i \in V_\text{test}}\mathscr{L}\{\tilde{y}_i; \hat{f}_\text{train}(\bm{x}_i)\}.
  </span></p></li>
<li class="fragment"><p>The data-splitting strategy we used before is an effective tool for assessing the error. However, its <span class="blue">interpretation</span> is changed: we are now estimating <span class="math inline">\text{Err}</span> and not <span class="math inline">\text{ErrF}</span>.</p></li>
</ul>
</div>
</section>
<section id="mse-on-training-and-test-cholesterol-data" class="slide level2 center">
<h2>MSE on training and test (<code>cholesterol</code> data)</h2>
<div class="cell" data-hash="un_B_cache/revealjs/unnamed-chunk-19_1a6611740cd84f3838d62126ee26ae0e">

</div>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-20-1.png" width="1200" class="r-stretch quarto-figure-center"></section>
<section id="training-validation-and-test-i" class="slide level2 center">
<h2>Training, validation and test I</h2>
<div>
<ul>
<li class="fragment"><p>In many occasions, we may need to select several complexity parameters and compare hundreds of models.</p></li>
<li class="fragment"><p>If the same test set is used for such a task, the final assessment of the error is somewhat biased and <span class="orange">too optimistic</span>, because we are “learning” from the test set.</p></li>
<li class="fragment"><p>If we are in a data-rich situation, the best approach is to randomly divide the dataset into three parts:</p>
<ul>
<li class="fragment">a <span class="orange">training set</span>, used for <span class="orange">fitting</span> the models;</li>
<li class="fragment">a <span class="blue">validation set</span>, used to estimate prediction error and perform <span class="blue">model selection</span>;</li>
<li class="fragment">a <span class="grey">test set</span>, for <span class="grey">assessment of the error</span> of the final chosen model.</li>
</ul></li>
<li class="fragment"><p>Ideally, the test set should be kept in a “vault,” and be brought out only at the end of the data analysis.</p></li>
</ul>
</div>
</section>
<section id="training-validation-and-test-ii" class="slide level2 center">
<h2>Training, validation and test II</h2>
<ul>
<li>There is no precise rule on how to select the size of these sets; some rule of thumbs are given in the picture below.</li>
</ul>
<div class="flourish-embed flourish-chart" data-src="visualisation/14246499">
<script src="https://public.flourish.studio/resources/embed.js"></script>
</div>
<ul>
<li>The training, validation and test setup <span class="orange">reduces</span> the <span class="orange">number of observations</span> that we can use for fitting the models. It could be problematic if the sample size is relatively small.</li>
</ul>
</section>
<section id="cross-validation-i" class="slide level2 center">
<h2>Cross-validation I</h2>
<ul>
<li><p>A way to partially overcome the loss of efficiency of the training / test paradigm consists in <span class="orange">randomly splitting the data</span> <span class="math inline">\{1,\dots,n\}</span> in equal parts, say <span class="math inline">V_1,\ldots,V_K</span>.</p></li>
<li><p>In the <span class="blue"><span class="math inline">K</span>-fold cross-validation</span> method we use the observations <span class="math inline">i \notin V_k</span> to train the model and the remaining observations <span class="math inline">i \in V_k</span> to assess to perform model selection.</p></li>
<li><p>In the following scheme we let <span class="math inline">K = 5</span>.</p></li>
</ul>
<div class="flourish-embed flourish-hierarchy" data-src="visualisation/14247663">
<script src="https://public.flourish.studio/resources/embed.js"></script>
</div>
</section>
<section id="cross-validation-ii" class="slide level2 center">
<h2>Cross-validation II</h2>
<div>
<ul>
<li class="fragment"><p>In the <span class="blue"><span class="math inline">K</span>-fold cross validation</span> we compute for each fold <span class="math inline">k</span> we fit a model <span class="math inline">\hat{f}_{-V_k}(\bm{x})</span> without using the observations of <span class="math inline">V_k</span>.</p></li>
<li class="fragment"><p>Hence, the model must be estimated <span class="orange"><span class="math inline">K</span> times</span>, which could be computationally challenging.</p></li>
<li class="fragment"><p>The error of each on the <span class="math inline">k</span>th folds is computed as <span class="math display">\widehat{\text{Err}}_{V_k} = \frac{1}{|V_k|} \sum_{i \in V_k} \mathscr{L}\{y_i; \hat{f}_{-V_k}(\bm{x}_i)\},
</span> where <span class="math inline">|V_k|</span> is the cardinality of <span class="math inline">V_k</span>, i.e. <span class="math inline">V_k \approx n / K</span>.</p></li>
<li class="fragment"><p>We summarize the above errors using for instance the mean, obtaining the following <span class="orange">estimate</span> for the <span class="orange">expected prediction error</span>: <span class="math display">\widehat{\mathrm{Err}} = \frac{1}{K} \sum_{k=1}^{K} \widehat{\text{Err}}_{V_k}= \frac{1}{K} \sum_{k=1}^{K}\left[ \frac{1}{|V_k|} \sum_{i \in V_k} \mathscr{L}\{y_i; \hat{f}_{-V_k}(\bm{x}_i)\} \right].</span></p></li>
</ul>
</div>
</section>
<section id="cross-validation-iii-cholesterol-data" class="slide level2 center">
<h2>Cross-validation III (<code>cholesterol</code> data)</h2>
<div class="cell" data-hash="un_B_cache/revealjs/unnamed-chunk-21_03afc6c47a975d5e007888655a88499a">

</div>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-22-1.png" width="1200" class="r-stretch quarto-figure-center"></section>
<section id="leave-one-out-cross-validation" class="slide level2 center">
<h2>Leave-one-out cross-validation</h2>
<ul>
<li><p>The maximum possible value for <span class="math inline">K</span> is <span class="math inline">n</span>, the <span class="orange">leave-one-out</span> cross-validation (LOO-CV).</p></li>
<li><p>The LOO-CV is hard to implement, because it requires the estimation of <span class="math inline">n</span> different models.</p></li>
<li><p>However in <span class="blue">linear models</span> there is a brilliant <span class="orange">computational shortcut</span> that we can exploit.</p></li>
</ul>
<div class="fragment">
<div class="callout callout-note no-icon callout-captioned callout-style-simple">
<div class="callout-body">
<div class="callout-caption">
<p><strong>LOO-CV (Linear models)</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\hat{y}_{-i} = \bm{x}_i^T\hat{\beta}_{-i}</span> be the leave-one-out predictions of a <span class="blue">linear model</span> and let <span class="math inline">h_i = [\bm{H}]_{ii}</span> and <span class="math inline">\hat{y}_i</span> be respectively the leverages and the predictions of the full model. Then: <span class="math display">
y_i - \hat{y}_{-i} = \frac{y_i - \hat{y}_i}{1 - h_i}, \qquad i=1,\dots,n.
</span> Therefore, the leave-one-out mean squared error is <span class="math display">\widehat{\mathrm{Err}} = \frac{1}{n} \sum_{i=1}^{n} \widehat{\text{Err}}_{V_i}= \frac{1}{n}\sum_{i=1}^n \left(\frac{y_i - \hat{y}_i}{1 - h_i}\right)^2.</span></p>
</div>
</div>
</div>
</div>
</section>
<section id="generalized-cross-validation" class="slide level2 center">
<h2>Generalized cross-validation</h2>
<div>
<ul>
<li class="fragment">An alternative to LOO-CV, sometimes used in more complex scenarios, is the so-called <span class="orange">generalized cross validation</span> (GCV), defined as <span class="math display">
\text{GCV} = \widehat{\mathrm{Err}} = \frac{1}{n}\sum_{i=1}^n \left(\frac{y_i - \hat{y}_i}{1 - p/n}\right)^2.
</span></li>
<li class="fragment">The GCV is an approximate LOO-CV, in which the leverages <span class="math inline">h_i</span> are replaced by their mean: <span class="math display">
\frac{1}{n}\sum_{i=1}^n h_i = \frac{p}{n}.
</span></li>
<li class="fragment">For small <span class="math inline">x &gt;0</span> it holds that <span class="math inline">(1 - x)^{-2} \approx 1 + 2x</span>. Then, we will write <span class="math display">
\text{GCV} \approx \frac{1}{n}\sum_{i=1}^n(y_i - \hat{f}(\bm{x}_i))^2 + 2 \hat{\sigma}^2 p, \qquad \hat{\sigma}^2 =\frac{1}{n}\sum_{i=1}^n(y_i - \hat{f}(\bm{x}_i))^2,
</span> revealing a sharp connection with the <span class="math inline">C_p</span> of Mallows.</li>
</ul>
</div>
</section>
<section id="loo-cv-and-gcv-cholesterol-data" class="slide level2 center">
<h2>LOO-CV and GCV (<code>cholesterol</code> data)</h2>
<div class="cell" data-hash="un_B_cache/revealjs/unnamed-chunk-23_3299d29d05bed53e96ec3660867b6036">

</div>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-24-1.png" width="1200" class="r-stretch quarto-figure-center"></section>
<section id="on-the-choice-of-k" class="slide level2 center">
<h2>On the choice of <span class="math inline">K</span></h2>
<div>
<ul>
<li class="fragment"><p>Common choices are <span class="math inline">K = 5</span> or <span class="math inline">K = 10</span>. It is quite evident that a <span class="blue">larger <span class="math inline">K</span></span> requires more <span class="blue">computations</span>.</p></li>
<li class="fragment"><p>A <span class="math inline">K</span>-fold CV with <span class="math inline">K=5</span> or <span class="math inline">K = 10</span> is a (upwords) <span class="orange">biased estimate</span> of <span class="math inline">\text{Err}</span> because it uses less observations than those available (either <span class="math inline">4/5</span> or <span class="math inline">9/10</span>).</p></li>
<li class="fragment"><p>The LOO-CV has a very <span class="blue">small bias</span>, since each fit uses <span class="math inline">n-1</span> observations, but it has <span class="orange">high variance</span>, being the average of <span class="math inline">n</span> highly positively correlated quantities.</p></li>
<li class="fragment"><p>Indeed, the estimates <span class="math inline">\hat{f}_{-i}</span> and <span class="math inline">\hat{f}_{-i'}</span> have <span class="math inline">n - 2</span> observations in common. Recall that the variance of the sum is: <span class="math display">\text{var}(X + Y) = \text{var}(X) + \text{var}(Y) + 2\text{cov}(X, Y).</span></p></li>
<li class="fragment"><p>Overall, the choice is very much context dependent.</p></li>
</ul>
</div>
<!-- ## ☠️ - Optimism, random-$X$ -->
<!-- -   What is a good definition and estimate of the [optimism]{.orange} in -->
<!--     the very much realistic scenario where the covariates are random? -->
<!-- ![](img/randomX.png){fig-align="center"} -->
</section></section>
<section>
<section id="information-criteria" class="title-slide slide level1 center">
<h1>Information criteria</h1>

</section>
<section id="goodness-of-fit-with-a-penalty-term" class="slide level2 center">
<h2>Goodness of fit with a penalty term</h2>
<div>
<ul>
<li class="fragment"><p>The main statistical method for estimating unknown parameters of a model is the <span class="blue">maximize the log-likelihood</span> <span class="math inline">\ell(\theta) = \ell(\theta; y_1,\dots,y_n)</span>.</p></li>
<li class="fragment"><p>However, we cannot pick the value of <span class="math inline">p</span> that maximizes the log-likelihood (why not?)</p></li>
<li class="fragment"><p>We must take into account the different number of parameters, introducing a <span class="orange">penalty</span>: <span class="math display">
\text{IC}(p) = -2 \ell(\hat{\theta}) + \text{penalty}(p),
</span></p></li>
<li class="fragment"><p>The <span class="math inline">\text{IC}</span> is called an <span class="orange">information criterion</span>. We select the number of parameters that minimizes the <span class="math inline">\text{IC}</span>.</p></li>
<li class="fragment"><p>The choice of the specific penalty identifies a particular criterion.</p></li>
<li class="fragment"><p>An advantage of <span class="math inline">IC</span> is that they are based on the full dataset.</p></li>
</ul>
</div>
</section>
<section id="the-akaike-information-criterion-i" class="slide level2 center">
<h2>The Akaike information criterion I</h2>
<div>
<ul>
<li class="fragment"><p>Akaike suggested minimizing over <span class="math inline">p</span> the following <span class="orange">Kullback-Leibler divergence</span>: <span class="math display">
\text{KL}(p(\cdot; \theta_0) \mid\mid p(\cdot;\theta)) = \int p(\tilde{\bm{Y}};\theta_0) \log{p(\tilde{\bm{Y}};\theta_0)}\mathrm{d}\tilde{\bm{Y}} - \int p(\tilde{\bm{Y}};\theta_0)\log{p(\tilde{\bm{Y}};\theta)\mathrm{d}\tilde{\bm{Y}}},
</span> between the “true” model <span class="math inline">p(\bm{Y};\theta_0)</span> with parameter <span class="math inline">\theta_0</span> and the specified model <span class="math inline">p(\bm{Y};\theta)</span>.</p></li>
<li class="fragment"><p>In the above Kullback-Leibler, for any <span class="math inline">p</span> the value <span class="math inline">\theta</span> is replaced by the <span class="blue">maximum likelihood estimator</span> <span class="math inline">\hat{\theta} = \hat{\theta}(\bm{Y})</span>, using the data <span class="math inline">\bm{Y} = (Y_1,\dots,Y_n)</span>.</p></li>
<li class="fragment"><p>Ideally, we would select <span class="math inline">p</span> such that the expectation w.r.t. <span class="math inline">p(\bm{Y}; \theta_0)</span> <span class="math display">
\begin{aligned}
S(p) &amp;= 2 \: \mathbb{E}_{\theta_0}\left[\text{KL}(p(\cdot; \theta_0) \mid\mid p(\cdot;\hat{\theta}))\right] - \underbrace{2 \int p(\tilde{\bm{Y}};\theta_0) \log{p(\tilde{\bm{Y}};\theta_0)}\mathrm{d}\tilde{\bm{Y}}}_{\text{Does not depend on } p} \\
&amp; = - 2 \: \mathbb{E}_{\theta_0}\left[\int p(\tilde{\bm{Y}};\theta_0)\log{p(\tilde{\bm{Y}};\hat{\theta})\mathrm{d}\tilde{\bm{Y}}} \right]
\end{aligned}
</span> is <span class="orange">minimized</span>. However, we cannot compute nor minimize <span class="math inline">S(p)</span>, because <span class="math inline">\theta_0</span> is unknown.</p></li>
</ul>
</div>
</section>
<section id="the-akaike-information-criterion-ii" class="slide level2 center">
<h2>The Akaike information criterion II</h2>
<div>
<ul>
<li class="fragment"><p>Although <span class="math inline">S(p)</span> cannot be obtained, under technical conditions it turns out that <span class="math display">
\text{AIC} = -2 \ell(\hat{\theta}) + 2 p,
</span> namely the <span class="orange">Akaike information criterion</span> is a good estimator of <span class="math inline">S(p)</span>.</p></li>
<li class="fragment"><p>In practice, we will select the value of <span class="math inline">p</span> minimizing the <span class="math inline">\text{AIC}</span>, which is typically quite easy.</p></li>
<li class="fragment"><p>The factor <span class="math inline">2</span> is just a <span class="blue">cosmetic convention</span>, which has been introduced to match the quantities appearing in the usual asymptotic theory.</p></li>
<li class="fragment"><p>More formally, it can be proved (under technical conditions) that <span class="math display">
\mathbb{E}_{\theta_0}(\text{AIC}) + o(1) = S(p),
</span> for <span class="math inline">n \rightarrow \infty</span>.</p></li>
</ul>
</div>
</section>
<section id="aic-aicc-bic" class="slide level2 center">
<h2>AIC, AICc, BIC</h2>
<ul>
<li>Akaike’s original work was followed by several other proposals, different in their assumptions and the way they approximate certain quantities.</li>
</ul>
<div class="fragment">
<table>
<colgroup>
<col style="width: 21%">
<col style="width: 31%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th>Criterion</th>
<th>Author</th>
<th>Penalty</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\text{AIC}</span></td>
<td>Akaike</td>
<td><span class="math inline">2p</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\text{AIC}_c</span></td>
<td>Sugiura, Hurvich-Tsay</td>
<td><span class="math inline">2p + \frac{2p(p+1)}{n - (p +1)}</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\text{BIC}</span></td>
<td>Akaike, Schwarz</td>
<td><span class="math inline">p \log{n}</span></td>
</tr>
</tbody>
</table>
<ul>
<li><p>The <span class="math inline">\text{AIC}_c</span> is an <span class="orange">higher order correction</span> of the <span class="math inline">\text{AIC}</span> and the differences tends to be negligible for high values of <span class="math inline">n</span>.</p></li>
<li><p>The justification of <span class="math inline">\text{BIC}</span> is instead related to Bayesian statistics.</p></li>
<li><p>Since <span class="math inline">\log{n} &gt; 2</span> for any <span class="math inline">n &gt; 7</span>, it means that the <span class="math inline">\text{BIC}</span> <span class="blue">penalty</span> is typically <span class="blue">stronger</span> than the one of <span class="math inline">\text{AIC}</span>.</p></li>
</ul>
</div>
</section>
<section id="aic-and-bic-cholesterol-data" class="slide level2 center">
<h2>AIC and BIC (<code>cholesterol</code> data)</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-25-1.png" width="1200" class="r-stretch quarto-figure-center"></section>
<section id="an-overly-optimistic-summary" class="slide level2 center">
<h2>An overly optimistic summary</h2>
</section>
<section id="the-cholesterol-data-final-model-p-4" class="slide level2 center">
<h2>The <code>cholesterol</code> data: final model (<span class="math inline">p = 4</span>)</h2>

<img data-src="un_B_files/figure-revealjs/unnamed-chunk-26-1.png" width="1200" class="r-stretch quarto-figure-center"></section></section>
<section id="references" class="title-slide slide level1 center">
<h1>References</h1>
<ul>
<li><span class="blue">Main references</span>
<ul>
<li><strong>Chapter 3</strong> of Azzalini, A. and Scarpa, B. (2011), <a href="http://azzalini.stat.unipd.it/Book-DM/"><em>Data Analysis and Data Mining</em></a>, Oxford University Press.</li>
<li><strong>Chapter 7</strong> of Hastie, T., Tibshirani, R. and Friedman, J. (2009), <a href="https://hastie.su.domains/ElemStatLearn/"><em>The Elements of Statistical Learning</em></a>, Second Edition, Springer.</li>
</ul></li>
<li><span class="orange">Advanced references</span>
<ul>
<li>Rosset, S., and R. J. Tibshirani (2020). “<a href="https://doi.org/10.1080/01621459.2018.1424632">From fixed-X to random-X regression: bias-variance decompositions, covariance penalties, and prediction error Estimation</a>.” <em>Journal of the American Statistical Association</em> <strong>115</strong> (529): 138–51.</li>
<li>Bates, S., Hastie, T., and R. Tibshirani (2023). “<a href="https://doi.org/10.1080/01621459.2023.2197686">Cross-validation: what does it estimate and how well does it do it?</a>” Journal of the American Statistical Association, in press.</li>
</ul></li>
</ul>

<img src="img/logoB.png" class="slide-logo r-stretch"><div class="footer footer-default">
<p><a href="https://tommasorigon.github.io/datamining">Home page</a></p>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="un_B_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="un_B_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="un_B_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="un_B_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>