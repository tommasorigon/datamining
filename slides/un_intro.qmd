---
title: "Introduction"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    df-print: tibble
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_intro_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 150
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
    highlight-style: github
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 150
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---



## [Homepage](../index.html)

::: columns
::: {.column width="25%"}
![](img/intro.jpg){fig-align="center"}
:::

::: {.column width="75%"}
::: incremental
-   Nowadays, [predictive algorithms]{.blue} have become
    [mainstream]{.orange} in the [popular culture]{.orange} due to some
    spectacular successes:

    -   iPhone's Siri;
    -   Google translate;
    -   recommendation systems (Netflix challenge);
    -   business analytics (churn prediction, credit risk assessment);
    -   and, more recently, chatGPT.

-   And yet, there is a lot of [confusion]{.orange} about the
    [history]{.blue} and the [boundaries]{.blue} of the field. For
    instance, what is "data mining"?

-   And what are then the differences, [if any]{.orange}, with
    statistics, machine learning, statistical learning, and data
    science?

-   What applied problems cannot be solved with [classical
    statistical]{.orange} tools? Why?

-   Let us consider some real [case studies]{.blue}...
:::
:::
:::

## Traffic prediction in telecommunications

::: columns
::: {.column width="30%"}
![](img/phone.png){fig-align="center"}
:::

::: {.column width="70%"}
::: incremental
-   The marketing section of a telecommunications company is interested
    in analyzing the [customer behavior]{.orange}.

-   Hence, the data science team would like to [predict]{.blue}, for
    every single customer, the [telephone traffic]{.orange}.

-   Traffic is measured as the total [number of seconds]{.blue} of
    [outgoing calls]{.blue} made in a given month by each customer.

-   Appropriate estimations of the [overall traffic]{.orange} provide
    necessary elements for:

    -   predicting the company's budget;
    -   early identification of possible dissatisfaction;
    -   finding issues in the primary services of the company;
    -   spotting fraudulent situations.

-   The dataset has $n = 30.619$ customers and $p = 99$
    [covariates]{.blue}, i.e., the customer activity in [previous
    months]{.orange}.
:::
:::
:::

## Traffic prediction in telecommunications II

-   The focus is on [prediction]{.blue} and on [learning something
    useful]{.orange} from the data, not much on hypothesis testing.

. . .

-   These are [observational data]{.blue}, which have been collected for other purposes, not
    for their analysis. Data "exists," there is [no sampling
    design]{.orange}.

-   Data are [dirty]{.orange} and often stored in big data warehouse
    (DWH).

. . .

-   The [dimension of the data]{.blue} is [large]{.orange} in both
    directions: large $n$ and large $p$. Hence:

    -   All [p-values]{.orange} are [ultra-significant]{.orange} and not
        very informative in this setting;
    -   [Computations]{.blue} are a crucial analysis aspect.

. . .

-   The relationship between covariates and the response is complex,
    thus, it is hard to believe our models will be "true."
    They are [all wrong]{.orange}!

-   However, having a lot of data means we can [split]{.blue} them,
    using the first half for estimation and the other half for testing.

## Microarray cancer data

::: columns
::: {.column width="30%"}
![](img/gene.png){fig-align="center"}
:::

::: {.column width="70%"}
::: incremental
-   [Expression matrix]{.orange} of $p = 6830$ genes (rows) and $n = 64$
    samples (columns), for the [human tumor data]{.blue}.

-   100 randomly chosen rows are shown

-   The picture is a [heatmap]{.blue}, ranging from bright green (under-expressed) to bright red (overexpressed).

-   [Missing values]{.grey} are gray. The rows and columns are displayed
    in a randomly chosen order.

-   Goal: [predict]{.blue} cancer class based on expression values.

-   The main [statistical difficulty]{.orange} here is that $p > n$!

-   Logistic regression and discriminant analysis wouldn't work; the
    estimates do not exist.

-   Is it even possible to fit a model in this context?
:::
:::
:::

## The pitfalls of the old-fashioned way

::: incremental
-   All the previous case studies cannot be solved using traditional
    tools; in fact:

    -   there are [tons of variables]{.orange}, sometimes even with
        $p > n$ and most of them are irrelevant. It is not clear how to
        select the most useful ones.
    -   [p-values]{.blue} are always [ultra-significant]{.blue} and
        potentially meaningless.
    -   there are [no true models]{.orange} in these contexts. There is
        little hope that reality follows a linear specification.

-   The objective is [predicting]{.blue} a response variable in the
    most accurate way. Classical statistics has [broader
    goals]{.orange} including, but not limited to, prediction.

- We need a [paradigm shift]{.orange} to address the above issues.

-   For instance, if reality is non-linear, what about going [nonparametric]{.blue}? We could let the data speak without making any assumption about the relationship between $y$ and $\bm{x}$.

-   Moreover, if p-values and residual plots are no longer informative in this context, how do we [validate]{.orange} our predictions?
:::

## A highly influential paper (Breiman, 2001)

![](img/breiman1.png){fig-align="center"}

## Data models vs. algorithmic models

![](img/breiman2.png){fig-align="center"}

## Focus on predictive accuracy & business solutions

::: columns
::: {.column width="50%"}
![](img/breiman3.png){fig-align="center"}
:::

::: {.column width="50%"}
![Leo Breiman, 2003](img/breiman4.jpg){fig-align="left"}
:::
:::

-   After the Ph.D., Breiman resigned and went into full-time
    [free-lance consulting]{.orange}, and it worked as a consultant for
    thirteen years.

-   Breiman joined the UC Berkeley [Statistics]{.blue} Department in 1980. 

-   Leo Breiman died in 2005 at the age of 77. He [invented]{.orange}
    many of the mainstream predictive tools: CART, bagging, random
    forests, stacking.

## Statistical modeling: the two cultures

::: incremental
-   It is [tempting]{.orange} to fully embrace the [pure predictive
    viewpoint]{.blue}, as Breiman did in his career, especially in light
    of the recent media attention and public interest.

-   "*Statistical modeling: the two cultures*" has been a highly
    influential paper written by an outstanding statistician.

-   In some cases, the paper may sound exaggerated and at times
    [confrontational]{.orange}. These were different times.

-   It was also a [discussion paper]{.blue}!

-   Two other giants of the discipline, [Sir David Cox]{.blue} (died
    in 2022) and [Bradley Efron]{.orange} were among the discussants and
    raised several critical points.

-   It is [premature]{.orange} to delve into those criticisms. We will
    get back to them at the end of the course once you have enough
    knowledge to understand them.
:::

## Prerequisites

::: incremental
-   If you are in this class today, it means...

    -  You already studied a lot of [real analysis]{.orange}, [linear
        algebra]{.grey} and [probability]{.blue};

    -  You know how to [estimate the parameters]{.blue} of a
        statistical model, to construct and interpret confidence
        intervals, p-values, etc. You know the [principles]{.orange} of
        [inference]{.orange};

    -  You know how to [explore data]{.orange} using the **R**
        statistical software and other tools (SAS, python, etc.). You
        know [principal component analysis]{.blue} and perhaps even
        factor models;

    -  You know how to fit [linear models]{.blue} and how to interpret
        the associated empirical findings. You are familiar with $R^2$s,
        likelihood ratio tests, [logistic regression]{.orange}, and so
        on;

    -  You may have attended a course named "data mining" before, and
        studied essential tools like linear discriminant analysis, $k$-nearest neighbors...
:::

. . .

-   These classical statistical tools are the [prerequisites]{.orange} of [Data
    Mining M]{.blue}. We will start from there.

## Overview of the topics

| [Unit]{.orange}                    | [Description]{.blue}                                                                                  |
|------------------------------|----------------------------------------------|
| A-B-C                              | Linear models. Data modeling, the old-fashioned way. Advanced computations.                             |
| Optimism, conflicts and trade-offs | Bias-variance trade-off. Training and test paradigm, cross-validation. Information criteria, optimism |
| Shrinkage and variable selection   | Best subset selection, principal component regression. Ridge regression. Lasso and LARS. Elastic net. |
| Nonparametric estimation           | Local linear regression. Regression and smoothing splines.                                            |
| Additive models                    | Generalized additive models (GAM). Multivariate adaptive regression splines (MARS).                    |

. . .

-   Trees, bagging, random forests, boosting, neural networks, support
    vector machine are not in the program due to [time
    constraints]{.orange}... but you will study them in other courses!

## A tension between prediction and interpretability


<!-- - There is often a tension between the [interpretability]{.orange} of a method and its [flexibility]{.blue}. -->

- Important [caveat]{.blue}. Less flexible methods may have [more accurate predictions]{.orange} in many case studies, on top of being more interpretable!

<div class="flourish-embed flourish-scatter" data-src="visualisation/14498992"><script src="https://public.flourish.studio/resources/embed.js"></script></div>



## Predictive interpretability $\neq$ causality

![](img/causality.png){fig-align="center" width=50%}

::: incremental

- Predictive [interpretability]{.blue} means transparent understanding the [driving factors]{.blue} of the predictions. An example is linear models with few (highly relevant) variables. 

- For example, if I change the value of a set of covariates, what is the impact on predictions?

- This is [useful]{.blue}, especially within the context of [ethical]{.orange} AI and machine learning.

- However, the [predictive relevance]{.blue} of a variable does [not]{.orange} imply a [causal effect]{.orange} on the response. 

- Finding causal relationship requires [careful thinking]{.orange}, a suitable [sampling design]{.blue}, or both. 
:::

## A definition of "data mining"

::: callout-note
#### Azzalini & Scarpa (2011)
Data mining represents the work of processing, graphically or numerically, [large]{.orange} amounts or continuous streams of [data]{.orange}, with the aim of [extracting information]{.blue} useful to those who possess them.
:::

. . .

::: callout-tip
#### Hand et al. (2001)
Data mining is fundamentally an applied discipline [...] data mining requires an understanding of both [statistical]{.orange} and [computational]{.blue} issues. (p. xxviii)

[...]

The most fundamental difference between classical statistical applications and data mining is the [size]{.orange} of the [data]{.orange}. (p. 19)
:::

## Back to the 1800s

::: incremental
- At this point, it may sound natural to ask yourself: what is [statistics]{.orange}? 

- Statistics existed before data mining, machine learning, data science, and all these fancy new names. 

- Statistical regression methods trace back to Gauss and Legendre in the early 1800s. Their goal was indeed [prediction]{.blue}!
:::

. . .

:::callout-note
#### Davison (2003) 
Statistics concerns what can be [learned]{.orange} from [data]{.orange}.
:::

. . .

::: callout-tip
#### Hand (2011)
Statistics [...] is the technology of [extracting meaning]{.blue} from [data]{.blue}. 
:::


## 50 years of data science (Donoho, 2017)

::: columns
::: {.column width="50%"}
![](img/donoho1.png){fig-align="center"}
:::

::: {.column width="50%"}
![](img/donoho2.png){fig-align="center"}
:::
:::

## Statistics, an evolving science


::: columns
::: {.column width="50%"}
![](img/donoho3.png){fig-align="center"}
:::

::: {.column width="50%"}
::: incremental

- Sure, old-fashioned statistics is often insufficient to address [modern challenges]{.blue}.

- But statistics has profoundly changed over the years, [broadening]{.orange} its [boundaries]{.orange}. 

- The road was paved by [Tukey]{.orange} in the 60s, with further exhortations  by [Breiman]{.blue}.

- [Modern statistics]{.blue} encompasses [also]{.orange}:
  - Data gathering and representation;
  - Computational aspects;
  - Algorithmic and modeling culture to prediction.

- Feel free to call it "data science" if you like the bright, shiny new term. 
:::
:::
:::

## A glossary

::: incremental

- While it might seem that [data science]{.blue} and [data mining]{.blue} have [strong roots]{.orange} in [statistics]{.orange}, it cannot be denied the existence of two distinct, albeit often overlapping, [communities]{.blue}.

- For the lack of a better term, we will call these communities the [statisticians]{.orange} and the [computer scientists]{.blue}, as identified by their [background]{.blue} and studies. 

:::

. . .

| [Statisticians]{.orange} | [Computer Scientists]{.blue} |
|----------|----------|
| Parameters | Weights |
| Covariate | Feature |
| Observation | Instance |
| Response | Label |
| **R** | Python |
| Regression / Classification | Supervised learning |
| Density estimation, clustering | Unsupervised learning |
| Lasso / Ridge penalty |$L^1$ and $L^2$ penalty |

## Press the button?

::: columns
::: {.column width="20%"}
![](img/button.png){fig-align="center"}
:::

::: {.column width="80%"}
::: incremental

- Several "automatic" tools have been developed over the years, tempting generations of analysts with [automatic pipelines]{.blue}.

- Those who choose to "[press the button]{.orange}": 
    - do not know which method is used, they may only know the name of the method they are using;
    - are not aware of its [advantages]{.blue} and [disadvantages]{.orange}.
    
- More or less advanced [knowledge]{.blue} of the methods is essential for:
  - choosing the most suitable method;
  - interpreting the results.

- Competence in [computational aspects]{.blue} is helpful to evaluate better the output of the computer, e.g., in terms of its [reliability]{.orange}.

- If you are not making the choices, somebody else is!
:::
:::
:::


## A matter of style

![](img/leonardo.jpg){fig-align="center" height=8cm}


"*Quelli che s'innamoran di pratica sanza scienzia son come 'l nocchier ch'entra in navilio senza timone o bussola, che mai ha certezza dove si vada.*"

[Leonardo da Vinci]{.blue}

## Course material

::: columns

::: {.column width="15%"}
:::
::: {.column width="25%"}
![A&S (2011)](img/AS.png){fig-align="center"}
:::

::: {.column width="20%"}
:::

::: {.column width="25%"}
![HTF (2009)](img/HTF.png){fig-align="center"}
:::

::: {.column width="15%"}
:::

:::

- Azzalini, A. and Scarpa, B. (2011), [*Data Analysis and Data Mining*](http://azzalini.stat.unipd.it/Book-DM/), Oxford University Press.
- Hastie, T., Tibshirani, R. and Friedman, J.
        (2009), [*The Elements of Statistical
        Learning*](https://hastie.su.domains/ElemStatLearn/), Second
        Edition, Springer.
        
## Data mining people

<div class="flourish-embed flourish-cards" data-src="visualisation/15126433"><script src="https://public.flourish.studio/resources/embed.js"></script></div>

## Exam


- The exam is made of [two parts]{.orange}:

- (20 / 30) [Written examination]{.blue}: a pen-and-paper exam about the theoretical aspects of the course.

- (10 / 30) [Individual assignment]{.blue}: a data challenge. 
   - You will be given a prediction task, and you will need to submit your [predictions]{.orange} and produce a [report]{.orange} of about 4-5 pages;
   - You will make use of the [Kaggle](https://www.kaggle.com) platform;
   - The data challenge will be announced in the second half of the course.
   - Further info will be provided in due time.
  
- Both parts are [mandatory]{.orange}, and you need to submit the assignment [before]{.orange} attempting the written part. The report expires after one year from the end of the course.

- The final grade is obtained as the [sum]{.blue} of the above scores. 

## Epilogue

![](img/efron.jpg){fig-align="center" height=7cm}

"*Those who ignore statistics are condemned to reinvent it.*" 

[Bradley Efron]{.grey}, Stanford University.

## References

-   [Main references]{.blue}
    -   Breiman, L. (2001). [Statistical modeling: the two
        cultures](http://projecteuclid.org/euclid.ss/1009213726).
        *Statistical Science*, **16** (3), 199--215.
    -   Donoho, D. (2017). [50 years of data
        science](https://www.tandfonline.com/doi/full/10.1080/10618600.2017.1384734).
        *Journal of Computational and Graphical Statistics*, **26**,
        745-766
-   [Specialized references]{.orange}
    - Efron, B. (2020). [Prediction, Estimation, and Attribution](https://doi.org/10.1080/01621459.2020.1762613). *Journal of the American Statistical Association* **115** (530), 636–55.
    -   Tukey, J. W. (1962). [The future of data
        analysis](https://doi.org/10.1162/99608f92.f09d7aa3). *Annals of
        Mathematical Statistics*, **33**, 1--67.
