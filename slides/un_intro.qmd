---
title: "Introduction"
subtitle: "Data mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
execute:
  cache: true
format:
  revealjs:
    html-math-method: katex
    transition: slide
    output-file: un_intro_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    fig-dpi: 250
    echo: false
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
  html:
    html-math-method: katex
    echo: true
    callout-appearance: minimal
    theme: [cosmo, template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    fig-dpi: 250
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## Errore quadratico medio di previsione

-   Si consideri un vettore di variabili esplicative a valori reali
    $x \in \mathbb{R}^p$.

-   Interessa penalizzare gli errori di previsione utilizzando la
    [funzione di perdita quadratica]{.blue} $$
    \mathscr{L}(y, f(x)) = (y - f(x))^2.
    $$

-   Il criterio di scelta di $f$ risulta essere l'[errore quadratico
    medio]{.orange} di previsione $$
    \mathbb{E}\{(y - f(x))^2 \}.
    $$

-   La cui soluzione di minimo è $$
    f(x) = \mathbb{E}\{y \mid x = x_0\},
    $$ il valore atteso condizionato, ovvero la [funzione di
    regressione]{.blue}.

## Slide per caterina

- Quanto fa 156 * 2867?

- Risposta: `r 156 * 286`

## Another slide

```{r}
#| echo: false

library(ggplot2)
ggplot(mtcars, aes(hp, mpg)) +
  geom_point() +
  geom_smooth(formula = y ~ x, method = "loess", se = F) + theme_bw() + 
  theme(legend.position = 'top')
```
