---
title: "Additive models"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    df-print: tibble
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_F_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 200
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 200
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_F.qmd", output = "../code/un_F.R", documentation = 0)
styler:::style_file("../code/un_F.R")
```

::: columns
::: {.column width="35%"}
![](img/reef.jpg){width="80%" fig-align="center"}

::: {style="font-size: 90%;"}
The [Great Barrier
Reef](https://en.wikipedia.org/wiki/Great_Barrier_Reef)
:::
:::

::: {.column width="65%"}
-   In this unit we will cover the following [topics]{.orange}:

    -   Generalized additive models (GAMs)
    -   Multivariate Adaptive Regression Splines (MARS)

-   We have seen that [fully nonparametric]{.blue} methods are plagued
    by the [curse of dimensionality]{.orange}.

-   GAMs and MARS are [semi-parametric]{.blue} approaches that keep the
    model complexity under control so that:

    -   they are more flexible than linear models;
    -   they are not hugely impacted by the curse of dimensionality.

-   The running example is about [trawl data]{.orange} from the [Great
    Barrier Reef]{.blue}.
:::
:::

## The `trawl` dataset

```{r}
#| message: false
rm(list = ls())
library(ggplot2)
library(ggthemes)
library(sm)

data(trawl)
```

```{r}
trawl <- na.omit(trawl)
trawl$Year <- factor(trawl$Year)
levels(trawl$Year) <- c("1992", "1993")
trawl$Zone <- factor(trawl$Zone)
levels(trawl$Zone) <- c("Open", "Closed")
# Data splitting
set.seed(1234)
id_train <- sample(1:nrow(trawl), size = floor(0.80 * nrow(trawl)), replace = FALSE)
id_test <- setdiff(1:nrow(trawl), id_train)
trawl_train <- trawl[id_train, ]
trawl_test <- trawl[id_test, ]
```

::: incremental
-   We consider the `trawl` dataset, which refers to a [survey]{.blue}
    of the [fauna]{.blue} on the sea bed lying between the coast of
    northern Queensland and the [Great Barrier Reef]{.orange}.

-   The [response]{.blue} variable is `Score`, which is a standardized
    numeric quantity measuring the amount of fishes caught on a given
    location.

-   We want to [predict]{.blue} the [catch score]{.orange}, as a
    function of a few covariates:

    -   the `Latitude` and `Longitude` of the sampling position. The
        longitude can be seen as a proxy of the distance from the coast
        in this specific experiment;
    -   the `Depth` of the sea on the sampling position;
    -   the `Zone` of the sampling region, either open or closed to
        [commercial fishing]{.blue};
    -   the `Year` of the sampling, which can be either `1992` or
        `1993`.

-   Having remove a few observations due to missingness, we split the
    data into [training]{.blue} (119 obs.) and [test]{.orange} set (30
    obs.). The full `trawl` dataset is available in the `sm` R package.
:::

## The `trawl` dataset

::: {.flourish-embed .flourish-map data-src="visualisation/15070357"}
```{=html}
<script src="https://public.flourish.studio/resources/embed.js"></script>
```
:::

## Getting started: linear models

-   Let begin our analysis by trying to predict the `Score` using a
    [linear model]{.orange} of the form $$
    y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_px_{ip}, \qquad i=1,\dots,n,
    $$

-   The above values correspond to the variables of the `trawl` dataset,
    so that $$
    \begin{aligned}
    \texttt{Score}_i = \beta_0 &+ \beta_1 \texttt{Latitude}_i + \beta_2 \texttt{Longitude}_i + \\
    &+ \beta_3\texttt{Depth}_i + \beta_4 I(\texttt{Zone}_i = \texttt{Closed}) + \beta_5 I(\texttt{Year}_i = \texttt{1993}).
    \end{aligned}
    $$

-   Such a model can be estimated using [ordinary least
    squares]{.orange}, resulting in:

```{r}
#| output: false
library(broom)
m_linear <- lm(Score1 ~ Latitude + Longitude + Depth + Zone + Year, data = trawl_train)
knitr::kable(tidy(summary(m_linear)), digits = 3)
```

::: {style="font-size: 75%;"}
| term          | estimate | std.error | statistic | p.value |
|:--------------|---------:|----------:|----------:|--------:|
|`(Intercept)` |  297.690|    26.821|    11.099|   0.000|
|`Latitude`    |    0.256|     0.222|     1.151|   0.252|
|`Longitude`   |   -2.054|     0.187|   -10.955|   0.000|
|`Depth`       |    0.020|     0.007|     3.003|   0.003|
|`Zone_Closed`  |   -0.116|     0.102|    -1.143|   0.255|
|`Year_1993`    |    0.127|     0.103|     1.242|   0.217|
:::

## Scatterplot with `loess` estimate

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center
#| message: false
library(ggthemes)
ggplot(data = trawl_train, aes(x = Longitude, y = Score1, col = Year)) +
  geom_point(size = 1) +
  geom_smooth(se = FALSE, span = 0.4, col = "#a3acb9", linetype = "dashed") +
  scale_color_tableau(palette = "Color Blind") +
  theme_minimal() +
  theme(legend.position = "top") +
  xlab("Longitude of the sampling position") +
  ylab("Catch score")
```

## Comments and cricism of linear models

-   Is this a good model?

. . .

-   Granted that every model is just an approximation of reality, it is
    undeniable that there are some [problematic]{.orange} aspects.

. . .

-   By simple graphical inspection, it seems that the relationship
    between `Score` and `Longitude` is [non-linear]{.orange}.

-   Also, an [interaction effect]{.blue} between `Year` and `Longitude`
    could be present.

. . .

-   These considerations support the idea that [nonparametric
    approach]{.blue} might be more appropriate.

-   However, the number of covariates is $p = 5$ and therefore a fully
    nonparametric estimation would [not]{.orange} be
    [feasible]{.orange}, because of the curse of dimensionality.

-   We need a simplified modelling strategy, that accounts for
    non-linearities but at the same time is not fully flexible.

# Generalized additive models (GAM)

## The ANOVA decomposition of a function

-   We seek for an estimate of (a suitable transformation of) the [mean
    function]{.blue}, namely $$
    g^{-1}\{\mathbb{E}(Y_i)\} = f(x_{i1},\dots,x_{ip}),
    $$ where $g^{-1}(\cdot)$ is the so-called [link function]{.orange}.

. . .

-   The [unknown]{.orange} multivariate function
    $f(\bm{x}) = f(x_1,\dots,x_p) : \mathbb{R}^p \rightarrow \mathbb{R}$
    is [too complex]{.orange}. However, the following
    [decomposition]{.blue} holds $$
    f(\bm{x}) = \beta_0 + \underbrace{\sum_{j=1}^p f_j(x_j)}_{\text{Main effect}} + \underbrace{\sum_{j=1}^p\sum_{k < j}f_{jk}(x_j, x_k)}_{\text{Interaction effect}} + \underbrace{\sum_{j=1}^p\sum_{k < j}\sum_{h < k < j}f_{jkh}(x_j, x_k, x_h)}_{\text{Higher order interaction}} + \cdots.
    $$

. . .

-   By imposing [suitable constraints]{.blue}, this decomposition can be
    made [unique]{.orange}.

-   More importantly, this decomposition gives us an intuition on how to
    build non-linear models with a simplified structure.

## Generalized additive models (GAM)

-   A [generalized additive model]{.blue} (GAM) presumes a
    representation of the following type: $$
    f(\bm{x}_i) = \beta_0 + f_1(x_{i1}) + \cdots + f_p(x_{ip}) = \beta_0 + \sum_{j=1}^pf_j(x_{ij}), \qquad i=1,\dots,n,
    $$ where $f_1,\dots,f_p$ are [smooth univariate]{.orange} functions
    with a potentially non-linear behavior.

. . .

-   In GAMs we include only the [main effects]{.blue} and we
    [exclude]{.orange} the [interactions]{.orange} terms.

-   Generalized linear models (GLMs) are a [special case]{.orange} of
    GAMs, in which $f_j(x_{ij}) = \beta_j x_{ij}$.

. . .

-   To avoid what is essentially a problem of [model
    identifiability]{.blue}, it is necessary for the various $f_j$ to be
    centered around $0$, that is $$
    \sum_{i=1}^n f_j(x_{ij}) = 0, \qquad j=1,\dots,p.
    $$

## The backfitting algorithm I

-   There exist several strategies for [estimating]{.orange} the
    [unknown functions]{.orange} $f_1,\dots,f_p$. One of them, called
    [backfitting]{.blue}, is particularly appealing because of its
    elegance and generality.

. . .

-   Suppose we model each
    $f_j(x) = \sum_{m = 1}^{M_j}\beta_{mj} h_{mj}(x)$ with a [basis
    expansion]{.orange}, for example using [regression splines]{.blue}.

-   In a [regression problem]{.orange} we need to minimize, over the
    unknown $\beta$ parameters, the loss $$
    \sum_{i=1}^n\left\{y_i - \beta_0 - \sum_{j=1}^pf_j(x_{ij})\right\}^2
    $$ subject to the constraint $\sum_{i=1}^n f_j(x_{ij}) = 0$.

. . .

-   When $f_j$ are regression splines, the above loss can be
    [minimized]{.orange} using [least squares]{.orange}. The
    identifiability issue could be handled by removing the intercept
    term from each spline basis.

-   However, here we consider an [alternative]{.orange} and
    [iterative]{.blue} minimization method, which is similar to the
    coordinate descent algorithm we employed for the elastic-net.

## The backfitting algorithm II

-   Now, let us re-arrange the term in the squared loss as follows: $$
      \sum_{i=1}^n\left\{\textcolor{red}{y_i - \beta_0 - \sum_{k\neq j}f_k(x_{ik})} - f_j(x_{ij})\right\}^2,
    $$ where the highlighted terms are sometimes called [partial
    residuals]{.blue}.

. . .

<!-- - If all the functions but the $j$th were known, the estimation would reduce to a [univariate nonparametric smoothing]{.orange} of the $j$th covariate using the partial residuals as response. -->

-   Hence, we can repeatedly and iteratively fit a [univariate
    smoothing]{.blue} model for $f_j$ using the [partial
    residuals]{.orange} as [response]{.orange}, keeping fixed the value
    of the other functions $f_k$, for $k \neq j$.

. . .

-   This algorithm produces the same fit of least squares when $f_j$ are
    regression splines, but the idea is appealing because it can be used
    with any [generic smoothers]{.blue} $\mathcal{S}_j$.

. . .

-   Finally, note that under the constraint
    $\sum_{i=1}^n f_j(x_{ij}) = 0$ the least square estimate for the
    [intercept]{.orange} term is $\hat{\beta}_0 = \bar{y}$, i.e. the
    arithmetic mean.

## The backfitting algorithm (regression)

::: callout-note
#### The backfitting algorithm for additive regression models

1.  Initialize $\hat{\beta}_0 = \bar{y}$ and set $f_j(x_j) = 0$, for
    $j=1,\dots,p$.

2.  Cycle $j =1,\dots,p$, $j =1,\dots,p$, $\dots$, until
    [convergence]{.orange}:

    i.  Update the $k$th function by smoothing via $\mathcal{S}_j$ the
        [partial residuals]{.blue}, so that $$
        \hat{f}_j(x) \leftarrow \mathcal{S}_j\left[\left\{x_{ij}, y_i - \hat{\beta}_0 - \sum_{k \neq j} \hat{f}_k(x_{ik})\right\}_{i=1}^n\right].
        $$

    ii. Center the function by subtracting its mean $$
        \hat{f}_j(x) \leftarrow \hat{f}_j(x) - \frac{1}{n}\sum_{i=1}^n\hat{f}_j(x_{ij}).
        $$
:::

## Backfitting: comments and considerations

-   The backfitting algorithm, when $f_j$ are modeled as [regression
    splines]{.blue}, is known as "Gauss-Seidel". The
    [convergence]{.orange} is [guaranteed]{.orange} under standard
    conditions.

-   Interestingly, even when $\mathcal{S}_j$ are [smoothing
    splines]{.blue} the [convergence]{.orange} of backfitting is
    [guaranteed]{.orange}; the proof for this statement is less
    straightforward.

. . .

-   In general, however, there is no theoretical guarantee that the
    algorithm will ever converge, even though the practical experience
    suggest that this is [not]{.orange} a [big concern]{.orange}.

. . .

-   When $\mathcal{S}_j$ is a [linear smoother]{.orange} with smoothing
    matrix $\bm{S}_j$, then by analogy with the previous unit we can
    define the [effective degrees of freedom]{.blue} of $\hat{f}_j$ as
    $$
    \text{df}_j = \text{tr}(\bm{S}_j).
    $$ The number of degrees of the whole model therefore is
    $\text{df} = 1 + \sum_{j=1}^p \text{df}_j$.

. . .

-   A variant of backfitting for classification problems is available.
    Once again, relying on [quadratic approximations]{.orange} of the
    log-likelihood allows for a generalization to GLMs.

## The backfitting algorithm (classification)

::: callout-note
#### Local scoring algorithm for additive logistic regression

1.  Initialize $\hat{\beta}_0 = \text{logit}(\bar{y})$ and set
    $f_j(x_j) = 0$, for $j=1,\dots,p$.

2.  Iterate [until convergence]{.orange}:

    i.  Define the quantities
        $\hat{\eta}_i = \hat{\beta}_0 + \sum_{j=1}^p\hat{f}_j(x_{ij})$
        and $\hat{\pi}_i = \{1 + \exp(-\hat{\eta}_i)\}^{-1}$.

    ii. Construct the [working response]{.orange} $$
        z_i = \hat{\eta}_i + \frac{y_i - \hat{\pi}_i}{\hat{\pi}_i(1 - \hat{\pi}_i)}, \qquad i=1,\dots,n.
        $$

    iii. Construct the [weights]{.orange}
         $w_i = \hat{\pi}_i(1 - \hat{\pi}_i)$, for $i=1,\dots,n$.

    iv. Use a [weighted backfitting]{.blue} algorithm using the $z_i$ as
        responses, which produces a new set of estimates
        $\hat{f}_1,\dots,\hat{f}_p$.
:::

## GAM using penalized splines

-   A common special instance of GAM occurs when [smoothing
    splines]{.blue} are employed. In the regression case, the
    [backfitting]{.orange} algorithm implicitly minimizes the following
    [penalized loss]{.orange} $$
    \mathscr{L}(f_1,\dots,f_p; \lambda) = \sum_{i=1}^n\left\{y_i - \beta_0 - \sum_{j=1}^pf_j(x_j)\right\}^2 + \sum_{j=1}^p\lambda_j \int_{a_j}^{b_j}\{f''_j(t)\}^2\mathrm{d}t,
    $$ where $\lambda = (\lambda_1,\dots,\lambda_p)$ is a vector of
    [smoothing parameters]{.blue}.

. . .

-   Each $f_j(x;\beta)$ is a [natural cubic spline]{.orange}, therefore
    the penalized least squares criterion is $$
    \mathscr{L}(\beta; \lambda) = \sum_{i=1}^n\left\{y_i - \beta_0 - \sum_{j=1}^pf_j(x_j; \beta_j)\right\}^2 + \sum_{j=1}^p\lambda_j \beta_j^T\bm{\Omega}_j\beta_j,
    $$ whose joint [minimization]{.blue} over $\beta$ is available in
    closed form.

-   Hence, a [direct algorithm]{.orange} that minimizes
    $\mathscr{L}(\beta; \lambda)$ is used instead of backfitting.

## On the choice of smoothing parameters

-   In GAMs there are $p$ [smoothing parameters]{.orange}
    $\lambda_1,\dots,\lambda_p$ that must be selected. We can proceed in
    the usual way, e.g. considering the [generalized
    cross-validation]{.blue} criteria: $$
    \text{GCV}(\lambda_1,\dots,\lambda_p) = \frac{1}{n}\sum_{i=1}^n \left(\frac{y_i - \hat{y}_i}{1 - \text{df}/n}\right)^2.
    $$

-   An alternative criterion in this context is the [REML]{.blue}
    (Restricted Maximum Likelihood), which is the [marginal
    likelihood]{.blue} of the corresponding [Bayesian model]{.orange}.

. . .

-   It is [not possible]{.orange} to construct a [grid]{.orange} of
    values for all the combinations of smoothing parameters
    $\lambda_1,\dots,\lambda_p$, because the number of terms increases
    exponentially in $p$.

. . .

-   Hence, many software packages [numerically optimize]{.blue} the
    $\text{GCV}(\lambda_1,\dots,\lambda_p)$, or other information
    criteria, as a function of $\lambda_1,\dots,\lambda_p$, using e.g.
    the Newton-Raphson method.

-   Such an approach is particularly convenient in combination with
    [smoothing splines]{.orange}, because the [derivatives]{.orange}
    needed for Newton's method are available in closed form.

## GAM and variable selection

::: incremental
-   When $p$ is large there is need to remove the potentially
    [irrelevant variables]{.orange}. There exist several [variable
    selection]{.blue} ideas for GAMs, but we will not cover the details
    here.

-   [Option 1]{.blue}. [Stepwise regression]{.orange}. Perhaps the
    simplest method, although it is not as efficient as in linear models
    because we cannot exploit the same computational tricks.

-   [Option 2]{.blue}. [COSSO: Component Selection and Smoothing
    Operator]{.orange} (Lin and Zhang, 2006). It's an idea based on
    combining lasso-type penalties and GAMs.

-   [Option 3]{.blue}. [SpAM: Sparse Additive Models]{.orange}
    (Ravikumar, Liu, Lafferty and Wasserman, 2009). Similar to the
    above, but it exploits a variation of the non-negative garrote.

-   [Option 4]{.blue}. [Double-penalty and shrinkage]{.orange} (Marra
    and Wood, 2011). It acts on the penalty term of smoothing splines so
    that high-values of $\lambda_1,\dots,\lambda_p$ leads to constant
    functions.

-   [Option X]{.blue}. [Fancy name]{.orange}. Yet another method for
    variable selection with GAMs.
:::

## GAM modeling of `trawl` data

-   Let us get back to the `trawl` data. A [specification]{.blue} based
    on GAM could be$$
      \begin{aligned}
      \texttt{Score}_i = \beta_0 &+  f_1(\texttt{Longitude}_i)+ f_2(\texttt{Latitude}_i) + f_3(\texttt{Depth}_i) +\\
      &+ \beta_1 I(\texttt{Zone}_i = \texttt{Closed}) + \beta_2 I(\texttt{Year}_i = \texttt{1993}).
      \end{aligned}
      $$

-   In GAMs the predictors are [not necessarily]{.blue} modeled using
    [nonparametric]{.blue} methods. Indeed, it is common to have a
    combination of smooth functions and linear terms.

-   Besides, it does [not make sense]{.orange} to "smooth" a [dummy
    variable]{.orange}.

. . .

::: {style="font-size: 75%;"}
```{r}
#| message: false
#| output: false
library(mgcv)
m_gam <- gam(Score1 ~ s(Longitude, bs = "tp") + s(Latitude, bs = "tp") + s(Depth, bs = "tp") + Zone + Year,
  data = trawl_train, method = "REML"
)
knitr::kable(tidy(m_gam, parametric = TRUE), digits = 3)
knitr::kable(tidy(m_gam, parametric = FALSE), digits = 3)
```

| term           | estimate | std.error |    df |
|:---------------|---------:|----------:|------:|
| `(Intercept)`  |    0.849|     0.088  |     1 |
| `Zone_Closed`  |   -0.075|       0.099|     1 |
| `Year_1993`    |    0.149|       0.093|     1 |
| `s(Longitude)` |       \- |        \- | 4.694 |
| `s(Latitude)`  |       \- |        \- | 1     |
| `s(Depth)`     |       \- |        \- | 2.447 |
:::

## Partial effect of GAMs (`Longitude`)

```{r}
#| fig-width: 7.8
#| fig-height: 4
#| fig-align: center
#| message: false
library(gratia)
data_plot <- smooth_estimates(m_gam, smooth = "s(Longitude)")

ggplot(data = data_plot, aes(x = Longitude, y = est)) +
  geom_line(linewidth = 1, col = "#1170aa") +
  geom_point(data = add_partial_residuals(m_gam, data = trawl_train), aes(x = Longitude, y = `s(Longitude)`), size = 0.7) +
  theme_minimal() +
  xlab("Longitude of the sampling position") +
  ylab("Partial effect")
```

## Partial effect of GAMs (`Latitude`)

```{r}
#| fig-width: 7.8
#| fig-height: 4
#| fig-align: center
#| message: false
data_plot <- smooth_estimates(m_gam, smooth = "s(Latitude)")

ggplot(data = data_plot, aes(x = Latitude, y = est)) +
  geom_line(linewidth = 1, col = "#1170aa") +
  geom_point(data = add_partial_residuals(m_gam, data = trawl_train), aes(x = Latitude, y = `s(Latitude)`), size = 0.7) +
  theme_minimal() +
  xlab("Latitude of the sampling position") +
  ylab("Partial effect")
```

## Partial effect of GAMs (`Depth`)

```{r}
#| fig-width: 7.8
#| fig-height: 4
#| fig-align: center
#| message: false
data_plot <- smooth_estimates(m_gam, smooth = "s(Depth)")

ggplot(data = data_plot, aes(x = Depth, y = est)) +
  geom_line(linewidth = 1, col = "#1170aa") +
  geom_point(data = add_partial_residuals(m_gam, data = trawl_train), aes(x = Depth, y = `s(Depth)`), size = 0.7) +
  theme_minimal() +
  xlab("Depth of the sampling position") +
  ylab("Partial effect")
```

## Comments and criticism (`trawl` data)

- The fitted GAM model highlights some interesting aspects of the `trawl` data. 

. . .

- In the first place, it seems confirmed that the `Longitude` has a [marked non-linear]{.orange} impact on the [catch score]{.blue}, as the initial analysis was suggesting.

- In particular, the catch score is high when the sampling location is close to the coast (but not too close!), and then it suddenly decreases.

. . .

- The effective degrees of freedom of `Latitude` is $\text{df}_2 = 1$, meaning that the estimated $\hat{f}_2$ [collapsed]{.blue} to a [linear term]{.blue}. The corresponding shrinkage parameter $\lambda_2$ is very high. 

- Overall, the effect due to the `Latitude` looks [small]{.orange} or [not present]{.orange} at all. 

. . .

- The `Depth` seems to have a [relevant effect]{.orange} on the `Score`, but this is likely due to a few [leverage points]{.blue} at the right extreme of the `Depth` range.  

. . .

- Finally, we note that both `Zone` and `Year` seem to have a minor effect.

## ☠️ - Naïve Bayes classifier and GAMs

-   The [naïve Bayes classifier]{.orange} expresses the [binary]{.blue} classification
    probability $\text{pr}(y = 1 \mid \bm{x})$ as $$
    \text{pr}(y = 1 \mid \bm{x}) = \frac{\pi_1 \prod_{j=1}^p p_{j1}(x_j)}{\pi_0\prod_{j=1}^p p_{j0}(x_j) + \pi_1\prod_{j=1}^p p_{j1}(x_j)} = \frac{\pi_1\prod_{j=1}^p p_{j1}(x_j)}{p(\bm{x})}.
    $$

-   Hence, using class $0$ as a [baseline]{.orange}, we can derive the following
    expression:$$
    \log\frac{\text{pr}(y = 1 \mid \bm{x})}{\text{pr}(y = 0 \mid \bm{x})} = \log\frac{\pi_1\prod_{j=1}^p p_{j1}(x_j)}{\pi_0\prod_{j=1}^p p_{j0}(x_j)} = \log\frac{\pi_1}{\pi_0} + \sum_{j=1}^p\log\frac{p_{j1}(x_j)}{p_{j0}(x_j)} = \beta_0 + \sum_{j=1}^pf_j(x_j).
    $$

. . .

-   Therefore, although naïve Bayes and GAMs are fitted in a quite
    different way, there is a [tight connection]{.orange} among the two
    methods.

-   Naïve Bayes has a [generalized additive model structure]{.blue}. This also suggests that the "[additive assumption]{.orange}" is linked to the notion of [independence]{.orange} among the covariates. 

## ☠️ - The `mgcv` **R** package

::: columns
::: {.column width="35%"}
![](img/iceberg.jpg){width="100%" fig-align="center"}
:::

::: {.column width="65%"}
::: incremental
- GAMs were [invented]{.orange} by Hastie and Tibshirani in 1986, including the backfitting algorithm.

- Simon Wood (2003) described [thin-plate regression splines]{.blue} and their estimation (no backfitting).

- Simon Wood (2004, 2011) invented methods for estimating $\lambda_1,\dots,\lambda_p$ in an [efficient]{.orange} and [stable]{.orange} manner.

- Marra and Wood (2011) discussed many methods for practical [variable selection]{.orange} for GAMs.

- For further details, there is a [recent and advanced book]{.blue} by Simon Wood (2017) entitled "*Generalized Additive Models: An Introduction with R*". 

- The `mgcv` package  in **R** (by Simon Wood) implements everything mentioned here.
:::
:::
:::

## Pros and cons of generalized additive models (GAMs)

::: callout-tip
#### Pros

- GAMs can automatically model [non-linear]{.orange} relationships. This can potentially make more accurate predictions for the response.

-   GAMs, as linear models, are [interpretable]{.blue}: the variation of the fitted response, holding all but one predictor fixed, [does not depend]{.orange} on the values of the [other predictors]{.orange}. 

- In practice, this means that we can [plot]{.orange} the [fitted functions]{.orange} $\hat{f}_j$ separately to examine the roles of the predictors in modelling the response.

- Additive assumption is quite strong, but it is still possible to [manually add interactions]{.blue} as in the linear regression case.
:::

. . .

::: callout-warning
#### Cons

-   Especially when $p$ is large, it is almost impossible to manually model all the [interactions]{.orange} among covariates. GAMs do [not]{.orange} take [second-order]{.orange} effects (or higher) into account. 
:::

# MARS

## [M]{.orange}ultivariate [A]{.orange}daptive [R]{.orange}egression [S]{.orange}plines

::: columns
::: {.column width="40%"}
![](img/mars.png)

:::

::: {.column width="60%"}
- MARS are a generalization of GAMs that avoid the [additivity assumption]{.orange}.

- MARS allow modeling of [non–linear interactions]{.blue} and not just non–linear marginal effects.

- MARS are a the same time: 
  - A generalization of [stepwise regression]{.blue};
  - A method based on multi-dimensional [tensor splines]{.orange};
  - A modification of [classification and regression trees]{.blue} (CART).
  
- MARS combine many of the techniques we have seen into a single sophisticated algorithm.
:::

:::

<!-- ```{r} -->
<!-- #| fig-width: 7.8 -->
<!-- #| fig-height: 4 -->
<!-- #| fig-align: center -->
<!-- x <- y <- seq(from = 0, to = 1, length = 50) -->
<!-- xy_seq <- expand.grid(x, y) -->

<!-- z <- matrix(apply(xy_seq, 1, function(x) pmax(0, x[1] - 0.5) * pmax(0, 0.75 - x[2])), nrow = length(x)) -->
<!-- persp(x, y, z, theta = -60, phi = 20, col = "#fc7d0b", shade = 0.5, xlab = "x1", ylab = "x2", zlab = "h(x)", ) -->
<!-- ``` -->

## MARS additive representation

- MARS is an [additive model]{.blue} of the form:
$$
f(\bm{x}; \beta) = \beta_0 + \sum_{m=1}^M \beta_m h_m(\bm{x}),
$$
where $h_m(\bm{x})$ are [basis functions]{.orange} and $\beta = (\beta_1,\dots,\beta_M)^T$ are regression coefficients. 

. . .

- Once the basis functions are specified, the estimate for $\hat{\beta}$ is straightforward, using for example [least squares]{.blue} or the IWLS algorithm in the classification case.

- The main distinction with GAMs is that in MARS the basis functions are [estimated]{.orange} from the [data]{.orange} and therefore they are not pre-specified in advance. 

. . .

- MARS is essentially a smart [heuristic algorithm]{.blue} for selecting a collection of basis functions $h_1(\bm{x}),\dots, h_M(\bm{x})$ that hopefully does not incur in the [curse of dimensionality]{.orange}.


## Basis functions for MARS (reflected pairs)

- The MARS algorithm begins by including just the [intercept term]{.blue}, i.e. $f(\bm{x}; \beta) = \beta_0$. Then, we proceed by [iteratively]{.orange} adding basis functions. 

. . .

- In MARS the basis functions are always [coupled]{.orange} (or [reflected]{.orange}), meaning that we always add them in pairs to the additive specification.

. . .

- Let us consider the following set of [pairs]{.blue} of [basis functions]{.blue} (linear splines):
$$
\mathcal{C} = \{(x_j - \xi)_+,  (\xi - x_j)_+ : \xi \in \{x_{1j},\dots,x_{nj}\}, \ j=1,\dots,p \}.
$$
For example, two basis functions could be $h_1(\bm{x}) = (x_1 - 0.5)_+$ and $h_2(\bm{x}) = (0.5 - x_1)_+$. 

- The knots are placed in correspondence of the observed data. Hence, there are in [total]{.orange} $2 n p$ [possible basis functions]{.orange} among which we can choose. 

. . .

- In the [first step]{.blue} of the MARS algorithm, we identify the pair $h_1(\bm{x}) = (x_j - \xi)_+$ and $h_2(\bm{x}) = (\xi - x_j)_+$ that, together with the [intercept]{.blue}, [minimize]{.orange} the [mean squared error]{.orange}.

## An example of reflected pair basis

```{r}
#| fig-width: 7.8
#| fig-height: 3.5
#| fig-align: center
x_seq <- seq(from = 0, to = 1, length = 200)
data_plot <- data.frame(
  x = x_seq,
  y = c(pmax(0, x_seq - 0.5), pmax(0, 0.5 - x_seq)),
  basis = rep(c("Basis 1", "Basis 2"), each = length(x_seq))
)
ggplot(data = data_plot, aes(x = x, y = y, col = basis, linetype = basis)) +
  geom_line() +
  scale_color_tableau(palette = "Color Blind") +
  theme_minimal() +
  geom_vline(xintercept = 0.5, linetype = "dotted") +
  theme(legend.position = "none") +
  xlab("x") +
  ylab("Basis function")
```

- The function $h_1(x) = (x - 0.5)_+$ ([blue]{.blue}) and its reflection $h_2(x) = (0.5 - x)_+$ ([orange]{.orange}). 

## A stepwise construction

::: incremental

- Hence, [after]{.orange} the [first step]{.orange} of the MARS algorithm, our model for example could be
$$
f(\bm{x}; \beta) = \beta_0 + \sum_{m=1}^2\beta_m h_m(\bm{x}) = \beta_0 + \beta_1 (x_1 - 0.5)_+ + \beta_2(0.5 - x_1)_+.
$$
- In the subsequent step, we consider a [new pair]{.blue} of basis functions $(x_j - \xi)_+,  (\xi - x_j)_+$ in $\mathcal{C}$, but this time we are allowed to perform two kind of operations:
     
     i. We can include the new pair to the predictor in an [additive]{.orange} way, obtaining for example $$
     f(\bm{x}; \beta) =  \beta_0 + \beta_1(x_1 - 0.5)_+ + \beta_2(0.5 - x_1)_+ + \beta_3\textcolor{red}{ (x_2 - 0.75)_+} + \beta_4\textcolor{red}{(0.75 - x_2)_+}.
     $$
     ii. We can include the new pair in a [multiplicative]{.orange} way, by considering the products between the [new basis]{.blue} and one of the [old basis]{.blue} of the model, obtaining for instance
     $$
     \begin{aligned}
     f(\bm{x}; \beta) =  \beta_0 &+ \beta_1\textcolor{darkblue}{(x_1 - 0.5)_+} + \beta_2(0.5 - x_1)_+ \\
     &+ \beta_3 \textcolor{red}{(x_1 - 0.5)_+(x_2 - 0.75)_+} + \beta_4 \textcolor{red}{(x_1 - 0.5)_+(0.75 - x_2)_+}.
     \end{aligned}
     $$
:::

## An example of tensor product basis

![](img/mars.png){width=50% fig-align="center"}

- The [product]{.orange} function $h(\bm{x}) = (x_1 - 0.5)_+ (x_2 - 0.75)_+$ in the range $(0, 1)^2$. 


## MARS algorithm (regression)

::: callout-note
#### MARS algorithm (degree $d$)

1. Initialize $f(\bm{x}; \beta) = \beta_0$ and let $K$ be [maximum number of pairs]{.orange}, so that $M = 2K$.

2. Identify the [initial pair]{.blue} of basis functions $h_1$ and $h_2$ in $\mathcal{C}$ that minimize the mean squared error. Then, let $h_0(\bm{x}) = 1$ and set $\mathcal{M}_1 = \{h_0, h_1, h_2\}$.

3.  For $k = 1,\dots,K - 1$, do:

    i.  Let $\mathcal{M}_k = \{h_0, h_1,\dots,h_{2k}\}$ be the basis functions already present in the model.

    ii. Consider a [novel pair]{.blue} of bases $\tilde{h}_1,\tilde{h}_2 \in \mathcal{C} \setminus \mathcal{M}_k$. A [candidate pair]{.orange} is obtained [multiplying]{.orange} $\tilde{h}_1,\tilde{h}_2$ with one of the bases in $\mathcal{M}_k$. Note that $h_0 \in \mathcal{M}_k$.
    
    iii. A [valid]{.orange} candidate pair does not contain the same variable $x_j$ more than once in the product and the involves at most [$d$ product terms]{.blue}. 
    
    iv. Identify the [optimal pair]{.blue} among the candidates at step (ii-iii) that [reduces]{.orange} the [mean squared error]{.orange} the most. This results in a new pair of bases $h_{2k+1}, h_{2k+2}$.
    
    v. Set $\mathcal{M}_{k+1} \leftarrow \mathcal{M}_k \cup \{h_{2k+1}, h_{2k+2}\}$.

:::


## Pruning and backward regression

## Heuristics behind MARS

## MARS modeling of `trawl` data I

```{r}
#| message: false
#| output: false
library(earth)
m_mars_deg1 <- earth(Score1 ~ Zone + Year + Latitude + Longitude + Depth, data = trawl_train, degree = 1, pmethod = "exhaustive", penalty = 3, nk = 15)
summary(m_mars_deg1, style = "pmax")
# plotmo(m_mars_deg1)
```

## MARS modeling of `trawl` data II

```{r}
#| output: false
m_mars_deg2 <- earth(Score1 ~ Zone + Year + Latitude + Longitude + Depth,
  data = trawl_train, degree = 2,
  pmethod = "exhaustive", penalty = 3, trace = TRUE, nk = 15
)
summary(m_mars_deg2, style = "pmax")
# plotmo(m_mars_deg2)
```

## Partial plots

```{r}
library(pdp)
partial_linear <- partial(m_linear, pred.var = c("Longitude", "Year"), grid.resolution = 40)
partial_gam <- partial(m_gam, pred.var = c("Longitude", "Year"), grid.resolution = 40)
partial_mars_deg1 <- partial(m_mars_deg1, pred.var = c("Longitude", "Year"), grid.resolution = 40)
partial_mars_deg2 <- partial(m_mars_deg2, pred.var = c("Longitude", "Year"), grid.resolution = 40)
```

::: panel-tabset
## Linear model

```{r}
#| fig-width: 9
#| fig-height: 4.5
#| fig-align: center
#| message: false
ggplot(data = trawl_train, aes(x = Longitude, y = Score1, col = Year)) +
  geom_point(size = 1) +
  facet_grid(. ~ Year) +
  geom_line(data = partial_linear, aes(x = Longitude, y = yhat)) +
  scale_color_tableau(palette = "Color Blind") +
  theme_light() +
  theme(legend.position = "none") +
  xlab("Longitude of the sampling position") +
  ylab("Catch score")
```

## GAM model

```{r}
#| fig-width: 9
#| fig-height: 4.5
#| fig-align: center
#| message: false
ggplot(data = trawl_train, aes(x = Longitude, y = Score1, col = Year)) +
  geom_point(size = 1) +
  facet_grid(. ~ Year) +
  geom_line(data = partial_gam, aes(x = Longitude, y = yhat)) +
  scale_color_tableau(palette = "Color Blind") +
  theme_light() +
  theme(legend.position = "none") +
  xlab("Longitude of the sampling position") +
  ylab("Catch score")
```

## MARS (degree 1)

```{r}
#| fig-width: 9
#| fig-height: 4.5
#| fig-align: center
#| message: false
ggplot(data = trawl_train, aes(x = Longitude, y = Score1, col = Year)) +
  geom_point(size = 1) +
  facet_grid(. ~ Year) +
  geom_line(data = partial_mars_deg1, aes(x = Longitude, y = yhat)) +
  scale_color_tableau(palette = "Color Blind") +
  theme_light() +
  theme(legend.position = "none") +
  xlab("Longitude of the sampling position") +
  ylab("Catch score")
```

## MARS (degree 2)

```{r}
#| fig-width: 9
#| fig-height: 4.5
#| fig-align: center
#| message: false
ggplot(data = trawl_train, aes(x = Longitude, y = Score1, col = Year)) +
  geom_point(size = 1) +
  facet_grid(. ~ Year) +
  geom_line(data = partial_mars_deg2, aes(x = Longitude, y = yhat)) +
  scale_color_tableau(palette = "Color Blind") +
  theme_light() +
  theme(legend.position = "none") +
  xlab("Longitude of the sampling position") +
  ylab("Catch score")
```
:::

<!-- ## The `Hitters` dataset -->

<!-- ```{r} -->

<!-- library(pdp) -->

<!-- gam_partial <- partial(m_gam_selected, pred.var = c("Years", "CHits"), grid.resolution = 40) -->

<!-- autoplot(gam_partial) + theme_light() -->

<!-- ``` -->

## Final results

```{r}
y_test <- trawl_test$Score1

y_null <- mean(trawl_test$Score1)
y_hat_linear <- predict(m_linear, newdata = trawl_test)
y_hat_gam <- predict(m_gam, newdata = trawl_test)
y_hat_mars_deg1 <- predict(m_mars_deg1, newdata = trawl_test)
y_hat_mars_deg2 <- predict(m_mars_deg2, newdata = trawl_test)

tab_results <- rbind(
  c(
    mean(abs(y_test - y_null)),
    mean(abs(y_test - y_hat_linear)),
    mean(abs(y_test - y_hat_gam)),
    mean(abs(y_test - y_hat_mars_deg1)),
    mean(abs(y_test - y_hat_mars_deg2))
  ),
  sqrt(c(
    mean(abs(y_test - y_null)^2),
    mean(abs(y_test - y_hat_linear)^2),
    mean(abs(y_test - y_hat_gam)^2),
    mean(abs(y_test - y_hat_mars_deg1)^2),
    mean(abs(y_test - y_hat_mars_deg2)^2)
  ))
)
colnames(tab_results) <- c("Null model", "Linear model", "GAM", "MARS (degree 1)", "MARS (degree 2)")
rownames(tab_results) <- c("MAE", "RMSE")
knitr::kable(tab_results, digits = 3)
```

## Pros and cons of MARS

::: callout-tip
#### Pros

-   asd
:::

. . .

::: callout-warning
#### Cons

-   asd
:::

## References

-   
