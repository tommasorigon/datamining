---
title: "Additive models"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    df-print: tibble
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_F_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 200
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 200
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_F.qmd", output = "../code/un_F.R")
styler:::style_file("../code/un_F.R")
```

::: columns
::: {.column width="35%"}
![](img/reef.jpg){width=80% fig-align="center"} 


::: {style="font-size: 90%;"}
The [Great Barrier Reef](https://en.wikipedia.org/wiki/Great_Barrier_Reef)
:::


:::

::: {.column width="65%"}

-   In this unit we will cover the following [topics]{.orange}:

    - Generalized additive models (GAMs)
    - Multivariate Adaptive Regression Splines (MARS)

- We have seen that [fully nonparametric]{.blue} methods are plagued by the [curse of dimensionality]{.orange}.

- GAMs and MARS are [semi-parametric]{.blue} approaches that keep the model complexity under control so that: 
  - they are more flexible than linear models; 
  - they are not hugely impacted by the curse of dimensionality.
    
- The running example is about [trawl data]{.orange} from the [Great Barrier Reef]{.blue}.
:::
:::

## The `trawl` dataset

```{r}
#| message: false
rm(list = ls())
library(ggplot2)
library(ggthemes)
library(sm)

data(trawl)
```

```{r}
trawl <- na.omit(trawl)
trawl$Year <- factor(trawl$Year)
levels(trawl$Year) <- c("1992", "1993")
trawl$Zone <- factor(trawl$Zone)
levels(trawl$Zone) <- c("Open", "Closed")
# Data splitting
set.seed(123)
id_train <- sample(1:nrow(trawl), size = floor(0.80 * nrow(trawl)), replace = FALSE)
id_test <- setdiff(1:nrow(trawl), id_train)
trawl_train <- trawl[id_train, ]
trawl_test <- trawl[id_test, ]
```



::: incremental

- We consider the `trawl` dataset, which refers to a [survey]{.blue} of the [fauna]{.blue} on the sea bed lying between the coast of northern Queensland and the [Great Barrier Reef]{.orange}. 

- The [response]{.blue} variable is `Score`, which is a standardized numeric quantity measuring the amount of fishes caught on a given location.

- We want to [predict]{.blue} the [catch score]{.orange}, as a function of a few covariates: 
  - the `Latitude` and `Longitude` of the sampling position. The longitude can be seen as a proxy of the distance from the coast in this specific experiment;
  - the `Depth` of the sea on the sampling position;
  - the `Zone` of the sampling region, either open or closed to [commercial fishing]{.blue};
  - the `Year` of the sampling, which can be either `1992` or `1993`. 



- Having remove a few observations due to missingness, we split the data into [training]{.blue} (119 obs.) and [test]{.orange} set (30 obs.). The full `trawl` dataset is available in the `sm` R package. 
:::

## The `trawl` dataset

<div class="flourish-embed flourish-map" data-src="visualisation/15070357"><script src="https://public.flourish.studio/resources/embed.js"></script></div>

## Getting started: linear models

- Let begin our analysis by trying to predict the `Score` using a [linear model]{.orange} of the form
$$
y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_px_{ip}, \qquad i=1,\dots,n,
$$
- The above values correspond to the variables of the `trawl` dataset, so that
$$
\begin{aligned}
\texttt{Score}_i = \beta_0 &+ \beta_1 \texttt{Latitude}_i + \beta_2 \texttt{Longitude}_i + \\
&+ \beta_3\texttt{Depth}_i + \beta_4 I(\texttt{Zone}_i = \texttt{Closed}) + \beta_5 I(\texttt{Year}_i = \texttt{1993}).
\end{aligned}
$$

- Such a model can be estimated using [ordinary least squares]{.orange}, resulting in:

```{r}
#| output: false
library(broom)
m_linear <- lm(Score1 ~ Latitude + Longitude + Depth + Zone + Year, data = trawl_train)
knitr::kable(tidy(summary(m_linear)), digits = 3)
```

::: {style="font-size: 75%;"}
|term        | estimate| std.error| statistic| p.value|
|:-----------|--------:|---------:|---------:|-------:|
|`(Intercept)` |  290.750|    23.314|    12.471|   0.000|
|`Latitude`    |    0.200|     0.192|     1.041|   0.300|
|`Longitude`   |   -2.010|     0.161|   -12.461|   0.000|
|`Depth`       |    0.016|     0.006|     2.624|   0.010|
|`Zone_Closed`  |   -0.109|     0.092|    -1.181|   0.240|
|`Year_1993`    |    0.314|     0.090|     3.504|   0.001|
:::

## Scatterplot with `loess` estimate

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center
#| message: false
library(ggthemes)
ggplot(data = trawl_train, aes(x = Longitude, y = Score1, col = Year)) +
  geom_point(size = 1) +
  geom_smooth(se = FALSE, span = 0.4, col = "#a3acb9", linetype = "dashed") +
  scale_color_tableau(palette = "Color Blind") +
  theme_minimal() +
  theme(legend.position = "top") +
  xlab("Longitude of the sampling position") +
  ylab("Catch score")
```


## Comments and cricism

- Is this a good model? 

. . .

- Granted that every model is just an approximation of reality, it is undeniable that there are some [problematic]{.orange} aspects.

. . .

- By simple graphical inspection, it seems that the relationship between `Score` and `Longitude` is [non-linear]{.orange}.

- Also, an [interaction effect]{.blue} between `Year` and `Longitude` seems to be present. 

. . .

- These considerations support the idea that [nonparametric approach]{.blue} might be more appropriate.

- However, the number of covariates is $p = 5$ and therefore a fully nonparametric estimation would [not]{.orange} be [feasible]{.orange}, because of the curse of dimensionality. 

- We need a simplified modelling strategy, that accounts for non-linearities but at the same time is not fully flexible. 


# Generalized additive models (GAM)

## The ANOVA decomposition of a function

- We seek for an estimate of (a suitable transformation of) the [mean function]{.blue}, namely
$$
g^{-1}\{\mathbb{E}(Y_i)\} = f(x_{i1},\dots,x_{ip}),
$$
where $g^{-1}(\cdot)$ is the so-called [link function]{.orange}.

. . .

- The [unknown]{.orange} multivariate function $f(\bm{x}) = f(x_1,\dots,x_p) : \mathbb{R}^p \rightarrow \mathbb{R}$ is [too complex]{.orange}. However, the following [decomposition]{.blue} holds
$$
f(\bm{x}) = \beta_0 + \underbrace{\sum_{j=1}^p f_j(x_j)}_{\text{Main effect}} + \underbrace{\sum_{j=1}^p\sum_{k < j}f_{jk}(x_j, x_k)}_{\text{Interaction effect}} + \underbrace{\sum_{j=1}^p\sum_{k < j}\sum_{h < k < j}f_{jkh}(x_j, x_k, x_h)}_{\text{Higher order interaction}} + \cdots.
$$

. . .

- By imposing [suitable constraints]{.blue}, this decomposition can be made [unique]{.orange}.

- More importantly, this decomposition gives us an intuition on how to build non-linear models with a simplified structure. 

## Generalized additive models (GAM)

- A [generalized additive model]{.blue} (GAM) presumes a representation of the following type: $$
f(\bm{x}_i) = \beta_0 + f_1(x_{i1}) + \cdots + f_p(x_{ip}) = \beta_0 + \sum_{j=1}^pf_j(x_{ij}), \qquad i=1,\dots,n,
$$
where $f_1,\dots,f_p$ are [smooth univariate]{.orange} functions with a potentially non-linear behavior. 

. . .

- In GAMs we include only the [main effects]{.blue} and we [exclude]{.orange} the [interactions]{.orange} terms. 

- Generalized linear models (GLMs) are a [special case]{.orange} of GAMs, in which $f_j(x_{ij}) = \beta_j x_{ij}$.

. . .

- To avoid what is essentially a problem of [model identifiability]{.blue}, it is necessary for the various $f_j$ to be centered around $0$, that is $$
\sum_{i=1}^n f_j(x_{ij}) = 0, \qquad j=1,\dots,p.
$$

## The backfitting algorithm I

## The backfitting algorithm II

::: callout-note
#### The backfitting algorithm for additive regression models

1.  Initialize $\hat{\beta}_0 = \bar{y}$ and set $f_j(x_j) = 0$, for $j=1,\dots,p$.

2.  Cycle $j =1,\dots,p$, $j =1,\dots,p$, $\dots$, until [convergence]{.orange}:

    i. Update the $k$th function by smoothing via $\mathcal{S}_j$ the [partial residuals]{.blue}, so that $$
    \hat{f}_j(x) \leftarrow \mathcal{S}_j\left[\left\{x_{ij}, y_i - \hat{\beta}_0 - \sum_{k \neq j} \hat{f}_k(x_{ik})\right\}_{i=1}^n\right].
    $$

    iii. Center the function by subtracting its
    $$
    \hat{f}_j(x) \leftarrow \hat{f}_j(x) - \frac{1}{n}\sum_{i=1}^n\hat{f}_j(x_{ij}).
    $$
:::

## The backfitting algorithm III

## Local scoring for additive logistic regression

## GAM using penalized splines

## GAM using penalized splines

## On the choice of smoothing parameters

## GAM and variable selection

## The `trawl` data

```{r}
library(mgcv)
m_gam <- gam(Score1 ~ s(Longitude, bs = "tp") + s(Latitude, bs = "tp") + s(Depth, bs = "tp") + Zone + Year,
  data = trawl_train, method = "REML")
knitr::kable(tidy(m_gam), digits = 3)
```

## Partial effect of GAMs (`Longitude`)

```{r}
#| fig-width: 7.8
#| fig-height: 4
#| fig-align: center
#| message: false
library(gratia)
data_plot <- smooth_estimates(m_gam, smooth = "s(Longitude)")

ggplot(data = data_plot, aes(x = Longitude, y = est)) +
  geom_line(linewidth = 1, col = "#1170aa") +
  geom_point(data = add_partial_residuals(m_gam, data = trawl_train), aes(x = Longitude, y = `s(Longitude)`), size = 0.7) +
  theme_minimal() +
  xlab("Longitude of the sampling position") +
  ylab("Partial effect")
```

## GAMs with interactions

## ☠️ - Naïve Bayes classifier and GAMs

## ☠️ - The `mgcv` R package

::: columns
::: {.column width="35%"}
![](img/iceberg.jpg){width=100% fig-align="center"} 

:::

::: {.column width="65%"}

-   
:::
:::


## Pros and cons of generalized additive models (GAMs)

::: callout-tip
#### Pros

-   asd
:::

. . .

::: callout-warning
#### Cons

- asd
:::


# MARS

## MARS


```{r}
library(earth)
m_mars_deg1 <- earth(Score1 ~ Zone + Year + Latitude + Longitude + Depth, data = trawl_train, degree = 1, pmethod = "exhaustive", penalty = 3)
# summary(m_mars_deg1, style = "pmax")
# plotmo(m_mars_deg1)
```


```{r}
m_mars_deg2 <- earth(Score1 ~ Zone + Year + Latitude + Longitude + Depth, data = trawl_train, degree = 2, 
                     pmethod = "exhaustive", penalty = 3, trace = TRUE, nk = 15)
# summary(m_mars_deg2, style = "pmax")
# plotmo(m_mars_deg2)
```


## Partial plots

```{r}
library(pdp)
partial_linear <- partial(m_linear, pred.var = c("Longitude", "Year"), grid.resolution = 40)
partial_gam <- partial(m_gam, pred.var = c("Longitude", "Year"), grid.resolution = 40)
partial_mars_deg1 <- partial(m_mars_deg1, pred.var = c("Longitude", "Year"), grid.resolution = 40)
partial_mars_deg2 <- partial(m_mars_deg2, pred.var = c("Longitude", "Year"), grid.resolution = 40)
```


::: panel-tabset

## Linear model

```{r}
#| fig-width: 9
#| fig-height: 4.5
#| fig-align: center
#| message: false
ggplot(data = trawl_train, aes(x = Longitude, y = Score1, col = Year)) +
  geom_point(size = 1) +
  facet_grid(.~Year) + 
  geom_line(data = partial_linear, aes(x = Longitude, y = yhat)) +
  scale_color_tableau(palette = "Color Blind") +
  theme_light() +
  theme(legend.position = "none") +
  xlab("Longitude of the sampling position") +
  ylab("Catch score")
```

## GAM model

```{r}
#| fig-width: 9
#| fig-height: 4.5
#| fig-align: center
#| message: false
ggplot(data = trawl_train, aes(x = Longitude, y = Score1, col = Year)) +
  geom_point(size = 1) +
  facet_grid(.~Year) + 
  geom_line(data = partial_gam, aes(x = Longitude, y = yhat)) +
  scale_color_tableau(palette = "Color Blind") +
  theme_light() +
  theme(legend.position = "none") +
  xlab("Longitude of the sampling position") +
  ylab("Catch score")
```

## MARS (degree 1)

```{r}
#| fig-width: 9
#| fig-height: 4.5
#| fig-align: center
#| message: false
ggplot(data = trawl_train, aes(x = Longitude, y = Score1, col = Year)) +
  geom_point(size = 1) +
  facet_grid(.~Year) + 
  geom_line(data = partial_mars_deg1, aes(x = Longitude, y = yhat)) +
  scale_color_tableau(palette = "Color Blind") +
  theme_light() +
  theme(legend.position = "none") +
  xlab("Longitude of the sampling position") +
  ylab("Catch score")
```

## MARS (degree 2)

```{r}
#| fig-width: 9
#| fig-height: 4.5
#| fig-align: center
#| message: false
ggplot(data = trawl_train, aes(x = Longitude, y = Score1, col = Year)) +
  geom_point(size = 1) +
  facet_grid(.~Year) + 
  geom_line(data = partial_mars_deg2, aes(x = Longitude, y = yhat)) +
  scale_color_tableau(palette = "Color Blind") +
  theme_light() +
  theme(legend.position = "none") +
  xlab("Longitude of the sampling position") +
  ylab("Catch score")
```

:::

<!-- ## The `Hitters` dataset -->

<!-- ```{r} -->
<!-- library(pdp) -->
<!-- gam_partial <- partial(m_gam_selected, pred.var = c("Years", "CHits"), grid.resolution = 40) -->
<!-- autoplot(gam_partial) + theme_light() -->
<!-- ``` -->

## Final results


```{r}
y_test <- trawl_test$Score1

y_null <- mean(trawl_test$Score1)
y_hat_linear <- predict(m_linear, newdata = trawl_test)
y_hat_gam <- predict(m_gam, newdata = trawl_test)
y_hat_mars_deg1 <- predict(m_mars_deg1, newdata = trawl_test)
y_hat_mars_deg2 <- predict(m_mars_deg2, newdata = trawl_test)

tab_results <- rbind(
  c(
    mean(abs(y_test - y_null)),
    mean(abs(y_test - y_hat_linear)),
    mean(abs(y_test - y_hat_gam)),
    mean(abs(y_test - y_hat_mars_deg1)),
    mean(abs(y_test - y_hat_mars_deg2))
  ),
  sqrt(c(
    mean(abs(y_test - y_null)^2),
    mean(abs(y_test - y_hat_linear)^2),
    mean(abs(y_test - y_hat_gam)^2),
    mean(abs(y_test - y_hat_mars_deg1)^2),
    mean(abs(y_test - y_hat_mars_deg2)^2)
  ))
)
colnames(tab_results) <- c("Null model", "Linear model", "GAM", "MARS (degree 1)", "MARS (degree 2)")
rownames(tab_results) <- c("MAE", "RMSE")
knitr::kable(tab_results, digits = 3)
```

## Pros and cons of MARS

::: callout-tip
#### Pros

-   asd
:::

. . .

::: callout-warning
#### Cons

- asd
:::



## References

-
