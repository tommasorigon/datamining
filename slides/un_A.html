<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tommaso Rigon">

<title>A-B-C</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="un_A_files/libs/clipboard/clipboard.min.js"></script>
<script src="un_A_files/libs/quarto-html/quarto.js"></script>
<script src="un_A_files/libs/quarto-html/popper.min.js"></script>
<script src="un_A_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="un_A_files/libs/quarto-html/anchor.min.js"></script>
<link href="un_A_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="un_A_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="un_A_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="un_A_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="un_A_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#homepage" id="toc-homepage" class="nav-link active" data-scroll-target="#homepage">Homepage</a></li>
  <li><a href="#old-friends-linear-models" id="toc-old-friends-linear-models" class="nav-link" data-scroll-target="#old-friends-linear-models">Old friends: linear models</a>
  <ul class="collapse">
  <li><a href="#car-data-diesel-or-gas" id="toc-car-data-diesel-or-gas" class="nav-link" data-scroll-target="#car-data-diesel-or-gas">Car data (<span class="blue">diesel</span> or <span class="orange">gas</span>)</a></li>
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">Linear regression</a></li>
  <li><a href="#regression-models" id="toc-regression-models" class="nav-link" data-scroll-target="#regression-models">Regression models</a></li>
  <li><a href="#linear-models" id="toc-linear-models" class="nav-link" data-scroll-target="#linear-models">Linear models</a></li>
  <li><a href="#matrix-notation" id="toc-matrix-notation" class="nav-link" data-scroll-target="#matrix-notation">Matrix notation</a></li>
  <li><a href="#linear-regression-estimation-i" id="toc-linear-regression-estimation-i" class="nav-link" data-scroll-target="#linear-regression-estimation-i">Linear regression: estimation I</a></li>
  <li><a href="#linear-regression-estimation-ii" id="toc-linear-regression-estimation-ii" class="nav-link" data-scroll-target="#linear-regression-estimation-ii">Linear regression: estimation II</a></li>
  <li><a href="#linear-regression-inference" id="toc-linear-regression-inference" class="nav-link" data-scroll-target="#linear-regression-inference">Linear regression: inference</a></li>
  <li><a href="#car-data-a-first-model" id="toc-car-data-a-first-model" class="nav-link" data-scroll-target="#car-data-a-first-model">Car data, a first model</a></li>
  <li><a href="#a-first-model-estimated-coefficients" id="toc-a-first-model-estimated-coefficients" class="nav-link" data-scroll-target="#a-first-model-estimated-coefficients">A first model: estimated coefficients</a></li>
  <li><a href="#a-first-model-fitted-values" id="toc-a-first-model-fitted-values" class="nav-link" data-scroll-target="#a-first-model-fitted-values">A first model: fitted values</a></li>
  <li><a href="#a-first-model-graphical-diagnostics" id="toc-a-first-model-graphical-diagnostics" class="nav-link" data-scroll-target="#a-first-model-graphical-diagnostics">A first model: graphical diagnostics</a></li>
  <li><a href="#comments-and-criticisms" id="toc-comments-and-criticisms" class="nav-link" data-scroll-target="#comments-and-criticisms">Comments and criticisms</a></li>
  <li><a href="#linear-models-and-non-linear-patterns" id="toc-linear-models-and-non-linear-patterns" class="nav-link" data-scroll-target="#linear-models-and-non-linear-patterns">Linear models and non-linear patterns</a></li>
  <li><a href="#second-model-fitted-values" id="toc-second-model-fitted-values" class="nav-link" data-scroll-target="#second-model-fitted-values">Second model: fitted values</a></li>
  <li><a href="#second-model-graphical-diagnostics" id="toc-second-model-graphical-diagnostics" class="nav-link" data-scroll-target="#second-model-graphical-diagnostics">Second model: graphical diagnostics</a></li>
  <li><a href="#comments-and-criticisms-1" id="toc-comments-and-criticisms-1" class="nav-link" data-scroll-target="#comments-and-criticisms-1">Comments and criticisms</a></li>
  <li><a href="#a-third-model-additional-variables" id="toc-a-third-model-additional-variables" class="nav-link" data-scroll-target="#a-third-model-additional-variables">A third model: additional variables</a></li>
  <li><a href="#a-third-model-graphical-diagnostics" id="toc-a-third-model-graphical-diagnostics" class="nav-link" data-scroll-target="#a-third-model-graphical-diagnostics">A third model: graphical diagnostics</a></li>
  <li><a href="#comments-and-criticisms-2" id="toc-comments-and-criticisms-2" class="nav-link" data-scroll-target="#comments-and-criticisms-2">Comments and criticisms</a></li>
  <li><a href="#a-tour-inside-old-fashioned-statistics" id="toc-a-tour-inside-old-fashioned-statistics" class="nav-link" data-scroll-target="#a-tour-inside-old-fashioned-statistics">A tour inside old-fashioned statistics</a></li>
  </ul></li>
  <li><a href="#normal-equations" id="toc-normal-equations" class="nav-link" data-scroll-target="#normal-equations">Normal equations</a>
  <ul class="collapse">
  <li><a href="#how-to-obtain-the-least-squares-estimate" id="toc-how-to-obtain-the-least-squares-estimate" class="nav-link" data-scroll-target="#how-to-obtain-the-least-squares-estimate">How to obtain the least squares estimate?</a></li>
  <li><a href="#the-normal-equations" id="toc-the-normal-equations" class="nav-link" data-scroll-target="#the-normal-equations">The normal equations</a></li>
  <li><a href="#cholesky-factorization" id="toc-cholesky-factorization" class="nav-link" data-scroll-target="#cholesky-factorization">Cholesky factorization</a></li>
  <li><a href="#cholesky-factorization-and-least-squares" id="toc-cholesky-factorization-and-least-squares" class="nav-link" data-scroll-target="#cholesky-factorization-and-least-squares">Cholesky factorization and least squares</a></li>
  <li><a href="#forward-and-backward-substitutions" id="toc-forward-and-backward-substitutions" class="nav-link" data-scroll-target="#forward-and-backward-substitutions">Forward and backward substitutions</a></li>
  <li><a href="#computational-complexity" id="toc-computational-complexity" class="nav-link" data-scroll-target="#computational-complexity">Computational complexity</a></li>
  <li><a href="#error-propagation-in-normal-equations" id="toc-error-propagation-in-normal-equations" class="nav-link" data-scroll-target="#error-propagation-in-normal-equations">Error propagation in normal equations</a></li>
  <li><a href="#condition-numbers-and-normal-equations" id="toc-condition-numbers-and-normal-equations" class="nav-link" data-scroll-target="#condition-numbers-and-normal-equations">Condition numbers and normal equations</a></li>
  </ul></li>
  <li><a href="#the-qr-decomposition" id="toc-the-qr-decomposition" class="nav-link" data-scroll-target="#the-qr-decomposition">The QR decomposition</a>
  <ul class="collapse">
  <li><a href="#orthogonal-predictors" id="toc-orthogonal-predictors" class="nav-link" data-scroll-target="#orthogonal-predictors">Orthogonal predictors</a></li>
  <li><a href="#regression-by-successive-orthogonalization" id="toc-regression-by-successive-orthogonalization" class="nav-link" data-scroll-target="#regression-by-successive-orthogonalization">Regression by successive orthogonalization</a></li>
  <li><a href="#gram-schmidt-algorithm" id="toc-gram-schmidt-algorithm" class="nav-link" data-scroll-target="#gram-schmidt-algorithm">Gram-Schmidt algorithm</a></li>
  <li><a href="#the-qr-decomposition-i" id="toc-the-qr-decomposition-i" class="nav-link" data-scroll-target="#the-qr-decomposition-i">The QR decomposition I</a></li>
  <li><a href="#the-qr-decomposition-ii" id="toc-the-qr-decomposition-ii" class="nav-link" data-scroll-target="#the-qr-decomposition-ii">The QR decomposition II</a></li>
  <li><a href="#the-qr-decomposition-and-least-squares" id="toc-the-qr-decomposition-and-least-squares" class="nav-link" data-scroll-target="#the-qr-decomposition-and-least-squares">The QR decomposition and least squares</a></li>
  <li><a href="#the-qr-decomposition-and-linear-models" id="toc-the-qr-decomposition-and-linear-models" class="nav-link" data-scroll-target="#the-qr-decomposition-and-linear-models">The QR decomposition and linear models</a></li>
  <li><a href="#computational-complexity-1" id="toc-computational-complexity-1" class="nav-link" data-scroll-target="#computational-complexity-1">Computational complexity</a></li>
  <li><a href="#pivoting-and-rank-deficiencies" id="toc-pivoting-and-rank-deficiencies" class="nav-link" data-scroll-target="#pivoting-and-rank-deficiencies">☠️ - Pivoting and rank deficiencies</a></li>
  </ul></li>
  <li><a href="#iterative-methods" id="toc-iterative-methods" class="nav-link" data-scroll-target="#iterative-methods">Iterative methods</a>
  <ul class="collapse">
  <li><a href="#when-n-is-very-large" id="toc-when-n-is-very-large" class="nav-link" data-scroll-target="#when-n-is-very-large">When <span class="math inline">n</span> is very large…</a></li>
  <li><a href="#recursive-data-import" id="toc-recursive-data-import" class="nav-link" data-scroll-target="#recursive-data-import">Recursive data import</a></li>
  <li><a href="#recursive-estimates" id="toc-recursive-estimates" class="nav-link" data-scroll-target="#recursive-estimates">Recursive estimates</a></li>
  <li><a href="#sherman-morrison-formula" id="toc-sherman-morrison-formula" class="nav-link" data-scroll-target="#sherman-morrison-formula">Sherman-Morrison formula</a></li>
  <li><a href="#the-recursive-least-squares-algorithm-i" id="toc-the-recursive-least-squares-algorithm-i" class="nav-link" data-scroll-target="#the-recursive-least-squares-algorithm-i">The recursive least squares algorithm I</a></li>
  <li><a href="#the-recursive-least-squares-algorithm-ii" id="toc-the-recursive-least-squares-algorithm-ii" class="nav-link" data-scroll-target="#the-recursive-least-squares-algorithm-ii">The recursive least squares algorithm II</a></li>
  </ul></li>
  <li><a href="#generalized-linear-models" id="toc-generalized-linear-models" class="nav-link" data-scroll-target="#generalized-linear-models">Generalized linear models</a>
  <ul class="collapse">
  <li><a href="#the-heart-dataset" id="toc-the-heart-dataset" class="nav-link" data-scroll-target="#the-heart-dataset">The <code>heart</code> dataset</a></li>
  <li><a href="#the-heart-dataset-1" id="toc-the-heart-dataset-1" class="nav-link" data-scroll-target="#the-heart-dataset-1">The <code>heart</code> dataset</a></li>
  <li><a href="#generalized-linear-models-1" id="toc-generalized-linear-models-1" class="nav-link" data-scroll-target="#generalized-linear-models-1">Generalized linear models</a></li>
  <li><a href="#likelihood-based-inference" id="toc-likelihood-based-inference" class="nav-link" data-scroll-target="#likelihood-based-inference">Likelihood-based inference</a></li>
  <li><a href="#linear-models-with-gaussian-errors" id="toc-linear-models-with-gaussian-errors" class="nav-link" data-scroll-target="#linear-models-with-gaussian-errors">Linear models with Gaussian errors</a></li>
  <li><a href="#binary-classification-via-logistic-regression" id="toc-binary-classification-via-logistic-regression" class="nav-link" data-scroll-target="#binary-classification-via-logistic-regression">Binary classification via logistic regression</a></li>
  <li><a href="#iteratively-re-weighted-least-squares-i" id="toc-iteratively-re-weighted-least-squares-i" class="nav-link" data-scroll-target="#iteratively-re-weighted-least-squares-i">Iteratively re-weighted least squares I</a></li>
  <li><a href="#iteratively-re-weighted-least-squares-ii" id="toc-iteratively-re-weighted-least-squares-ii" class="nav-link" data-scroll-target="#iteratively-re-weighted-least-squares-ii">Iteratively re-weighted least squares II</a></li>
  <li><a href="#computational-considerations" id="toc-computational-considerations" class="nav-link" data-scroll-target="#computational-considerations">Computational considerations</a></li>
  <li><a href="#the-estimated-model" id="toc-the-estimated-model" class="nav-link" data-scroll-target="#the-estimated-model">The estimated model</a></li>
  <li><a href="#prediction-and-model-assessment-i" id="toc-prediction-and-model-assessment-i" class="nav-link" data-scroll-target="#prediction-and-model-assessment-i">Prediction and model assessment I</a></li>
  <li><a href="#prediction-and-model-assessment-ii" id="toc-prediction-and-model-assessment-ii" class="nav-link" data-scroll-target="#prediction-and-model-assessment-ii">Prediction and model assessment II</a></li>
  <li><a href="#the-roc-curve" id="toc-the-roc-curve" class="nav-link" data-scroll-target="#the-roc-curve">The ROC curve</a></li>
  <li><a href="#other-topics" id="toc-other-topics" class="nav-link" data-scroll-target="#other-topics">Other topics</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="un_A_slides.html"><i class="bi bi-file-slides"></i>RevealJS</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">A-B-C</h1>
<p class="subtitle lead">Data Mining - CdL CLAMSES</p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><span class="orange">Tommaso Rigon</span> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <em>Università degli Studi di Milano-Bicocca</em>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="homepage" class="level2">
<h2 class="anchored" data-anchor-id="homepage"><a href="../index.html">Homepage</a></h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><img src="img/ABC.png" class="img-fluid"> <em>“Everything should be made as simple as possible, but not simpler”</em></p>
<p>Attributed to <span class="grey">Albert Einstein</span></p>
</div><div class="column" style="width:60%;">
<ul>
<li><p>This unit will cover the following <span class="orange">topics</span>:</p>
<ul>
<li>Linear models and the modeling process</li>
<li>Cholesky factorization</li>
<li>Orthogonalization and QR decomposition</li>
<li>Iterative methods</li>
<li>Generalized linear models</li>
</ul></li>
<li><p>The <span class="blue">computational aspects</span> of linear models will be novel to most of you…</p></li>
<li><p>… but you should be already <span class="orange">very familiar</span> with linear models!</p></li>
</ul>
</div>
</div>
</section>
<section id="old-friends-linear-models" class="level1">
<h1>Old friends: linear models</h1>
<section id="car-data-diesel-or-gas" class="level2">
<h2 class="anchored" data-anchor-id="car-data-diesel-or-gas">Car data (<span class="blue">diesel</span> or <span class="orange">gas</span>)</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="750"></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>We consider data for <span class="math inline">n = 203</span> models of cars in circulation in 1985 in the USA.</li>
<li>We want to <span class="blue">predict</span> the distance per unit of fuel as a function of the vehicle features.</li>
<li>We consider the following <span class="orange">variables</span>:
<ul>
<li>The city distance per unit of fuel (km/L, <code>city.distance</code>)</li>
<li>The engine size (L, <code>engine.size</code>)</li>
<li>The number of cylinders (<code>n.cylinders</code>)</li>
<li>The curb weight (kg, <code>curb.weight</code>)</li>
<li>The fuel type (gasoline or diesel, <code>fuel</code>).</li>
</ul></li>
</ul>
</div>
</div>
</section>
<section id="linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression">Linear regression</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="600"></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li><p>Let us consider the variables <code>city.distance</code> (<span class="math inline">y</span>), <code>engine.size</code> (<span class="math inline">x</span>) and <code>fuel</code> (<span class="math inline">z</span>).</p></li>
<li><p>A <span class="blue">simple linear regression</span> <span class="math display">
Y_i = \beta_1 + \beta_2 x_i + \epsilon_i, \qquad i=1,\dots,n,
</span> could be easily fit by least squares…</p></li>
<li><p>… but the plot suggests that the relationship between <code>city.distance</code> and <code>engine.size</code> is <span class="orange">not</span> well approximated by a <span class="orange">linear</span> function.</p></li>
<li><p>… and also that <code>fuel</code> has a non-negligible effect on the response.</p></li>
</ul>
</div>
</div>
</section>
<section id="regression-models" class="level2">
<h2 class="anchored" data-anchor-id="regression-models">Regression models</h2>
<div class="incremental">
<ul class="incremental">
<li><p>A <span class="orange">general</span> and <span class="orange">more flexible formulation</span> for modeling the relationship between a vector of <span class="blue">fixed covariates</span> <span class="math inline">\bm{x}_i = (x_{i1},\dots,x_{ip})^T \in \mathbb{R}^p</span> and a random variable <span class="math inline">Y_i \in \mathbb{R}</span> is <span class="math display">
Y_i = f(\bm{x}_i; \beta) + \epsilon_i, \qquad i=1,\dots,n,
</span> where the “errors” <span class="math inline">\epsilon_i</span> are iid random variables, having zero mean and variance <span class="math inline">\sigma^2</span>.</p></li>
<li><p>To estimate the unknown parameters <span class="math inline">\beta</span>, a possibility is to rely on the <span class="blue">least squares criterion</span>: we seek the <span class="orange">minimum</span> of the objective function <span class="math display">
D(\beta) = \sum_{i=1}^n\{y_i - f(\bm{x}_i; \beta)\}^2,
</span> using <span class="math inline">n</span> pairs of covariates <span class="math inline">\bm{x}_i = (x_{i1},\dots,x_{ip})^T</span> and the observed realizations <span class="math inline">y_i</span> of the random variables <span class="math inline">Y_i</span>, for <span class="math inline">i = 1,\dots,n</span>. The <span class="orange">optimal value</span> is denoted by <span class="math inline">\hat{\beta}</span>.</p></li>
<li><p>The <span class="blue">predicted values</span> are <span class="math inline">\hat{y}_i = \mathbb{E}(Y_i) = f(\bm{x}_i; \hat{\beta})</span>, for <span class="math inline">i=1,\dots,n.</span></p></li>
</ul>
</div>
</section>
<section id="linear-models" class="level2">
<h2 class="anchored" data-anchor-id="linear-models">Linear models</h2>
<ul>
<li><p>Let us consider again the variables <code>city.distance</code> (<span class="math inline">y</span>), <code>engine.size</code> (<span class="math inline">x</span>) and <code>fuel</code> (<span class="math inline">z</span>).</p></li>
<li><p>Which function <span class="math inline">f(x,z;\beta)</span> should we choose?</p></li>
</ul>
<ul>
<li>A first attempt is to consider a <span class="orange">polynomial term</span> combined with a <span class="blue">dummy variable</span> <span class="math display">
f(x, z; \beta) = \beta_1 + \beta_2 x + \beta_3 x^2 + \beta_4 x^3 + \beta_5 I(z = \texttt{gas}),
</span> which is a special instance of <span class="orange">linear model</span>.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition (Linear model)
</div>
</div>
<div class="callout-body-container callout-body">
<p>In a <span class="blue">linear model</span> the response variable <span class="math inline">Y_i</span> is related to the covariates through the function<span class="math display">
    \mathbb{E}(Y_i) =f(\bm{x}_i; \beta) = \beta_1 x_{i1} + \cdots + \beta_p x_{ip} =\bm{x}_i^T\beta,
    </span> where <span class="math inline">\bm{x}_i = (x_{i1},\dots,x_{ip})^T</span> is a vector of <span class="orange">covariates</span> and <span class="math inline">\beta = (\beta_1,\dots,\beta_p)^T</span> is the corresponding vector of <span class="orange">coefficients</span>.</p>
</div>
</div>
</section>
<section id="matrix-notation" class="level2">
<h2 class="anchored" data-anchor-id="matrix-notation">Matrix notation</h2>
<ul>
<li><p>The <span class="orange">response random variables</span> are collected in the random vector <span class="math inline">\bm{Y} = (Y_1,\dots,Y_n)^T</span>, whose <span class="blue">observed realization</span> is <span class="math inline">\bm{y} = (y_1,\dots,y_n)^T</span>.</p></li>
<li><p>The <span class="blue">design matrix</span> is a <span class="math inline">n \times p</span> matrix, comprising the covariate’s values, defined by <span class="math display">
\bm{X} =
\begin{bmatrix}
x_{11} &amp; \cdots &amp; x_{1p}\\
\vdots &amp; \ddots &amp; \vdots \\
x_{n1} &amp; \cdots &amp; x_{np}
\end{bmatrix}.
</span></p></li>
</ul>
<ul>
<li>The <span class="math inline">j</span>th variable (column) is denoted with <span class="math inline">\tilde{\bm{x}}_j</span>, whereas the <span class="math inline">i</span>th observation (row) is <span class="math inline">\bm{x}_i</span>: <span class="math display">
\bm{X} = (\tilde{\bm{x}}_1,\dots,\tilde{\bm{x}}_p) = (\bm{x}_1, \dots,\bm{x}_n)^T.
</span></li>
</ul>
<ul>
<li>Then, a <span class="blue">linear model</span> can be written using the <span class="orange">compact notation</span>: <span class="math display">
\bm{Y} = \bm{X}\beta + \bm{\epsilon},
</span> where <span class="math inline">\bm{\epsilon} = (\epsilon_1,\dots,\epsilon_n)^T</span> is a vector of iid error terms with zero mean and variance <span class="math inline">\sigma^2</span>.</li>
</ul>
</section>
<section id="linear-regression-estimation-i" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-estimation-i">Linear regression: estimation I</h2>
<ul>
<li>The optimal set of coefficients <span class="math inline">\hat{\beta}</span> is the minimizer of the <span class="orange">least squared criterion</span> <span class="math display">
D(\beta) = (\bm{y} - \bm{X}\beta)^T(\bm{y} - \bm{X}\beta) = ||\bm{y} - \bm{X}\beta||^2,
</span> also known as <span class="orange">residual sum of squares (RSS)</span>, where <span class="math display">
||\bm{y}|| = \sqrt{y_1^2 + \cdots + y_n^2},</span> denotes the <span class="blue">Euclidean norm</span>.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Least square estimate (OLS)
</div>
</div>
<div class="callout-body-container callout-body">
<p>If the design matrix has <span class="blue">full rank</span>, that is, if <span class="math inline">\text{rk}(\bm{X}^T\bm{X}) = p</span>, then the <span class="orange">least square estimate</span> has an explicit solution: <span class="math display">
    \hat{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T \bm{y}.
    </span></p>
</div>
</div>
</section>
<section id="linear-regression-estimation-ii" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-estimation-ii">Linear regression: estimation II</h2>
<ul>
<li><p>In matrix notation, the predicted values can be obtained as <span class="math display">
\hat{\bm{y}} = \bm{X}\hat{\beta} = \bm{H}\bm{y}, \qquad \bm{H} = \bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T.
</span></p></li>
<li><p><span class="math inline">\bm{H}</span> is a <span class="math inline">n \times n</span> <span class="orange">projection matrix</span> matrix sometimes called <span class="blue">hat matrix</span>.</p></li>
<li><p>It can be shown that <span class="math inline">\text{tr}(\bm{H}) = \text{rk}(\bm{H}) = p</span>. Moreover, it holds <span class="math inline">\bm{H} = \bm{H}^T</span> and <span class="math inline">\bm{H}^2 = \bm{H}</span>.</p></li>
</ul>
<ul>
<li><p>The quantity <span class="math inline">D(\hat{\beta})</span> is the so-called <span class="blue">deviance</span>, which is equal to <span class="math display">
D(\hat{\beta}) = ||\bm{y} - \hat{\bm{y}}||^2 = \bm{y}^T(I_n - \bm{H})\bm{y}.
</span></p></li>
<li><p>Moreover, a typical estimate for the <span class="orange">residual variance</span> <span class="math inline">\sigma^2</span> is obtained as follows: <span class="math display">
s^2 = \frac{D(\hat{\beta})}{n - p} = \frac{1}{n-p}\sum_{i=1}^n(y_i - \bm{x}_i^T\hat{\beta})^2.
</span></p></li>
</ul>
</section>
<section id="linear-regression-inference" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-inference">Linear regression: inference</h2>
<ul>
<li><p>Let us additionally assume that the errors follow a Gaussian distribution: <span class="math inline">\epsilon_i \overset{\text{iid}}{\sim} N(0, \sigma^2)</span>.</p></li>
<li><p>This implies that the <span class="orange">distribution</span> of the <span class="orange">estimator</span> <span class="math inline">\hat{\beta}</span> is <span class="math display">
\hat{\beta} \sim N_p(\beta, \sigma^2 (\bm{X}^T\bm{X})^{-1}).
</span></p></li>
<li><p>Hence, the estimator <span class="math inline">\hat{\beta}</span> is <span class="orange">unbiased</span> and its <span class="blue">variance</span> can be estimated by <span class="math display">
\widehat{\text{var}}(\hat{\beta}) = s^2 (\bm{X}^T\bm{X})^{-1}.
</span></p></li>
<li><p>The <span class="orange">standard errors</span> of the components of <span class="math inline">\hat{\beta}</span> correspond to the square root of the diagonal of the above covariance matrix.</p></li>
</ul>
<ul>
<li>Confidence interval and Wald’s tests can be obtained through classical inferential theory.</li>
</ul>
<ul>
<li>Ok, we are ready to get back to the original problem…</li>
</ul>
</section>
<section id="car-data-a-first-model" class="level2">
<h2 class="anchored" data-anchor-id="car-data-a-first-model">Car data, a first model</h2>
<ul>
<li><p>Our first attempt for predicting <code>city.distance</code> (<span class="math inline">y</span>) via <code>engine.size</code> (<span class="math inline">x</span>) and <code>fuel</code> (<span class="math inline">z</span>) is: <span class="math display">
  f(x, z; \beta) = \beta_1 + \beta_2 x + \beta_3 x^2 + \beta_4 x^3 + \beta_5 I(z = \texttt{gas}).
  </span></p></li>
<li><p>Indeed, by looking at the plot of the data, it is plausible that we need a <span class="orange">polynomial</span> of degree <span class="math inline">3</span> or <span class="math inline">4</span></p></li>
<li><p>It is also clear from the plot that <code>fuel</code> is a relevant variable. Categorical variables are <span class="orange">encoded</span> using <span class="blue">indicator variables</span>.</p></li>
</ul>
<ul>
<li>To evaluate the goodness of fit, we can calculate the <span class="orange">coefficient of determination</span>: <span class="math display">
R^2 = 1 - \frac{\text{(``Residual deviance'')}}{\text{(``Total deviance'')}} = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n(y_i - \bar{y})^2}.
</span></li>
</ul>
</section>
<section id="a-first-model-estimated-coefficients" class="level2">
<h2 class="anchored" data-anchor-id="a-first-model-estimated-coefficients">A first model: estimated coefficients</h2>
<ul>
<li>We obtain the following <span class="orange">summary</span> for the regression coefficients <span class="math inline">\hat{\beta}</span>.</li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>(Intercept)</code></td>
<td style="text-align: right;">28.045</td>
<td style="text-align: right;">3.076</td>
<td style="text-align: right;">9.119</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>engine.size</code></td>
<td style="text-align: right;">-10.980</td>
<td style="text-align: right;">3.531</td>
<td style="text-align: right;">-3.109</td>
<td style="text-align: right;">0.002</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>engine.size^2</code></td>
<td style="text-align: right;">2.098</td>
<td style="text-align: right;">1.271</td>
<td style="text-align: right;">1.651</td>
<td style="text-align: right;">0.100</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>engine.size^3</code></td>
<td style="text-align: right;">-0.131</td>
<td style="text-align: right;">0.139</td>
<td style="text-align: right;">-0.939</td>
<td style="text-align: right;">0.349</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>fuel_gas</code></td>
<td style="text-align: right;">-3.214</td>
<td style="text-align: right;">0.427</td>
<td style="text-align: right;">-7.523</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
<ul>
<li>Moreover, the coefficient <span class="math inline">R^2</span> and the residual standard deviation <span class="math inline">s</span> are:</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">r.squared</th>
<th style="text-align: right;">sigma</th>
<th style="text-align: right;">deviance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.5973454</td>
<td style="text-align: right;">1.790362</td>
<td style="text-align: right;">634.6687</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="a-first-model-fitted-values" class="level2">
<h2 class="anchored" data-anchor-id="a-first-model-fitted-values">A first model: fitted values</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1170"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="a-first-model-graphical-diagnostics" class="level2">
<h2 class="anchored" data-anchor-id="a-first-model-graphical-diagnostics">A first model: graphical diagnostics</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1170"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="comments-and-criticisms" class="level2">
<h2 class="anchored" data-anchor-id="comments-and-criticisms">Comments and criticisms</h2>
<ul>
<li><p>Is this a good model?</p></li>
<li><p>The overall fit <span class="blue">seems satisfactory</span> at first glance, especially if we aim at predicting the urban distance of cars when average engine size (i.e., between <span class="math inline">1.5L</span> and <span class="math inline">3L</span>).</p></li>
</ul>
<ul>
<li>However, the plot of the <span class="orange">residuals</span> <span class="math inline">r_i = y_i - \hat{y}_i</span> suggests that the homoscedasticity assumption, i.e.&nbsp;<span class="math inline">\text{var}(\epsilon_i) = \sigma^2</span>, might be violated.</li>
</ul>
<ul>
<li><p>Also, this model is unsuitable for <span class="orange">extrapolation</span>. Indeed:</p>
<ul>
<li>It has no grounding in physics or engineering, leading to difficulties when interpreting the trend and to paradoxical situations.</li>
<li>For example, the curve of the set of gasoline cars shows a local minimum around <span class="math inline">4.6 L</span> and then rises again!</li>
</ul></li>
<li><p>It is plausible that we can find a better one, so what’s next?</p></li>
</ul>
</section>
<section id="linear-models-and-non-linear-patterns" class="level2">
<h2 class="anchored" data-anchor-id="linear-models-and-non-linear-patterns">Linear models and non-linear patterns</h2>
<ul>
<li>A significant advantage of linear models is that they can describe non-linear relationships via <span class="blue">variable transformations</span> such as polynomials, logarithms, etc.</li>
</ul>
<ul>
<li>This gives the statistician a lot of modeling flexibility. For instance, we could let: <span class="math display">
\log{Y_i} = \beta_1 + \beta_2 \log{x_i} + \beta_3 I(z_i = \texttt{gas}) + \epsilon_i, \qquad i=1,\dots,n.
</span></li>
</ul>
<ul>
<li>This specification is <span class="orange">linear in the parameters</span>, it fixes the domain issues, and it imposes a monotone relationship between engine size and consumption.</li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>(Intercept)</code></td>
<td style="text-align: right;">3.060</td>
<td style="text-align: right;">0.047</td>
<td style="text-align: right;">64.865</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>log(engine.size)</code></td>
<td style="text-align: right;">-0.682</td>
<td style="text-align: right;">0.040</td>
<td style="text-align: right;">-17.129</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>fuel_gas</code></td>
<td style="text-align: right;">-0.278</td>
<td style="text-align: right;">0.038</td>
<td style="text-align: right;">-7.344</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</section>
<section id="second-model-fitted-values" class="level2">
<h2 class="anchored" data-anchor-id="second-model-fitted-values">Second model: fitted values</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1170"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="second-model-graphical-diagnostics" class="level2">
<h2 class="anchored" data-anchor-id="second-model-graphical-diagnostics">Second model: graphical diagnostics</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1170"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="comments-and-criticisms-1" class="level2">
<h2 class="anchored" data-anchor-id="comments-and-criticisms-1">Comments and criticisms</h2>
<ul>
<li>The <span class="blue">goodness of fit</span> indices are the following:</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">r.squared.original</th>
<th style="text-align: right;">r.squared</th>
<th style="text-align: right;">sigma</th>
<th style="text-align: right;">deviance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.5847555</td>
<td style="text-align: right;">0.6196093</td>
<td style="text-align: right;">0.1600278</td>
<td style="text-align: right;">5.121777</td>
</tr>
</tbody>
</table>
</div>
</div>
<ul>
<li>Do not mix <span class="blue">apple</span> and <span class="orange">oranges</span>! Compare <span class="math inline">R^2</span>s only if they refer to the same scale!</li>
</ul>
<ul>
<li><p>This second model is <span class="blue">more parsimonious</span>, and yet it reaches satisfactory predictive performance.</p></li>
<li><p>It is also more coherent with the nature of the data: the predictions cannot be negative, and the relationship between engine size and the consumption is monotone.</p></li>
<li><p>Yet, there is still some heteroscedasticity in the residuals — is this is due to a missing covariate that has not been included in the model?</p></li>
</ul>
</section>
<section id="a-third-model-additional-variables" class="level2">
<h2 class="anchored" data-anchor-id="a-third-model-additional-variables">A third model: additional variables</h2>
<ul>
<li><p>Let us consider <span class="blue">two additional variables</span>: <code>curb.weight</code> (<span class="math inline">w</span>) and <code>n.cylinders</code> (<span class="math inline">v</span>).</p></li>
<li><p>A richer model, therefore, could be: <span class="math display">
\log{Y_i} = \beta_1 + \beta_2 \log{x_i} +  \beta_3 \log{w_i} + \beta_4 I(z_i = \texttt{gas}) + \beta_5 I(v_i = 2) + \epsilon_i,
  </span> for <span class="math inline">i=1,\dots,n</span>. The estimates are:</p></li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>(Intercept)</code></td>
<td style="text-align: right;">9.423</td>
<td style="text-align: right;">0.482</td>
<td style="text-align: right;">19.549</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>log(engine.size)</code></td>
<td style="text-align: right;">-0.180</td>
<td style="text-align: right;">0.051</td>
<td style="text-align: right;">-3.504</td>
<td style="text-align: right;">0.001</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>log(curb.weight)</code></td>
<td style="text-align: right;">-0.943</td>
<td style="text-align: right;">0.072</td>
<td style="text-align: right;">-13.066</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>fuel_gas</code></td>
<td style="text-align: right;">-0.353</td>
<td style="text-align: right;">0.022</td>
<td style="text-align: right;">-15.934</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>cylinders2_TRUE</code></td>
<td style="text-align: right;">-0.481</td>
<td style="text-align: right;">0.052</td>
<td style="text-align: right;">-9.301</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
</section>
<section id="a-third-model-graphical-diagnostics" class="level2">
<h2 class="anchored" data-anchor-id="a-third-model-graphical-diagnostics">A third model: graphical diagnostics</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1170"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="comments-and-criticisms-2" class="level2">
<h2 class="anchored" data-anchor-id="comments-and-criticisms-2">Comments and criticisms</h2>
<ul>
<li>The goodness of fit greatly <span class="blue">improved</span>:</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">r.squared.original</th>
<th style="text-align: right;">r.squared</th>
<th style="text-align: right;">sigma</th>
<th style="text-align: right;">deviance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0.869048</td>
<td style="text-align: right;">0.8819199</td>
<td style="text-align: right;">0.0896089</td>
<td style="text-align: right;">1.589891</td>
</tr>
</tbody>
</table>
</div>
</div>
<ul>
<li><p>In this third model, we handled the <span class="orange">outliers</span> appearing in the residual plots, which it turns out are identified by the group of cars having 2 cylinders.</p></li>
<li><p>The diagnostic plots are also very much improved, although still not perfect.</p></li>
<li><p>The estimates are coherent with our expectations, based on common knowledge. Have a look at the textbook (A&amp;S) for a detailed explanation of <span class="math inline">\beta_4</span>!</p></li>
<li><p>The car dataset is available from the textbook (A&amp;S) website:</p>
<ul>
<li>Dataset <a href="http://azzalini.stat.unipd.it/Book-DM/auto.dat" class="uri">http://azzalini.stat.unipd.it/Book-DM/auto.dat</a></li>
<li>Variable description <a href="http://azzalini.stat.unipd.it/Book-DM/auto.names" class="uri">http://azzalini.stat.unipd.it/Book-DM/auto.names</a></li>
</ul></li>
</ul>
</section>
<section id="a-tour-inside-old-fashioned-statistics" class="level2">
<h2 class="anchored" data-anchor-id="a-tour-inside-old-fashioned-statistics">A tour inside old-fashioned statistics</h2>
<ul>
<li>The first part of the unit is a tour in the “old style” data modeling, the kind of culture that Leo Breiman so <span class="orange">heavily criticized</span> in his 2001 <em>Statistical Science</em> paper.</li>
</ul>
<ul>
<li><p>However, this dataset was sufficiently small, meaning it could be “manually” analyzed and modeled. We gained much <span class="blue">understanding</span> by doing so.</p></li>
<li><p>Hence, these old tools should not be considered useless or irrelevant.</p></li>
</ul>
<ul>
<li><p>The second half of the unit will have an entirely different flavor, though.</p></li>
<li><p>Given the vast amount of data we now have, it makes sense to focus on <span class="orange">computations</span> for fitting linear models.</p></li>
<li><p>As we will see, the mathematical simplicity of linear models leads to extremely <span class="blue">fast computations</span>, an important advantage in the era of big data.</p></li>
</ul>
</section>
</section>
<section id="normal-equations" class="level1">
<h1>Normal equations</h1>
<section id="how-to-obtain-the-least-squares-estimate" class="level2">
<h2 class="anchored" data-anchor-id="how-to-obtain-the-least-squares-estimate">How to obtain the least squares estimate?</h2>
<ul>
<li><p>In B.Sc. courses, it is often suggested that the least square estimate should be computed using the formula <span class="math display">
\hat{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T \bm{y},
</span> that is, using the R code <code>solve(t(X) %*% X) %*% t(X) %*% y</code>.</p></li>
<li><p>This approach works reasonably well in many <span class="blue">simple cases</span>.</p></li>
</ul>
<ul>
<li>Unfortunately, in more challenging scenarios, e.g., when we have a lot of data (large <span class="math inline">n</span>) and correlated variables, the above code is
<ul>
<li><span class="orange">computationally inefficient</span></li>
<li><span class="blue">numerically inaccurate</span></li>
</ul></li>
<li>The main computational bottleneck is the calculation of the inverse of <span class="math inline">\bm{X}^T\bm{X}</span>, which is very costly and often numerically unstable, especially when the predictors are almost collinear.</li>
</ul>
</section>
<section id="the-normal-equations" class="level2">
<h2 class="anchored" data-anchor-id="the-normal-equations">The normal equations</h2>
<ul>
<li><p>The least square estimate is the solution of the system of equations (<span class="orange">normal equations</span>):<span class="math display">
\bm{X}^T\bm{X} \beta = \bm{X}^T \bm{y}.
</span></p></li>
<li><p>This system could be solved using <code>solve(crossprod(X), crossprod(X, y))</code>.</p></li>
<li><p>This avoids the explicit computation of <span class="math inline">(\bm{X}^T\bm{X})^{-1}</span> and it is preferable compared to the “direct solution.” However, it does not exploit the properties of the matrix <span class="math inline">\bm{X}^T\bm{X}</span>.</p></li>
</ul>
<ul>
<li>Recall (from your favorite linear algebra textbook) that a <span class="blue">symmetric</span> matrix <span class="math inline">\bm{A} \in \mathbb{R}^{p \times p}</span> is <span class="orange">positive definite</span> if and only if one of the following properties is satisfied
<ul>
<li>The quadratic form <span class="math inline">\bm{x}^T \bm{A} \bm{x} &gt; 0</span> for all <span class="math inline">\bm{x} \in \mathbb{R}^p</span> such that <span class="math inline">\bm{x} \neq 0</span>.</li>
<li>The eigenvalues <span class="math inline">\lambda_1,\dots,\lambda_p</span> of <span class="math inline">\bm{A}</span> are all strictly positive.</li>
</ul></li>
<li>We now describe a strategy to compute <span class="math inline">\hat{\beta}</span> that exploits the fact that <span class="math inline">\bm{X}^T\bm{X}</span> is <span class="orange">positive definite</span>, resulting in more <span class="blue">efficient computations</span>.</li>
</ul>
</section>
<section id="cholesky-factorization" class="level2">
<h2 class="anchored" data-anchor-id="cholesky-factorization">Cholesky factorization</h2>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition A.1
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose <span class="math inline">\bm{X} \in \mathbb{R}^{n \times p}</span> with <span class="math inline">n \ge p</span> has full rank, that is <span class="math inline">\text{rk}(\bm{X}) = p</span>. Then, the matrix <span class="math display">
\bm{X}^T\bm{X}
</span> is <span class="blue">symmetric</span> and <span class="orange">positive definite</span>.</p>
</div>
</div>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (Cholesky factorization)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\bm{A} \in \mathbb{R}^{p \times p}</span> be a symmetric and positive definite matrix. Then, there exists a unique <span class="orange">upper triangular</span> <span class="math inline">p \times p</span> matrix <span class="math inline">\bm{R}</span> with positive entries such that <span class="math display">
\bm{A} = \bm{R}^T\bm{R}.
</span></p>
</div>
</div>
</section>
<section id="cholesky-factorization-and-least-squares" class="level2">
<h2 class="anchored" data-anchor-id="cholesky-factorization-and-least-squares">Cholesky factorization and least squares</h2>
<div class="incremental">
<ul class="incremental">
<li><p>The Cholesky factorization is incredibly helpful for computing least squares. Indeed:</p></li>
<li><p>Let <span class="math inline">\bm{R}^T\bm{R}</span> be the Cholesky factorization of the matrix <span class="math inline">\bm{X}^T\bm{X}</span>. Then, the <span class="orange">normal equations</span> can be written as <span class="math display">
\bm{R}^T\bm{R} \beta = \bm{X}^T \bm{y}.
</span> This system can now solved in <span class="orange">two steps</span>:</p></li>
<li><p><span class="blue">Step 1 (Forwardsolve)</span>. Solve with respect to <span class="math inline">\bm{z}</span> the system of equations <span class="math display">
\bm{R}^T \bm{z} = \bm{X}^T \bm{y}.
</span></p></li>
<li><p><span class="blue">Step 2 (Backsolve)</span>. Given <span class="math inline">\bm{z}</span>, now solve with respect to <span class="math inline">\beta</span> the system of equations <span class="math display">
\bm{R} \beta = \bm{z}.
</span></p></li>
<li><p>Why is this procedure computationally more efficient than the naïve solution?</p></li>
</ul>
</div>
</section>
<section id="forward-and-backward-substitutions" class="level2">
<h2 class="anchored" data-anchor-id="forward-and-backward-substitutions">Forward and backward substitutions</h2>
<div class="incremental">
<ul class="incremental">
<li><p>The key observation is that the solution of <span class="blue">triangular systems</span> is computationally <span class="orange">straightforward</span>.</p></li>
<li><p>As an example, consider the following <span class="math inline">3 \times 3</span> lower triangular system: <span class="math display">
\begin{bmatrix}
l_{11} &amp; 0 &amp; 0 \\
l_{21} &amp; l_{22} &amp; 0 \\
l_{31} &amp; l_{32} &amp; l_{33} \\
\end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix}
b_1 \\ b_2 \\ b_3
\end{bmatrix}.
</span></p></li>
<li><p>The solution for <span class="math inline">x_1,x_2,x_3</span> can be found sequentially: <span class="math display">
x_1 = \frac{b_1}{l_{11}}, \qquad x_2 = \frac{b_2 -  l_{21}x_1}{l_{22}}, \qquad x_3 = \frac{b_3 - l_{31}x_1 - l_{32}x_2}{l_{33}}.
</span></p></li>
<li><p>Finding the <span class="orange">inverse</span> <span class="math inline">\bm{R}^{-1}</span> is simple, again because <span class="math inline">\bm{R}</span> is upper triangular. Also, note that <span class="math display">
(\bm{X}^T \bm{X})^{-1} = (\bm{R}^T \bm{R})^{-1} = \bm{R}^{-1} (\bm{R}^{-1})^T.
</span></p></li>
</ul>
</div>
</section>
<section id="computational-complexity" class="level2">
<h2 class="anchored" data-anchor-id="computational-complexity">Computational complexity</h2>
<div class="incremental">
<ul class="incremental">
<li><p>The solution via Cholesky factorization is a <span class="blue">fast direct approach</span> for finding <span class="math inline">\hat{\beta}</span>.</p></li>
<li><p>The expensive steps are:</p>
<ul class="incremental">
<li>The formation of the matrix <span class="math inline">\bm{X}^T\bm{X}</span> requires <span class="math inline">\sim n p^2</span> elementary operations</li>
<li>The Cholesky factorization of <span class="math inline">\bm{X}^T\bm{X}</span> requires <span class="math inline">\sim p^3 / 3</span> elementary operations.</li>
</ul></li>
<li><p>This gives an overall computational complexity of order <span class="math display">
\sim n p^2 + p^3 /3,
</span> which corrects the typographical error of the A&amp;S textbook.</p></li>
<li><p>This means, unfortunately, that in <span class="blue">high-dimensional</span> settings (large <span class="math inline">p</span>) computations become <span class="orange">very costly</span>, since the complexity is cubic in <span class="math inline">p</span>.</p></li>
</ul>
</div>
</section>
<section id="error-propagation-in-normal-equations" class="level2">
<h2 class="anchored" data-anchor-id="error-propagation-in-normal-equations">Error propagation in normal equations</h2>
<div class="incremental">
<ul class="incremental">
<li><p>The normal equations method is typically <span class="blue">quicker</span> than other algorithms, as it removes the dependency on <span class="math inline">n</span>, but it is in general numerically more <span class="orange">unstable</span>.</p></li>
<li><p>Consider, for example, the following matrix: <span class="math display">
\bm{X} = \begin{bmatrix}1 &amp; 1 \\
\epsilon &amp; 0 \\
0 &amp; \epsilon \end{bmatrix},
</span> for a small value <span class="math inline">\epsilon &gt; 0</span>. Then, we obtain that <span class="math display">\bm{X}^T \bm{X} = \begin{bmatrix}1 + \epsilon^2&amp; 1 \\
1 &amp; 1 + \epsilon^2 \end{bmatrix}.
</span></p></li>
<li><p>The numerical computation of <span class="math inline">\epsilon^2</span> in <span class="math inline">\bm{X}^T\bm{X}</span> requires a higher precision compared to <span class="math inline">\epsilon</span>, leading to numerical instabilities and/or a <span class="orange">loss in accuracy</span>.</p></li>
</ul>
</div>
</section>
<section id="condition-numbers-and-normal-equations" class="level2">
<h2 class="anchored" data-anchor-id="condition-numbers-and-normal-equations">Condition numbers and normal equations</h2>
<div class="incremental">
<ul class="incremental">
<li><p>Suppose <span class="math inline">\bm{X} \in \mathbb{R}^{n \times p}</span> with <span class="math inline">n \ge p</span> has full rank and singular values <span class="math inline">d_1 \ge d_2 \ge \dots \ge d_p</span>. Then its <span class="orange">condition number</span> is <span class="math display">
\kappa(\bm{X}) = ||\bm{X}|| \cdot ||\bm{X}^+|| = \frac{d_1}{d_p},
</span> where <span class="math inline">\bm{X}^+</span> is the Moore-Penrose pseudo-inverse. Note that <span class="math inline">\kappa(\bm{X}) \ge 1</span>.</p></li>
<li><p>If <span class="math inline">\kappa(\bm{X})</span> is small, the matrix <span class="math inline">\bm{X}</span> is <span class="blue">well conditioned</span>. Otherwise, we say it is <span class="orange">ill conditioned</span>.</p></li>
<li><p>The condition number determines how accurately we can solve linear systems.</p></li>
<li><p>An important fact is: <span class="math display">
\kappa(\bm{X}^T\bm{X}) = \kappa(\bm{X})^2,
</span> implying that there is an evident loss of numerical accuracy when using normal equations.</p></li>
</ul>
</div>
</section>
</section>
<section id="the-qr-decomposition" class="level1">
<h1>The QR decomposition</h1>
<section id="orthogonal-predictors" class="level2">
<h2 class="anchored" data-anchor-id="orthogonal-predictors">Orthogonal predictors</h2>
<ul>
<li>Another approach for computing least squares is based on the notion of orthogonality.</li>
</ul>
<ul>
<li><p>If the <span class="blue">predictors</span> were mutually <span class="orange">orthogonal</span>, the problem would be much simpler.</p></li>
<li><p>In other words, consider a linear model of the form <span class="math display">
\bm{Y} = \bm{Z}\bm{\beta} + \bm{\epsilon},
</span> where <span class="math inline">\bm{Z} = (\tilde{\bm{z}}_1,\dots,\tilde{\bm{z}}_p)</span>. <span class="orange">Orthogonality</span> means that <span class="math inline">\bm{Z}^T\bm{Z} = \text{diag}(\tilde{\bm{z}}_1^T\tilde{\bm{z}}_1,\dots,\tilde{\bm{z}}_p^T\tilde{\bm{z}}_p)</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition A.2. OLS with orthogonal predictors
</div>
</div>
<div class="callout-body-container callout-body">
<p>The least square estimate <span class="math inline">\hat{\bm{\beta}} = (\hat{\beta}_1,\dots,\hat{\beta}_p)^T</span> with orthogonal predictors is <span class="math display">
\hat{\beta}_j = \frac{\tilde{\bm{z}}_j^T\bm{y}}{\tilde{\bm{z}}_j^T\tilde{\bm{z}}_j}, \qquad j=1,\dots,p.
</span></p>
</div>
</div>
</section>
<section id="regression-by-successive-orthogonalization" class="level2">
<h2 class="anchored" data-anchor-id="regression-by-successive-orthogonalization">Regression by successive orthogonalization</h2>
<div class="incremental">
<ul class="incremental">
<li><p>Clearly, the predictors of <span class="math inline">\bm{X}</span> are generally not orthogonal. Hence, we want to find a suitable transformation <span class="math inline">\bm{Z} = \bm{X} \bm{\Gamma}^{-1}</span> that <span class="blue">orthogonalize</span> the <span class="orange">predictors</span>.</p></li>
<li><p>Suppose, for example, that <span class="math inline">p = 2</span>. We set <span class="orange">first orthogonal predictor</span> <span class="math inline">\tilde{\bm{z}}_1 = \tilde{\bm{x}}_1</span>.</p></li>
<li><p>We then consider the following <span class="blue">univariate</span> regression problem <span class="math display">
\tilde{\bm{x}}_2 = \gamma \tilde{\bm{z}}_1 + \bm{\epsilon}, \qquad \text{which leads} \qquad \hat{\gamma} = \frac{\tilde{\bm{z}}_1^T\tilde{\bm{x}}_2}{\tilde{\bm{z}}_1^T\tilde{\bm{z}}_1}.
</span></p></li>
<li><p>The <span class="orange">second orthogonal predictor</span> is obtained as the <span class="blue">residual term</span>: <span class="math display">
\tilde{\bm{z}}_2 = \tilde{\bm{x}}_2 - \hat{\gamma}\tilde{\bm{z}}_1.
</span></p></li>
<li><p>The geometry of linear models guarantees that <span class="math inline">\tilde{\bm{z}}_1^T\tilde{\bm{z}}_2 = 0</span>.</p></li>
</ul>
</div>
</section>
<section id="gram-schmidt-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="gram-schmidt-algorithm">Gram-Schmidt algorithm</h2>
<div class="incremental">
<ul class="incremental">
<li><p>Let us now consider the <span class="orange">general case</span>, valid for any value of <span class="math inline">p</span>.</p></li>
<li><p><span class="blue">Initialization</span>. Set <span class="math inline">\tilde{\bm{z}}_1 = \tilde{\bm{x}}_1</span>.</p></li>
<li><p><span class="blue">For <span class="math inline">j= 2,\dots,p</span></span>. Consider the regression problem with <span class="math inline">j-1</span> orthogonal predictors <span class="math display">
\tilde{\bm{x}}_j = \sum_{k=1}^{j-1}\gamma_{kj} \tilde{\bm{z}}_k + \bm{\epsilon}_j, \quad \text{which leads} \quad \hat{\gamma}_{kj} = \frac{\tilde{\bm{z}}_k^T\tilde{\bm{x}}_j}{\tilde{\bm{z}}_k^T\tilde{\bm{z}}_k}, \quad k=1,\dots,j-1,
</span> Then, compute the new vector <span class="math inline">\bm{z}_j</span> as the <span class="orange">residual</span> term <span class="math display">
\tilde{\bm{z}}_j = \tilde{\bm{x}}_j - \sum_{k=1}^{j-1}\hat{\gamma}_{kj} \tilde{\bm{z}}_k
</span></p></li>
<li><p>The geometry of linear models guarantees <span class="orange">orthogonality</span>, that is <span class="math inline">\tilde{\bm{z}}_j^T \tilde{\bm{z}}_{j'} = 0</span> for any <span class="math inline">j \neq j'</span>.</p></li>
</ul>
</div>
</section>
<section id="the-qr-decomposition-i" class="level2">
<h2 class="anchored" data-anchor-id="the-qr-decomposition-i">The QR decomposition I</h2>
<div class="incremental">
<ul class="incremental">
<li><p>By construction, the Gram-Schmidt algorithm produces the following decomposition <span class="math display">
\bm{X} = \bm{Z} \bm{\Gamma}, \qquad \bm{\Gamma} =
\begin{bmatrix}
1 &amp; \hat{\gamma}_{12} &amp; \hat{\gamma}_{13} &amp;\cdots &amp; \hat{\gamma}_{1p} \\
0 &amp;  1 &amp; \hat{\gamma}_{23} &amp;\cdots &amp; \hat{\gamma}_{2p} \\
\vdots &amp; \vdots &amp; \vdots &amp;\ddots &amp; \vdots\\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1
\end{bmatrix}, \qquad \bm{Z} = (\tilde{\bm{z}}_1,\dots,\tilde{\bm{z}}_p).
</span></p></li>
<li><p>The <span class="math inline">p \times p</span> matrix <span class="math inline">\bm{\Gamma}</span> is <span class="orange">upper triangular</span>, whereas the columns of the <span class="math inline">n \times p</span> matrix <span class="math inline">\bm{Z}</span> are <span class="blue">mutually orthogonal</span>, due to the properties of the residuals of a linear model.</p></li>
<li><p>It is often convenient to <span class="orange">standardize</span> the columns of <span class="math inline">\bm{Z}</span>, dividing them by their norm <span class="math inline">||\tilde{\bm{z}}_j||</span>. Let <span class="math inline">\bm{D} = \text{diag}(||\tilde{\bm{z}}_1||, \dots, ||\tilde{\bm{z}}_p||)</span>, then in matrix notation: <span class="math display">
\bm{X} = \bm{Z} \bm{\Gamma} = \bm{Z} \bm{D}^{-1} \bm{D} \bm{\Gamma} = \bm{Q} \bm{R}, \quad \text{with} \quad \bm{Q} = \bm{Z}\bm{D}^{-1}.
</span></p></li>
<li><p><span class="orange">Remark</span>. Note that <span class="math inline">\bm{Q}^T \bm{Q} = I_p</span>, i.e.&nbsp;the columns of <span class="math inline">\bm{Q}</span> are <span class="blue">orthonormal</span>.</p></li>
</ul>
</div>
</section>
<section id="the-qr-decomposition-ii" class="level2">
<h2 class="anchored" data-anchor-id="the-qr-decomposition-ii">The QR decomposition II</h2>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (QR factorization)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose <span class="math inline">\bm{X} \in \mathbb{R}^{n \times p}</span> with <span class="math inline">n \ge p</span> has full rank, that is <span class="math inline">\text{rk}(\bm{X}) = p</span>. Then, there exists a factorization of the form <span class="math display">
\bm{X} = \bm{Q} \bm{R},
</span> where <span class="math inline">\bm{Q} \in \mathbb{R}^{n \times p}</span> has <span class="orange">orthonormal columns</span> and <span class="math inline">\bm{R} \in \mathbb{R}^{p \times p}</span> is an <span class="blue">upper triangular</span> matrix.</p>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Corollary (QR factorization)
</div>
</div>
<div class="callout-body-container callout-body">
<p>The QR decomposition is <span class="blue">unique</span> up to <span class="blue">sign flips</span> of the columns of <span class="math inline">\bm{Q}</span> and the rows of <span class="math inline">\bm{R}</span>. Moreover, if <span class="math inline">\bm{R}</span> has positive diagonal entries, as the one obtained using Gram-Schmidt, then it coincides with the <span class="orange">Cholesky factor</span> of <span class="math inline">\bm{X}^T\bm{X}</span>.</p>
</div>
</div>
</section>
<section id="the-qr-decomposition-and-least-squares" class="level2">
<h2 class="anchored" data-anchor-id="the-qr-decomposition-and-least-squares">The QR decomposition and least squares</h2>
<div class="incremental">
<ul class="incremental">
<li><p>The QR decomposition greatly facilitates computations for linear models. Indeed: <span class="math display">
\begin{aligned}
\hat{\beta} &amp;= (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y} = [(\bm{Q}\bm{R})^T\bm{Q}\bm{R}]^{-1}(\bm{Q}\bm{R})^T\bm{y}\\
&amp;= (\bm{R}^T\bm{Q}^T\bm{Q}\bm{R})^{-1}\bm{R}^T\bm{Q}^T\bm{y} \\
&amp;= \bm{R}^{-1} (\bm{R}^T)^{-1} \bm{R}^T\bm{Q}^T\bm{y} \\
&amp;= \bm{R}^{-1}\bm{Q}^T \bm{y}.
\end{aligned}
</span></p></li>
<li><p>Hence, the least square estimate is obtained as the solution of the <span class="blue">triangular system</span> <span class="math display">
\bm{R}\beta = \bm{Q}^T\bm{y},
</span> which can be easily solved via <span class="orange">backward substitution</span>.</p></li>
<li><p>As a particular case of the above equation, one gets <span class="math inline">\hat{\beta}_p = (\tilde{\bm{z}}_p^T\bm{y}) / (\tilde{\bm{z}}_p^T \tilde{\bm{z}}_p)</span>.</p></li>
</ul>
</div>
</section>
<section id="the-qr-decomposition-and-linear-models" class="level2">
<h2 class="anchored" data-anchor-id="the-qr-decomposition-and-linear-models">The QR decomposition and linear models</h2>
<div class="incremental">
<ul class="incremental">
<li><p>An important advantage of the QR factorization is that many other useful quantities can be readily computed. For example, the <span class="orange">covariance matrix</span> is obtained as: <span class="math display">
  s^2 (\bm{X}^T \bm{X})^{-1} = s^2 \bm{R}^{-1} (\bm{R}^{-1})^T.
  </span></p></li>
<li><p>The <span class="orange">predicted values</span> and the <span class="blue">projection matrix</span> are also easily obtained as <span class="math display">
\hat{\bm{y}} = \bm{H}\bm{y} = \bm{Q}\bm{Q}^T\bm{y}.
</span></p></li>
<li><p>The diagonal elements <span class="math inline">h_i = [\bm{H}]_{ii}</span> of the hat matrix <span class="math inline">\bm{H}</span> are called <span class="orange">leverages</span> and one may want to compute them without evaluating the full <span class="math inline">n \times n</span> matrix, using <span class="math display">
h_i = \sum_{j=1}^p q_{ij}^2, \qquad i=1,\dots,n,
</span> where <span class="math inline">q_{ij}</span> are the entries of <span class="math inline">\bm{Q}</span>.</p></li>
</ul>
</div>
</section>
<section id="computational-complexity-1" class="level2">
<h2 class="anchored" data-anchor-id="computational-complexity-1">Computational complexity</h2>
<div class="incremental">
<ul class="incremental">
<li><p>The solution via QR factorization is <span class="orange">numerically reliable</span> and it facilitates the computation of other quantities of interest.</p></li>
<li><p>In practice, the QR is computed via a <span class="blue">modified Gram-Schmidt</span>, that fixes the instabilities of the naïve Gram-Schmidt algorithm, or via <span class="orange">Householder reflections</span>.</p></li>
<li><p>The expensive step is the QR factorization. The overall computational complexity is <span class="math display">
\sim 2 n p^2,</span> which is about twice that of the Cholesky, when <span class="math inline">n</span> is much larger than <span class="math inline">p</span>, and about the same when <span class="math inline">p \approx n</span>.</p></li>
<li><p>Depending on the context and assuming we only care about <span class="math inline">\hat{\beta}</span>, we may prefer the Cholesky (<span class="blue">fast</span> but <span class="orange">imprecise</span>) or the QR (<span class="orange">slower</span> but more <span class="blue">reliable</span>).</p></li>
<li><p>The default approach in R, i.e., the one implemented in the <code>lm</code> function is the QR factorization because one typically also needs to compute <span class="math inline">\bm{H}</span>, or the leverages.</p></li>
</ul>
</div>
</section>
<section id="pivoting-and-rank-deficiencies" class="level2">
<h2 class="anchored" data-anchor-id="pivoting-and-rank-deficiencies">☠️ - Pivoting and rank deficiencies</h2>
<div class="incremental">
<ul class="incremental">
<li><p>If <span class="math inline">\text{rk}(\bm{X}) = k &lt; p</span> (<span class="orange">rank deficiency</span>) then it is still possible to obtain a “QR” factorization of the form <span class="math display">
\bm{X}\bm{P} = \bm{Q}\begin{bmatrix}\bm{R}_{11} &amp; \bm{R}_{12} \\
0 &amp; 0\end{bmatrix},
</span> where <span class="math inline">\bm{P}</span> is a <span class="math inline">p × p</span> permutation matrix and <span class="math inline">\bm{R}_{11}</span> is an <span class="math inline">k \times k</span> upper triangular and non-singular matrix.</p></li>
<li><p>This operation is sometimes called <span class="blue">pivoting</span>, and it is particularly important even when <span class="math inline">\text{rk}(\bm{X}) = p</span> to prevent numerical issues when the condition number <span class="math inline">\kappa(\bm{X})</span> is high.</p></li>
<li><p>In the presence of perfect collinearity, the implementation of the QR decomposition in R (<code>qr</code>) relies on pivoting. This is why the <code>lm</code> function can automatically “omit” a predictor.</p></li>
</ul>
</div>
</section>
</section>
<section id="iterative-methods" class="level1">
<h1>Iterative methods</h1>
<section id="when-n-is-very-large" class="level2">
<h2 class="anchored" data-anchor-id="when-n-is-very-large">When <span class="math inline">n</span> is very large…</h2>
<div class="incremental">
<ul class="incremental">
<li><p>When the <span class="blue">sample size</span> <span class="math inline">n</span> is <span class="orange">extremely large</span>, as it is common in data mining problems, then the QR factorization cannot be computed.</p></li>
<li><p>Indeed, even <span class="orange">loading</span> <span class="math inline">\bm{X}</span> into <span class="orange">memory</span> could be problematic.</p></li>
<li><p>In the normal equations approach, we only need to compute the <span class="blue">sufficient statistics</span>: <span class="math display">
\bm{W} = \bm{X}^T\bm{X}, \qquad \bm{u} = \bm{X}^T\bm{y},
</span> which are of dimension <span class="math inline">p\times p</span> and <span class="math inline">p \times 1</span>, respectively.</p></li>
<li><p>If we knew <span class="math inline">\bm{W}</span> and <span class="math inline">\bm{u}</span>, then we could obtain the least square estimate <span class="math inline">\hat{\beta}</span> using the Cholesky factorization.</p></li>
<li><p>However, when <span class="math inline">n</span> is extremely large, the difficult part is indeed computing <span class="math inline">\bm{W}</span> and <span class="math inline">\bm{u}</span>!</p></li>
</ul>
</div>
</section>
<section id="recursive-data-import" class="level2">
<h2 class="anchored" data-anchor-id="recursive-data-import">Recursive data import</h2>
<div class="incremental">
<ul class="incremental">
<li><p>Using matrix notation, we express <span class="math inline">\bm{W} = \bm{W}_{(n)}</span> and <span class="math inline">\bm{u} = \bm{u}_{(n)}</span> as follows <span class="math display">
\bm{W}_{(n)} = \sum_{i=1}^n \bm{x}_i \bm{x}_i^T, \qquad \bm{u}_{(n)} = \sum_{i=1}^n\bm{x}_i y_i.
</span></p></li>
<li><p>Let us define the <span class="blue">initial conditions</span> <span class="math inline">\bm{W}_{(1)} = \bm{x}_1 \bm{x}_1^T</span> and <span class="math inline">\bm{u}_{(1)} = \bm{x}_1 y_1</span>.</p></li>
<li><p>Then, the following <span class="orange">recursive relationship</span> holds: <span class="math display">
\bm{W}_{(i)} = \bm{W}_{(i-1)} +  \bm{x}_i \bm{x}_i^T, \qquad \bm{u}_{(i)} = \bm{u}_{(i-1)} + \bm{x}_i y_i, \qquad i=2,\dots,n,
</span> where <span class="math inline">\bm{W}_{(i)}</span> is the matrix formed by the first <span class="math inline">i</span> summands of <span class="math inline">\bm{W}_{(n)}</span> and analogously <span class="math inline">\bm{u}_{(i)}</span>.</p></li>
<li><p>Hence <span class="math inline">\bm{W}_{(n)}</span> and <span class="math inline">\bm{u}_{(n)}</span> can be calculated by <span class="blue">importing a single record</span> at a time, which does not create memory issues.</p></li>
</ul>
</div>
</section>
<section id="recursive-estimates" class="level2">
<h2 class="anchored" data-anchor-id="recursive-estimates">Recursive estimates</h2>
<div class="incremental">
<ul class="incremental">
<li><p>In many occasions, the data flows continuously, meaning that we get a new pair of observations <span class="math inline">(\bm{x}_{n+1}, y_{n+1})</span> every minute, or even every second.</p></li>
<li><p>In these cases, we would like to <span class="orange">update</span> the current least square estimate <span class="math inline">\hat{\beta}_{(n)}</span> with the new information <span class="math inline">(\bm{x}_{n+1}, y_{n+1})</span>, but ideally without re-doing all the calculations.</p></li>
<li><p>The <span class="blue">recursive data import</span> of the <a href="#recursive-data-import">previous slide</a> is partially unsatisfactory, because one would need to invert (or factorize) a <span class="math inline">p \times p</span> matrix every time, which could be costly.</p></li>
<li><p>Let us define some useful quantity: <span class="math display">
\bm{V}_{(n)} = \bm{W}_{(n)}^{-1} = (\bm{X}_{(n)}^T\bm{X}_{(n)})^{-1},
</span> where <span class="math inline">\bm{X}_{(n)}</span> denotes the design matrix with <span class="math inline">n</span> observations and analogously <span class="math inline">\bm{y}_{(n)}</span>.</p></li>
</ul>
</div>
</section>
<section id="sherman-morrison-formula" class="level2">
<h2 class="anchored" data-anchor-id="sherman-morrison-formula">Sherman-Morrison formula</h2>
<ul>
<li><p>When the new data points arrive, we can write the updated quantities <span class="math display">
\bm{X}_{(n+1)} = (\bm{X}_{(n)}, \bm{x}_{n + 1})^T, \quad \bm{W}_{(n + 1)} = (\bm{X}_{(n+1)}^T\bm{X}_{(n + 1)}) = (\bm{X}_{(n)}^T\bm{X}_{(n)} +  \bm{x}_{n + 1} \bm{x}_{n + 1}^T).
</span></p></li>
<li><p>The difficult part is to <span class="orange">efficiently</span> compute <span class="math inline">\bm{V}_{(n+1)} = \bm{W}_{(n + 1)}^{-1}</span>. The following result of linear algebra is of incredible help in this regard.</p></li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Sherman-Morrison formula
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\bm{A} \in \mathbb{R}^{p\times p}</span> be an invertible matrix and let <span class="math inline">\bm{b},\bm{d}</span> be <span class="math inline">p</span>-dimensional vectors. Then <span class="math display">
(\bm{A} + \bm{b} \bm{d}^T)^{-1} = \bm{A}^{-1} - \frac{1}{1 + \bm{d}^T \bm{A}^{-1}\bm{b}}\bm{A}^{-1}\bm{b}\bm{d}^T\bm{A}^{-1}.
</span></p>
</div>
</div>
</section>
<section id="the-recursive-least-squares-algorithm-i" class="level2">
<h2 class="anchored" data-anchor-id="the-recursive-least-squares-algorithm-i">The recursive least squares algorithm I</h2>
<div class="incremental">
<ul class="incremental">
<li><p>Using the Sherman-Morrison formula, then we can express the new matrix <span class="math inline">\bm{V}_{(n + 1)}</span> as a function of previously computed quantities: <span class="math display">
\bm{V}_{(n + 1)} = \bm{V}_{(n)} - v_n\bm{V}_{(n)} \bm{x}_{n + 1}\bm{x}_{n + 1}^T \bm{V}_{(n)}, \quad v_n = \frac{1}{(1 + \bm{x}_{n + 1}^T \bm{V}_{(n)}\bm{x}_{n + 1})}.
</span></p></li>
<li><p>The <span class="orange">updated least square estimate</span> therefore becomes <span class="math display">
\begin{aligned}
\hat{\beta}_{(n+1)} &amp;= \bm{V}_{(n + 1)}(\bm{X}_{(n)}^T\bm{y}_{(n)} +  \bm{x}_{n + 1} y_{n+1}) \\
&amp;=\hat{\beta}_{(n)} +  \underbrace{v_n \bm{V}_{(n)}\bm{x}_{n + 1}}_{\bm{k}_n}\underbrace{(y_{n+1} - \bm{x}_{n + 1}^T\hat{\beta}_{(n)})}_{e_{n + 1}} \\
&amp;= \hat{\beta}_{(n)} + \bm{k}_n e_{n+1}.
\end{aligned}
</span></p></li>
<li><p>The quantity <span class="math inline">e_{n+1}</span> is the <span class="blue">prediction error</span> of <span class="math inline">y_{n+1}</span> based on the previous estimate <span class="math inline">\hat{\beta}_{(n)}</span>.</p></li>
</ul>
</div>
</section>
<section id="the-recursive-least-squares-algorithm-ii" class="level2">
<h2 class="anchored" data-anchor-id="the-recursive-least-squares-algorithm-ii">The recursive least squares algorithm II</h2>
<ul>
<li><p>The recursive estimation <span class="math inline">\hat{\beta}_{(n+1)} = \hat{\beta}_{(n)} + \bm{k}_n e_{n+1}</span> takes the form of a <span class="orange">linear filter</span>, in which the new estimate <span class="math inline">\hat{\beta}_{(n+1)}</span> is obtained by modifying the old one <span class="math inline">\hat{\beta}_{(n+1)}</span>.</p></li>
<li><p>This is performed according to the <span class="blue">prediction error</span> <span class="math inline">\epsilon_{n+1}</span> and the <span class="orange">gain</span> <span class="math inline">k_n</span> of the filter.</p></li>
<li><p>Using a terminology typical of the machine learning field, we say that the estimator “learns from its errors.”</p></li>
<li><p>If <span class="math inline">n</span> is sufficiently high, it is also possible to get an <span class="orange">approximate</span> solution by initializing the algorithm by setting <span class="math inline">\bm{V}_{(0)} = I_p</span>, to avoid any matrix inversion / factorization.</p></li>
</ul>
<ul>
<li>With further algebraic steps, we also obtain a <span class="blue">recursive formula</span> for the <span class="blue">deviance</span></li>
</ul>
<p><span class="math display">
||\bm{y}_{(n + 1)} - \bm{X}_{(n+1)}\hat{\beta}_{(n+1)}||^2 = ||\bm{y}_{(n)} - \bm{X}_{(n)}\hat{\beta}_{(n)}||^2 + v_n e_{n+1}^2.
</span></p>
<ul>
<li>The complete algorithm is provided in A&amp;S, Algorithm 2.2.</li>
</ul>
</section>
</section>
<section id="generalized-linear-models" class="level1">
<h1>Generalized linear models</h1>
<section id="the-heart-dataset" class="level2">
<h2 class="anchored" data-anchor-id="the-heart-dataset">The <code>heart</code> dataset</h2>
<div class="columns">
<div class="column" style="width:40%;">
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="750"></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li><p>Let us consider the South African <code>heart</code> <a href="https://hastie.su.domains/ElemStatLearn/datasets/SAheart.data">dataset</a>, described is <span class="blue">Section 4.4.2</span> of HTF (2009).</p></li>
<li><p>We want to <span class="blue">predict</span> the insurgence of <span class="orange">coronary heart disease</span> (<code>chd</code>) as a function of known risk factors:</p>
<ul>
<li>Cholesterol level (<code>ldl</code>) and obesity (<code>obesity</code>)</li>
<li>Consumption of tobacco (<code>tobacco</code>) and alcohol (<code>alcool</code>)</li>
<li>Systolic blood pressure (<code>sbp</code>)</li>
<li>Age (<code>age</code>) and family history (<code>famhist</code>)</li>
</ul></li>
<li><p>The response variable is <span class="orange">binary</span>: we cannot rely on linear regression models.</p></li>
</ul>
</div>
</div>
</section>
<section id="the-heart-dataset-1" class="level2">
<h2 class="anchored" data-anchor-id="the-heart-dataset-1">The <code>heart</code> dataset</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1650"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="generalized-linear-models-1" class="level2">
<h2 class="anchored" data-anchor-id="generalized-linear-models-1">Generalized linear models</h2>
<ul>
<li><p>If the outcome is binary, a count, or if the errors are heteroscedastic, then the Gaussian linear regression model might be inappropriate.</p></li>
<li><p>We let <span class="math inline">Y_i</span> be iid draws from an <span class="blue">exponentialy dispersion family</span>, which includes the Binomial, the Poisson, and the Gaussian distribution as special cases.</p></li>
<li><p>The canonical statistical solution are <span class="orange">generalized linear models</span>, which are usually taught in undergraduate courses (e.g.&nbsp;Statistica III). This is just a <span class="orange">short recap</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Generalized linear model (GLMs)
</div>
</div>
<div class="callout-body-container callout-body">
<p>In a <span class="blue">generalized linear model</span>, the response variable is related to the covariates through: <span class="math display">
\mathbb{E}(Y_i) = g\{f(\bm{x}_i; \beta)\} = g(\beta_1 x_{i1} + \cdots + \beta_p x_{ip}) = g(\bm{x}_i^T\beta),
</span> where <span class="math inline">g^{-1}(\cdot)</span> is the so-called <span class="orange">link function</span>, which is known and invertible.</p>
</div>
</div>
</section>
<section id="likelihood-based-inference" class="level2">
<h2 class="anchored" data-anchor-id="likelihood-based-inference">Likelihood-based inference</h2>
<div class="incremental">
<ul class="incremental">
<li><p>Let <span class="math inline">\pi(y; \bm{x}, \theta)</span> be the <span class="blue">density</span> associated to each <span class="math inline">Y_i</span>. Here <span class="math inline">\theta</span> is a vector of parameters.</p></li>
<li><p>The <span class="orange">log-likelihood</span> of a GLM is <span class="math display">
\ell(\theta) = \ell(\theta;\bm{y}) = \sum_{i=1}^n \log{\pi(y_i; \bm{x}_i, \theta)} + c,
</span> where <span class="math inline">c</span> is an additive constant not depending on <span class="math inline">\beta</span>.</p></li>
<li><p>The <span class="orange">maximum likelihood estimate</span> for the regression coefficients <span class="math inline">\beta</span> is <span class="math display">
\hat{\theta} = \arg\max_\theta \ell(\theta).
</span></p></li>
<li><p>Standard errors, tests, and confidence intervals can be easily obtained from (derivatives of) the log-likelihood, as you have seen in undergraduate courses (e.g., Statistica II and III).</p></li>
<li><p>The aforementioned inferential results are often grounded on <span class="orange">asymptotic theory</span> and <span class="blue">quadratic approximations</span> of the log-likelihood.</p></li>
</ul>
</div>
</section>
<section id="linear-models-with-gaussian-errors" class="level2">
<h2 class="anchored" data-anchor-id="linear-models-with-gaussian-errors">Linear models with Gaussian errors</h2>
<div class="incremental">
<ul class="incremental">
<li><p>When <span class="math inline">Y_i</span> are Gaussian random variables, then the log-likelihood is <span class="math display">
\ell(\beta, \sigma^2) = -\frac{n}{2}\log{\sigma^2} - \frac{1}{\sigma^2}D(\beta),
</span> where <span class="math inline">D(\beta)</span> is the same quantity we have <a href="/#/linear-regression-estimation-i">defined before</a>.</p></li>
<li><p>Hence, the <span class="blue">ordinary least squares</span> estimate <span class="math inline">\hat{\beta}</span> is also the <span class="orange">maximum likelihood</span> estimate. Indeed, the maximizer of the log-likelihood with respect to <span class="math inline">\beta</span> is also the minimizer of <span class="math inline">D(\beta)</span>.</p></li>
<li><p>Instead, the maximum likelihood estimate for the variance is <span class="math inline">\hat{\sigma}^2 = D(\hat{\beta}) / n</span>.</p></li>
<li><p>Note in addition that the log-likelihood, evaluated at its maximum, is <span class="math display">
-2 \ell(\hat{\beta},\hat{\sigma}^2) =  n \log\{D(\beta)/n\} + n,
</span> a quantity that will turn useful in <a href="un_B.html">Unit B</a>.</p></li>
</ul>
</div>
</section>
<section id="binary-classification-via-logistic-regression" class="level2">
<h2 class="anchored" data-anchor-id="binary-classification-via-logistic-regression">Binary classification via logistic regression</h2>
<div class="incremental">
<ul class="incremental">
<li><p>The <code>heart</code> dataset presents a <span class="blue">classification</span> problem, in which we assume that <span class="math display">
Y_i \overset{\text{ind}}{\sim} \text{Bern}\{p(\bm{x}_i; \beta)\}, \qquad i=1,\dots,n.
</span></p></li>
<li><p>The <span class="blue">canonical link</span> function <span class="math inline">g(x) = e^x / (1 + e^{x})</span> leads to the GLM <span class="math display">
\mathbb{P}(Y_i = 1)= p(\bm{x}_i; \beta) = \frac{\exp\{f(\bm{x}_i; \beta)\}}{1 + \exp\{f(\bm{x}_i; \beta)\}} = \frac{\exp(\beta_1 x_{i1} + \cdots + \beta_p x_{ip})}{1 + \exp(\beta_1 x_{i1} + \cdots + \beta_p x_{ip})},
</span> known as <span class="orange">logistic regression</span>. Note that in this model <span class="math inline">\mathbb{E}(Y_i) = \mathbb{P}(Y_i = 1)</span>.</p></li>
<li><p>After some algebra, it can be shown that the log-likelihood is <span class="math display">
\ell(\beta) = \sum_{i=1}^n y_i (\bm{x}_i^T\beta) - \log\{1 + \exp(\bm{x}_i^T\beta)\}.
</span></p></li>
<li><p>Moreover, the <span class="orange">predicted values</span> <span class="math inline">p(\bm{x}_i; \hat{\beta})</span> are <span class="blue">probabilities</span> and they belong to <span class="math inline">(0,1)</span>.</p></li>
</ul>
</div>
</section>
<section id="iteratively-re-weighted-least-squares-i" class="level2">
<h2 class="anchored" data-anchor-id="iteratively-re-weighted-least-squares-i">Iteratively re-weighted least squares I</h2>
<div class="incremental">
<ul class="incremental">
<li><p>Let us define the so-called <span class="blue">score function</span>: <span class="math display">
\ell'(\beta) = \frac{\partial}{\partial \beta}\ell(\beta) = \sum_{i=1}^n\bm{x}_i \{y_i - p(\bm{x}_i; \beta)\}, \qquad
</span></p></li>
<li><p>Moreover, the so-called <span class="blue">observed information matrix</span> is <span class="math display">
j(\beta) = - \frac{\partial^2}{\partial \beta\partial \beta^T}\ell(\beta) = -\sum_{i=1}^n \bm{x}_i\bm{x}_i^T w_i(\beta), \quad w_i(\beta) = p(\bm{x}_i; \beta)\{1 - p(\bm{x}_i; \beta)\}.
</span></p></li>
<li><p>In <span class="orange">matrix notation</span>, we will write <span class="math display">
\ell'(\beta) = \bm{X}^T(\bm{y} - \bm{p}), \qquad j(\beta) = \bm{X}^T\bm{W}\bm{X},
</span> where <span class="math inline">\bm{p} = (p(\bm{x}_1; \beta), \dots, p(\bm{x}_n; \beta))</span> and <span class="math inline">\bm{W} = \text{diag}(w_1(\beta),\dots, w_n(\beta))</span>.</p></li>
<li><p>Unfortunately, a closed-form expression for <span class="math inline">\hat{\beta}</span>, solving the <span class="blue">likelihood equations</span> <span class="math inline">\bm{X}^T(\bm{y} - \bm{p}) = 0</span>, is not available. We need to use <span class="orange">iterative algorithms</span>.</p></li>
</ul>
</div>
</section>
<section id="iteratively-re-weighted-least-squares-ii" class="level2">
<h2 class="anchored" data-anchor-id="iteratively-re-weighted-least-squares-ii">Iteratively re-weighted least squares II</h2>
<div class="incremental">
<ul class="incremental">
<li><p>As you <a href="https://tommasorigon.github.io/introR/lezioni/un_K.html#il-metodo-di-newton-raphson-i">may remember</a>, in the Newton-Raphson <span class="orange">iterative method</span> we consider a <span class="blue">quadratic approximation</span> of the log-likelihood on <span class="math inline">\beta_0</span>, so that: <span class="math display">
\ell(\beta) \approx \ell(\beta_0) + \ell'(\beta_0)^T(\beta - \beta_0) - \frac{1}{2}(\beta - \beta_0)^Tj(\beta)(\beta - \beta_0).
</span></p></li>
<li><p>By maximizing the quadratic approximation, in logistic regression we get the update <span class="math display">
\begin{aligned}
\beta^{\text{new}} &amp;= \beta^{(\text{old})} + j(\beta^{\text{old}})^{-1}\ell'(\beta^{\text{old}}) = \beta^{(\text{old})} + (\bm{X}^T\bm{W}\bm{X})^{-1}\bm{X}^T(\bm{y} - \bm{p}) \\
&amp; = (\bm{X}^T\bm{W}\bm{X})^{-1}\bm{X}^T\bm{W}\bm{z},
\end{aligned}
</span> where <span class="math inline">\bm{z} = \bm{X}\beta^{(\text{old})} + \bm{W}^{-1}(\bm{y} - \bm{p})</span>, which we <span class="orange">cycle repeatedly</span> until <span class="blue">convergence</span>.</p></li>
<li><p>This algorithm is sometimes called <span class="blue">iteratively re-weighted least squares</span> (IRLS), because <span class="orange">each step</span> can be seen as the solution of the weighted least squares problem <span class="math display">
\beta^{\text{new}} = \arg\min_\beta (\bm{z} - \bm{X}\beta)^T \bm{W}(\bm{z} - \bm{X}\beta).
</span></p></li>
</ul>
</div>
</section>
<section id="computational-considerations" class="level2">
<h2 class="anchored" data-anchor-id="computational-considerations">Computational considerations</h2>
<div class="incremental">
<ul class="incremental">
<li><p>Each <span class="orange">step</span> of the IRLS algorithm finds the solution of the likelihood equations <span class="math display">
(\bm{X}^T\bm{W}\bm{X})\beta = \bm{X}^T\bm{W}\bm{z}.
</span></p></li>
<li><p>Once again, the Cholesky and the QR decomposition can be exploited to speed up computations. See Section 5.4 of Arnold et al.&nbsp;(2019) for further details.</p></li>
<li><p>It can be shown that <span class="math inline">\beta = (0,\dots,0)^T</span> is a good starting point for the initialization.</p></li>
<li><p>Unfortunately, the IRLS is <span class="orange">not</span> guaranteed to <span class="orange">converge</span> nor to increase the log-likelihood at every step, but there are easy fixes.</p></li>
<li><p>See this <a href="https://github.com/tommasorigon/logisticVB/blob/master/em_logistic_tutorial.md">tutorial</a> for an example of the <span class="orange">failure</span> of IRLS.</p></li>
<li><p>These considerations and the IRLS algorithm can be easily generalized to all GLMs.</p></li>
</ul>
</div>
</section>
<section id="the-estimated-model" class="level2">
<h2 class="anchored" data-anchor-id="the-estimated-model">The estimated model</h2>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">term</th>
<th style="text-align: right;">estimate</th>
<th style="text-align: right;">std.error</th>
<th style="text-align: right;">statistic</th>
<th style="text-align: right;">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>(Intercept)</code></td>
<td style="text-align: right;">-4.130</td>
<td style="text-align: right;">0.964</td>
<td style="text-align: right;">-4.283</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>sbp</code></td>
<td style="text-align: right;">0.006</td>
<td style="text-align: right;">0.006</td>
<td style="text-align: right;">1.023</td>
<td style="text-align: right;">0.306</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>tobacco</code></td>
<td style="text-align: right;">0.080</td>
<td style="text-align: right;">0.026</td>
<td style="text-align: right;">3.034</td>
<td style="text-align: right;">0.002</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>ldl</code></td>
<td style="text-align: right;">0.185</td>
<td style="text-align: right;">0.057</td>
<td style="text-align: right;">3.219</td>
<td style="text-align: right;">0.001</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>famhist_Present</code></td>
<td style="text-align: right;">0.939</td>
<td style="text-align: right;">0.225</td>
<td style="text-align: right;">4.177</td>
<td style="text-align: right;">0.000</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>obesity</code></td>
<td style="text-align: right;">-0.035</td>
<td style="text-align: right;">0.029</td>
<td style="text-align: right;">-1.187</td>
<td style="text-align: right;">0.235</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>alcohol</code></td>
<td style="text-align: right;">0.001</td>
<td style="text-align: right;">0.004</td>
<td style="text-align: right;">0.136</td>
<td style="text-align: right;">0.892</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>age</code></td>
<td style="text-align: right;">0.043</td>
<td style="text-align: right;">0.010</td>
<td style="text-align: right;">4.181</td>
<td style="text-align: right;">0.000</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Results of the IRLS algorithm applied to the logistic regression model using <code>heart</code> data.</p></li>
<li><p>The coefficient of the variable <code>obesity</code> is <span class="orange">negative</span>. Why do you think this is the case?</p></li>
</ul>
</section>
<section id="prediction-and-model-assessment-i" class="level2">
<h2 class="anchored" data-anchor-id="prediction-and-model-assessment-i">Prediction and model assessment I</h2>
<ul>
<li><p>The <span class="orange">predicted probabilities</span> <span class="math inline">p(\bm{x}_i; \hat{\beta})</span> are often <span class="blue">thresholded</span>, to obtain 0-1 predicted values, mostly useful for interpretative reasons <span class="math display">
\hat{y}_i = \mathbb{I}(p(\bm{x}_i; \hat{\beta}) &gt; c), \qquad i=1,\dots,n
</span> for some threshold <span class="math inline">c</span>, which is <span class="orange">usually</span> set equal to <span class="math inline">1/2</span>.</p></li>
<li><p>We can then compare the responses with the predicted values using the <span class="blue">confusion matrix</span>:</p></li>
</ul>
<table class="table">
<tbody>
<tr class="odd">
<td></td>
<td><strong>Actual response</strong></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Prediction</strong></td>
<td><strong>0</strong></td>
<td><strong>1</strong></td>
<td><strong>Total</strong></td>
</tr>
<tr class="odd">
<td><strong>0</strong></td>
<td><span class="math inline">n_{00}</span></td>
<td><span class="math inline">n_{01}</span></td>
<td><span class="math inline">n_{0.}</span></td>
</tr>
<tr class="even">
<td><strong>1</strong></td>
<td><span class="math inline">n_{10}</span></td>
<td><span class="math inline">n_{11}</span></td>
<td><span class="math inline">n_{1.}</span></td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td><span class="math inline">n_{.0}</span></td>
<td><span class="math inline">n_{.1}</span></td>
<td><span class="math inline">n</span></td>
</tr>
</tbody>
</table>
</section>
<section id="prediction-and-model-assessment-ii" class="level2">
<h2 class="anchored" data-anchor-id="prediction-and-model-assessment-ii">Prediction and model assessment II</h2>
<ul>
<li>In the <code>heart</code> dataset, using <span class="math inline">c = 0.5</span>, we get the following <span class="blue">confusion matrix</span>:</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">Actual 0</th>
<th style="text-align: right;">Actual 1</th>
<th style="text-align: right;">Actual total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Predicted 0</td>
<td style="text-align: right;">255</td>
<td style="text-align: right;">78</td>
<td style="text-align: right;">333</td>
</tr>
<tr class="even">
<td style="text-align: left;">Predicted 1</td>
<td style="text-align: right;">47</td>
<td style="text-align: right;">82</td>
<td style="text-align: right;">129</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Predicted total</td>
<td style="text-align: right;">302</td>
<td style="text-align: right;">160</td>
<td style="text-align: right;">462</td>
</tr>
</tbody>
</table>
</div>
</div>
<ul>
<li>Hence, the <span class="orange">overall accuracy</span> is <span class="math inline">(255 + 82) / 462 \approx 0.72</span>.</li>
</ul>
<ul>
<li>The <span class="blue">true positive rate</span> (<span class="orange">specificity</span>) is defined as <span class="math display">
\text{specificity} = 1 - \mathbb{P}(``\text{false positive}") \approx \frac{n_{00}}{n_{00} + n_{10}} = \frac{255}{255 + 47} = 0.844.
</span></li>
<li>On the other hand, the <span class="blue">true negative rate</span> (<span class="orange">sensitivity</span>) is defined as <span class="math display">
\text{sensitivity} = 1 - \mathbb{P}(``\text{false negative}") \approx \frac{n_{11}}{n_{01} + n_{11}} = \frac{82}{72 + 82} = 0.532.
</span></li>
</ul>
</section>
<section id="the-roc-curve" class="level2">
<h2 class="anchored" data-anchor-id="the-roc-curve">The ROC curve</h2>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_A_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="1050"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="other-topics" class="level2">
<h2 class="anchored" data-anchor-id="other-topics">Other topics</h2>
<ul>
<li><p>When the number of responses labels <span class="math inline">K &gt; 2</span>, we cannot use any more logistic regression.</p></li>
<li><p>The natural extension is called <span class="orange">multinomial regression</span>, in which we model the probabilities <span class="math display">
\mathbb{P}(Y_i = k) = p_k(\bm{x}_i,\beta), \qquad k=1,\dots,K,
</span> for example, using <span class="orange">multinomial logit</span>. The ideas of GLMs can be easily borrowed.</p></li>
<li><p>An alternative and straightforward approach is <span class="blue">linear discriminant analysis</span>, which is based on Bayes theorem.</p></li>
<li><p>You can find these topics in the textbook A&amp;S (2011), although you have likely seen these models in the previous courses.</p></li>
</ul>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><span class="blue">Main references (M.Sc. level)</span>
<ul>
<li><strong>Chapters 2 and 5</strong> of Azzalini, A. and Scarpa, B. (2011), <a href="http://azzalini.stat.unipd.it/Book-DM/"><em>Data Analysis and Data Mining</em></a>, Oxford University Press.</li>
<li><strong>Chapters 3 and 4</strong> of Hastie, T., Tibshirani, R. and Friedman, J. (2009), <a href="https://hastie.su.domains/ElemStatLearn/"><em>The Elements of Statistical Learning</em></a>, Second Edition, Springer.</li>
</ul></li>
<li><span class="grey">Basic references (B.Sc. level)</span>
<ul>
<li><strong>Chapters 5 and 6</strong> of Azzalini, A. (2000), <a href="http://azzalini.stat.unipd.it/Book_inference/copertina2000.jpg"><em>Inferenza Statistica. Una presentazione basata sul concetto di verosimiglianza</em></a>, Springer.</li>
</ul></li>
<li><span class="orange">Specialized references</span>
<ul>
<li><strong>Chapters 1–3</strong> of Quarteroni, A., Sacco, R., and Saleri F. (2007). <em>Numerical mathematics</em>. Second Edition, Springer.</li>
<li><strong>Chapters 1–5</strong> of Golub, G.H., and Van Loan, C.F. (1983). <em>Matrix computations</em>. Hopkins University Press.</li>
</ul></li>
</ul>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<script src="un_A_files/libs/quarto-html/zenscroll-min.js"></script>
</body></html>