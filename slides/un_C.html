<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tommaso Rigon">

<title>Shrinkage and variable selection</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="un_C_files/libs/clipboard/clipboard.min.js"></script>
<script src="un_C_files/libs/quarto-html/quarto.js"></script>
<script src="un_C_files/libs/quarto-html/popper.min.js"></script>
<script src="un_C_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="un_C_files/libs/quarto-html/anchor.min.js"></script>
<link href="un_C_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="un_C_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="un_C_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="un_C_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="un_C_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<style>html{ scroll-behavior: smooth; }</style>

<script src="un_C_files/libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<link href="un_C_files/libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet">
<script src="un_C_files/libs/datatables-binding-0.28/datatables.js"></script>
<script src="un_C_files/libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="un_C_files/libs/dt-core-1.13.4/css/jquery.dataTables.min.css" rel="stylesheet">
<link href="un_C_files/libs/dt-core-1.13.4/css/jquery.dataTables.extra.css" rel="stylesheet">
<script src="un_C_files/libs/dt-core-1.13.4/js/jquery.dataTables.min.js"></script>
<link href="un_C_files/libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="un_C_files/libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#homepage" id="toc-homepage" class="nav-link active" data-scroll-target="#homepage">Homepage</a></li>
  <li><a href="#a-biostatistical-motivation" id="toc-a-biostatistical-motivation" class="nav-link" data-scroll-target="#a-biostatistical-motivation">A biostatistical motivation</a>
  <ul class="collapse">
  <li><a href="#the-prostate-dataset" id="toc-the-prostate-dataset" class="nav-link" data-scroll-target="#the-prostate-dataset">The <code>prostate</code> dataset</a></li>
  <li><a href="#a-glimpse-of-the-prostate-dataset" id="toc-a-glimpse-of-the-prostate-dataset" class="nav-link" data-scroll-target="#a-glimpse-of-the-prostate-dataset">A <code>glimpse</code> of the <code>prostate</code> dataset</a></li>
  <li><a href="#correlation-matrix-of-prostate" id="toc-correlation-matrix-of-prostate" class="nav-link" data-scroll-target="#correlation-matrix-of-prostate">Correlation matrix of <code>prostate</code></a></li>
  <li><a href="#the-regression-framework" id="toc-the-regression-framework" class="nav-link" data-scroll-target="#the-regression-framework">The regression framework</a></li>
  <li><a href="#the-variable-selection-problem" id="toc-the-variable-selection-problem" class="nav-link" data-scroll-target="#the-variable-selection-problem">The variable selection problem</a></li>
  <li><a href="#a-naïve-approach-abusing-p-values" id="toc-a-naïve-approach-abusing-p-values" class="nav-link" data-scroll-target="#a-naïve-approach-abusing-p-values">A naïve approach: (ab)using p-values</a></li>
  <li><a href="#the-predictive-culture" id="toc-the-predictive-culture" class="nav-link" data-scroll-target="#the-predictive-culture">The predictive culture</a></li>
  <li><a href="#overview-of-this-unit" id="toc-overview-of-this-unit" class="nav-link" data-scroll-target="#overview-of-this-unit">Overview of this unit</a></li>
  <li><a href="#overview-of-the-final-results" id="toc-overview-of-the-final-results" class="nav-link" data-scroll-target="#overview-of-the-final-results">Overview of the final results</a></li>
  </ul></li>
  <li><a href="#best-subset-selection" id="toc-best-subset-selection" class="nav-link" data-scroll-target="#best-subset-selection">Best subset selection</a>
  <ul class="collapse">
  <li><a href="#best-subset-selection-1" id="toc-best-subset-selection-1" class="nav-link" data-scroll-target="#best-subset-selection-1">Best subset selection</a></li>
  <li><a href="#step-1.-and-2.-of-best-subset-selection" id="toc-step-1.-and-2.-of-best-subset-selection" class="nav-link" data-scroll-target="#step-1.-and-2.-of-best-subset-selection">Step 1. and 2. of best subset selection</a></li>
  <li><a href="#the-best-models-mathcalm_1dots-mathcalm_p" id="toc-the-best-models-mathcalm_1dots-mathcalm_p" class="nav-link" data-scroll-target="#the-best-models-mathcalm_1dots-mathcalm_p">The “best” models <span class="math inline">\mathcal{M}_1,\dots, \mathcal{M}_p</span></a></li>
  <li><a href="#the-wrong-way-of-doing-cross-validation" id="toc-the-wrong-way-of-doing-cross-validation" class="nav-link" data-scroll-target="#the-wrong-way-of-doing-cross-validation">The wrong way of doing cross-validation</a></li>
  <li><a href="#step-3.-of-best-subset-selection-via-cross-validation" id="toc-step-3.-of-best-subset-selection-via-cross-validation" class="nav-link" data-scroll-target="#step-3.-of-best-subset-selection-via-cross-validation">Step 3. of best subset selection via cross-validation</a></li>
  <li><a href="#comments-and-computations" id="toc-comments-and-computations" class="nav-link" data-scroll-target="#comments-and-computations">Comments and computations</a></li>
  <li><a href="#forward-regression" id="toc-forward-regression" class="nav-link" data-scroll-target="#forward-regression">Forward regression</a></li>
  <li><a href="#backward-regression" id="toc-backward-regression" class="nav-link" data-scroll-target="#backward-regression">Backward regression</a></li>
  <li><a href="#forward-backward-and-best-subset" id="toc-forward-backward-and-best-subset" class="nav-link" data-scroll-target="#forward-backward-and-best-subset">Forward, backward and best subset</a></li>
  <li><a href="#pros-and-cons-of-subset-selection-strategies" id="toc-pros-and-cons-of-subset-selection-strategies" class="nav-link" data-scroll-target="#pros-and-cons-of-subset-selection-strategies">Pros and cons of subset selection strategies</a></li>
  </ul></li>
  <li><a href="#principal-components-regression" id="toc-principal-components-regression" class="nav-link" data-scroll-target="#principal-components-regression">Principal components regression</a>
  <ul class="collapse">
  <li><a href="#data-compression" id="toc-data-compression" class="nav-link" data-scroll-target="#data-compression">Data compression</a></li>
  <li><a href="#the-intercept-term" id="toc-the-intercept-term" class="nav-link" data-scroll-target="#the-intercept-term">The intercept term</a></li>
  <li><a href="#centering-the-predictors" id="toc-centering-the-predictors" class="nav-link" data-scroll-target="#centering-the-predictors">Centering the predictors</a></li>
  <li><a href="#centering-the-predictors-ii" id="toc-centering-the-predictors-ii" class="nav-link" data-scroll-target="#centering-the-predictors-ii">Centering the predictors II</a></li>
  <li><a href="#singular-value-decomposition-svd" id="toc-singular-value-decomposition-svd" class="nav-link" data-scroll-target="#singular-value-decomposition-svd">Singular value decomposition (SVD)</a></li>
  <li><a href="#principal-component-analysis-i" id="toc-principal-component-analysis-i" class="nav-link" data-scroll-target="#principal-component-analysis-i">Principal component analysis I</a></li>
  <li><a href="#principal-component-analysis-ii" id="toc-principal-component-analysis-ii" class="nav-link" data-scroll-target="#principal-component-analysis-ii">Principal component analysis II</a></li>
  <li><a href="#principal-component-analysis-prostate-data" id="toc-principal-component-analysis-prostate-data" class="nav-link" data-scroll-target="#principal-component-analysis-prostate-data">Principal component analysis: <code>prostate</code> data</a></li>
  <li><a href="#principal-components-regression-pcr" id="toc-principal-components-regression-pcr" class="nav-link" data-scroll-target="#principal-components-regression-pcr">Principal components regression (PCR)</a></li>
  <li><a href="#selection-of-k-cross-validation" id="toc-selection-of-k-cross-validation" class="nav-link" data-scroll-target="#selection-of-k-cross-validation">Selection of <span class="math inline">k</span>: cross-validation</a></li>
  <li><a href="#shrinkage-effect-of-principal-components-i" id="toc-shrinkage-effect-of-principal-components-i" class="nav-link" data-scroll-target="#shrinkage-effect-of-principal-components-i">Shrinkage effect of principal components I</a></li>
  <li><a href="#shrinkage-effect-of-principal-components-ii" id="toc-shrinkage-effect-of-principal-components-ii" class="nav-link" data-scroll-target="#shrinkage-effect-of-principal-components-ii">Shrinkage effect of principal components II</a></li>
  <li><a href="#shrinkage-effect-of-principal-components-iii" id="toc-shrinkage-effect-of-principal-components-iii" class="nav-link" data-scroll-target="#shrinkage-effect-of-principal-components-iii">Shrinkage effect of principal components III</a></li>
  <li><a href="#pros-and-cons-of-pcr" id="toc-pros-and-cons-of-pcr" class="nav-link" data-scroll-target="#pros-and-cons-of-pcr">Pros and cons of PCR</a></li>
  </ul></li>
  <li><a href="#ridge-regression" id="toc-ridge-regression" class="nav-link" data-scroll-target="#ridge-regression">Ridge regression</a>
  <ul class="collapse">
  <li><a href="#shrinkage-methods" id="toc-shrinkage-methods" class="nav-link" data-scroll-target="#shrinkage-methods">Shrinkage methods</a></li>
  <li><a href="#the-ridge-regularization-method" id="toc-the-ridge-regularization-method" class="nav-link" data-scroll-target="#the-ridge-regularization-method">The ridge regularization method</a></li>
  <li><a href="#centering-and-scaling-the-predictors-i" id="toc-centering-and-scaling-the-predictors-i" class="nav-link" data-scroll-target="#centering-and-scaling-the-predictors-i">Centering and scaling the predictors I</a></li>
  <li><a href="#centering-and-scaling-the-predictors-ii" id="toc-centering-and-scaling-the-predictors-ii" class="nav-link" data-scroll-target="#centering-and-scaling-the-predictors-ii">Centering and scaling the predictors II</a></li>
  <li><a href="#lagrange-multipliers-and-ridge-solution" id="toc-lagrange-multipliers-and-ridge-solution" class="nav-link" data-scroll-target="#lagrange-multipliers-and-ridge-solution">Lagrange multipliers and ridge solution</a></li>
  <li><a href="#the-geometry-of-the-ridge-solution" id="toc-the-geometry-of-the-ridge-solution" class="nav-link" data-scroll-target="#the-geometry-of-the-ridge-solution">The geometry of the ridge solution</a></li>
  <li><a href="#the-ridge-path" id="toc-the-ridge-path" class="nav-link" data-scroll-target="#the-ridge-path">The ridge path</a></li>
  <li><a href="#comments-on-the-ridge-path" id="toc-comments-on-the-ridge-path" class="nav-link" data-scroll-target="#comments-on-the-ridge-path">Comments on the ridge path</a></li>
  <li><a href="#shrinkage-effect-of-ridge-regression-i" id="toc-shrinkage-effect-of-ridge-regression-i" class="nav-link" data-scroll-target="#shrinkage-effect-of-ridge-regression-i">Shrinkage effect of ridge regression I</a></li>
  <li><a href="#shrinkage-effect-of-ridge-regression-ii" id="toc-shrinkage-effect-of-ridge-regression-ii" class="nav-link" data-scroll-target="#shrinkage-effect-of-ridge-regression-ii">Shrinkage effect of ridge regression II</a></li>
  <li><a href="#bias-variance-trade-off" id="toc-bias-variance-trade-off" class="nav-link" data-scroll-target="#bias-variance-trade-off">Bias-variance trade-off</a></li>
  <li><a href="#a-historical-perspective-i" id="toc-a-historical-perspective-i" class="nav-link" data-scroll-target="#a-historical-perspective-i">☠️ - A historical perspective I</a></li>
  <li><a href="#a-historical-perspective-ii" id="toc-a-historical-perspective-ii" class="nav-link" data-scroll-target="#a-historical-perspective-ii">☠️ - A historical perspective II</a></li>
  <li><a href="#a-historical-perspective-iii" id="toc-a-historical-perspective-iii" class="nav-link" data-scroll-target="#a-historical-perspective-iii">☠️ - A historical perspective III</a></li>
  <li><a href="#on-the-choice-of-lambda" id="toc-on-the-choice-of-lambda" class="nav-link" data-scroll-target="#on-the-choice-of-lambda">On the choice of <span class="math inline">\lambda</span></a></li>
  <li><a href="#effective-degrees-of-freedom-i" id="toc-effective-degrees-of-freedom-i" class="nav-link" data-scroll-target="#effective-degrees-of-freedom-i">Effective degrees of freedom I</a></li>
  <li><a href="#effective-degrees-of-freedom-ii" id="toc-effective-degrees-of-freedom-ii" class="nav-link" data-scroll-target="#effective-degrees-of-freedom-ii">Effective degrees of freedom II</a></li>
  <li><a href="#effective-degrees-of-freedom-iii" id="toc-effective-degrees-of-freedom-iii" class="nav-link" data-scroll-target="#effective-degrees-of-freedom-iii">Effective degrees of freedom III</a></li>
  <li><a href="#cross-validation-for-ridge-regression-i" id="toc-cross-validation-for-ridge-regression-i" class="nav-link" data-scroll-target="#cross-validation-for-ridge-regression-i">Cross-validation for ridge regression I</a></li>
  <li><a href="#cross-validation-for-ridge-regression-ii" id="toc-cross-validation-for-ridge-regression-ii" class="nav-link" data-scroll-target="#cross-validation-for-ridge-regression-ii">Cross-validation for ridge regression II</a></li>
  <li><a href="#the-ridge-estimate" id="toc-the-ridge-estimate" class="nav-link" data-scroll-target="#the-ridge-estimate">The ridge estimate</a></li>
  <li><a href="#further-properties-of-ridge-regression" id="toc-further-properties-of-ridge-regression" class="nav-link" data-scroll-target="#further-properties-of-ridge-regression">Further properties of ridge regression</a></li>
  <li><a href="#pros-and-cons-of-ridge-regression" id="toc-pros-and-cons-of-ridge-regression" class="nav-link" data-scroll-target="#pros-and-cons-of-ridge-regression">Pros and cons of ridge regression</a></li>
  </ul></li>
  <li><a href="#the-lasso" id="toc-the-lasso" class="nav-link" data-scroll-target="#the-lasso">The lasso</a>
  <ul class="collapse">
  <li><a href="#looking-for-sparsity" id="toc-looking-for-sparsity" class="nav-link" data-scroll-target="#looking-for-sparsity">Looking for sparsity</a></li>
  <li><a href="#the-least-absolute-selection-and-shrinkage-operator" id="toc-the-least-absolute-selection-and-shrinkage-operator" class="nav-link" data-scroll-target="#the-least-absolute-selection-and-shrinkage-operator">The <span class="orange">l</span>east <span class="orange">a</span>bsolute <span class="orange">s</span>election and <span class="orange">s</span>hrinkage <span class="orange">o</span>perator</a></li>
  <li><a href="#centering-and-scaling-the-predictors" id="toc-centering-and-scaling-the-predictors" class="nav-link" data-scroll-target="#centering-and-scaling-the-predictors">Centering and scaling the predictors</a></li>
  <li><a href="#lagrange-multipliers-and-lasso-solution" id="toc-lagrange-multipliers-and-lasso-solution" class="nav-link" data-scroll-target="#lagrange-multipliers-and-lasso-solution">Lagrange multipliers and lasso solution</a></li>
  <li><a href="#the-geometry-of-the-lasso-solution" id="toc-the-geometry-of-the-lasso-solution" class="nav-link" data-scroll-target="#the-geometry-of-the-lasso-solution">The geometry of the lasso solution</a></li>
  <li><a href="#lasso-with-a-single-predictor-i" id="toc-lasso-with-a-single-predictor-i" class="nav-link" data-scroll-target="#lasso-with-a-single-predictor-i">Lasso with a single predictor I</a></li>
  <li><a href="#lasso-with-a-single-predictor-ii" id="toc-lasso-with-a-single-predictor-ii" class="nav-link" data-scroll-target="#lasso-with-a-single-predictor-ii">Lasso with a single predictor II</a></li>
  <li><a href="#soft-thresholding-and-lasso-solution" id="toc-soft-thresholding-and-lasso-solution" class="nav-link" data-scroll-target="#soft-thresholding-and-lasso-solution">Soft-thresholding and lasso solution</a></li>
  <li><a href="#the-lasso-path" id="toc-the-lasso-path" class="nav-link" data-scroll-target="#the-lasso-path">The lasso path</a></li>
  <li><a href="#least-angle-regression-i" id="toc-least-angle-regression-i" class="nav-link" data-scroll-target="#least-angle-regression-i">Least angle regression I</a></li>
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section"></a></li>
  <li><a href="#section-1" id="toc-section-1" class="nav-link" data-scroll-target="#section-1"></a></li>
  <li><a href="#least-angle-regression-remarks" id="toc-least-angle-regression-remarks" class="nav-link" data-scroll-target="#least-angle-regression-remarks">Least angle regression: remarks</a></li>
  <li><a href="#lasso-and-lar-relationship" id="toc-lasso-and-lar-relationship" class="nav-link" data-scroll-target="#lasso-and-lar-relationship">☠️ - Lasso and LAR relationship</a></li>
  <li><a href="#uniqueness-of-the-lasso-solution" id="toc-uniqueness-of-the-lasso-solution" class="nav-link" data-scroll-target="#uniqueness-of-the-lasso-solution">Uniqueness of the lasso solution</a></li>
  <li><a href="#the-degrees-of-freedom-of-the-lasso" id="toc-the-degrees-of-freedom-of-the-lasso" class="nav-link" data-scroll-target="#the-degrees-of-freedom-of-the-lasso">The degrees of freedom of the lasso</a></li>
  <li><a href="#effective-degrees-of-freedom-of-lar-and-best-subset" id="toc-effective-degrees-of-freedom-of-lar-and-best-subset" class="nav-link" data-scroll-target="#effective-degrees-of-freedom-of-lar-and-best-subset">☠️ - Effective degrees of freedom of LAR and best subset</a></li>
  <li><a href="#cross-validation-for-lasso" id="toc-cross-validation-for-lasso" class="nav-link" data-scroll-target="#cross-validation-for-lasso">Cross-validation for lasso</a></li>
  <li><a href="#the-lar-lasso-estimate" id="toc-the-lar-lasso-estimate" class="nav-link" data-scroll-target="#the-lar-lasso-estimate">The LAR (lasso) estimate</a></li>
  <li><a href="#other-properties-of-lar-and-lasso" id="toc-other-properties-of-lar-and-lasso" class="nav-link" data-scroll-target="#other-properties-of-lar-and-lasso">Other properties of LAR and lasso</a></li>
  <li><a href="#summary-of-lars-and-lasso" id="toc-summary-of-lars-and-lasso" class="nav-link" data-scroll-target="#summary-of-lars-and-lasso">Summary of LARS and lasso</a></li>
  <li><a href="#the-prostate-dataset.-a-summary-of-the-estimates" id="toc-the-prostate-dataset.-a-summary-of-the-estimates" class="nav-link" data-scroll-target="#the-prostate-dataset.-a-summary-of-the-estimates">The <code>prostate</code> dataset. A summary of the estimates</a></li>
  <li><a href="#the-results-on-the-test-set" id="toc-the-results-on-the-test-set" class="nav-link" data-scroll-target="#the-results-on-the-test-set">The results on the test set</a></li>
  </ul></li>
  <li><a href="#elastic-net-and-pathwise-algorithms" id="toc-elastic-net-and-pathwise-algorithms" class="nav-link" data-scroll-target="#elastic-net-and-pathwise-algorithms">Elastic-net and pathwise algorithms</a>
  <ul class="collapse">
  <li><a href="#elastic-net" id="toc-elastic-net" class="nav-link" data-scroll-target="#elastic-net">Elastic-net</a></li>
  <li><a href="#convex-optimization" id="toc-convex-optimization" class="nav-link" data-scroll-target="#convex-optimization">Convex optimization</a></li>
  <li><a href="#elastic-net-with-a-single-predictor" id="toc-elastic-net-with-a-single-predictor" class="nav-link" data-scroll-target="#elastic-net-with-a-single-predictor">Elastic-net with a single predictor</a></li>
  <li><a href="#coordinate-descent" id="toc-coordinate-descent" class="nav-link" data-scroll-target="#coordinate-descent">Coordinate descent</a></li>
  <li><a href="#coordinate-descent---example" id="toc-coordinate-descent---example" class="nav-link" data-scroll-target="#coordinate-descent---example">Coordinate descent - Example</a></li>
  <li><a href="#pathwise-coordinate-optimization" id="toc-pathwise-coordinate-optimization" class="nav-link" data-scroll-target="#pathwise-coordinate-optimization">Pathwise coordinate optimization</a></li>
  </ul></li>
  <li><a href="#generalized-linear-models" id="toc-generalized-linear-models" class="nav-link" data-scroll-target="#generalized-linear-models">Generalized linear models</a>
  <ul class="collapse">
  <li><a href="#generalized-linear-models-1" id="toc-generalized-linear-models-1" class="nav-link" data-scroll-target="#generalized-linear-models-1">Generalized linear models</a></li>
  <li><a href="#shrinkage-methods-for-glms" id="toc-shrinkage-methods-for-glms" class="nav-link" data-scroll-target="#shrinkage-methods-for-glms">Shrinkage methods for GLMs</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a>
  <ul class="collapse">
  <li><a href="#references-i" id="toc-references-i" class="nav-link" data-scroll-target="#references-i">References I</a></li>
  <li><a href="#references-ii" id="toc-references-ii" class="nav-link" data-scroll-target="#references-ii">References II</a></li>
  <li><a href="#references-iii" id="toc-references-iii" class="nav-link" data-scroll-target="#references-iii">References III</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="un_C_slides.html"><i class="bi bi-file-slides"></i>RevealJS</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Shrinkage and variable selection</h1>
<p class="subtitle lead">Data Mining - CdL CLAMSES</p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><span class="orange">Tommaso Rigon</span> </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <em>Università degli Studi di Milano-Bicocca</em>
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  

</header>

<section id="homepage" class="level2">
<h2 class="anchored" data-anchor-id="homepage"><a href="../index.html">Homepage</a></h2>
<div class="columns">
<div class="column" style="width:30%;">
<p><img src="img/cowboy.jpg" class="img-fluid"></p>
</div><div class="column" style="width:70%;">
<ul>
<li><p>In this unit we will cover the following <span class="orange">topics</span>:</p>
<ul>
<li>Best subset regression</li>
<li>Principal component regression</li>
<li>Ridge regression</li>
<li>Lasso, LARS, elastic-net</li>
</ul></li>
<li><p>The common themes are called <span class="blue">variable selection</span> and <span class="orange">shrinkage estimation</span>.</p></li>
<li><p>The issue we face is the presence of a high number <span class="math inline">p</span> of covariates that are <span class="blue">potentially irrelevant</span>.</p></li>
<li><p>This problem is quite challenging when the <span class="blue">ratio</span> <span class="math inline">p / n</span> is <span class="blue">large</span>.</p></li>
<li><p>In the <span class="orange">extreme case</span> <span class="math inline">p &gt; n</span>, is there any hope to fit a meaningful model?</p></li>
</ul>
</div>
</div>
</section>
<section id="a-biostatistical-motivation" class="level1">
<h1>A biostatistical motivation</h1>
<section id="the-prostate-dataset" class="level2">
<h2 class="anchored" data-anchor-id="the-prostate-dataset">The <code>prostate</code> dataset</h2>
<ul>
<li>The <code>prostate</code> cancer data investigates the relationship between the prostate-specific <span class="orange">antigen</span> and a number of clinical measures, in men about to receive a prostatectomy.</li>
</ul>
<ul>
<li>This <a href="https://hastie.su.domains/ElemStatLearn/datasets/prostate.data">dataset</a> has been used in the <span class="orange">original paper</span> by Tibshirani (1996) to present the lasso. A description is given in <span class="blue">Section 3.2.1</span> of HTF (2009).</li>
</ul>
<ul>
<li><p>We want to <span class="blue">predict</span> the logarithm of a <span class="orange">prostate-specific antigen</span> (<code>lpsa</code>) as a function of:</p>
<ul>
<li>logarithm of the cancer volume (<code>lcavol</code>);</li>
<li>logarithm of the prostate weight (<code>lweight</code>);</li>
<li>age each man (<code>age</code>);</li>
<li>logarithm of the benign prostatic hyperplasia amount (<code>lbph</code>);</li>
<li>seminal vesicle invasion (<code>svi</code>), a binary variable;</li>
<li>logarithm of the capsular penetration (<code>lcp</code>);</li>
<li>Gleason score (<code>gleason</code>), an ordered categorical variable;</li>
<li>Percentage of Gleason scores <span class="math inline">4</span> and <span class="math inline">5</span> (<code>pgg45</code>).</li>
</ul></li>
</ul>
</section>
<section id="a-glimpse-of-the-prostate-dataset" class="level2">
<h2 class="anchored" data-anchor-id="a-glimpse-of-the-prostate-dataset">A <code>glimpse</code> of the <code>prostate</code> dataset</h2>
<ul>
<li><p>Summarizing, there are in total <span class="math inline">8</span> <span class="orange">variables</span> that can be used to predict the antigen <code>lpsa</code>.</p></li>
<li><p>We <span class="orange">centered</span> and <span class="blue">standardized</span> all the covariates before the training/test split.</p></li>
<li><p>There are <span class="math inline">n = 67</span> observations in the <span class="orange">training</span> set and <span class="math inline">30</span> in the <span class="blue">test</span> set.</p></li>
</ul>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" href="">Original dataset</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false" href="">Standardized dataset</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="cell" data-hash="un_C_cache/html/unnamed-chunk-2_3059dfd7b8a3faf9f891bb978764f04d">
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 97
Columns: 10
$ lcavol  &lt;dbl&gt; -0.5798185, -0.9942523, -0.5108256, -1.2039728, 0.7514161, -1.…
$ lweight &lt;dbl&gt; 2.769459, 3.319626, 2.691243, 3.282789, 3.432373, 3.228826, 3.…
$ age     &lt;int&gt; 50, 58, 74, 58, 62, 50, 64, 58, 47, 63, 65, 63, 63, 67, 57, 66…
$ lbph    &lt;dbl&gt; -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1…
$ svi     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
$ lcp     &lt;dbl&gt; -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1…
$ gleason &lt;int&gt; 6, 6, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 6, 7, 6, 6, 6, 6,…
$ pgg45   &lt;int&gt; 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 30, 5, 5, 0, 30, 0, 0, 0,…
$ lpsa    &lt;dbl&gt; -0.4307829, -0.1625189, -0.1625189, -0.1625189, 0.3715636, 0.7…
$ train   &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE,…</code></pre>
</div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="cell" data-hash="un_C_cache/html/unnamed-chunk-3_6326223f62ed29de0622fc34f7a810f2">
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 97
Columns: 10
$ lcavol  &lt;dbl&gt; -1.63735563, -1.98898046, -1.57881888, -2.16691708, -0.5078744…
$ lweight &lt;dbl&gt; -2.00621178, -0.72200876, -2.18878403, -0.80799390, -0.4588340…
$ age     &lt;dbl&gt; -1.86242597, -0.78789619, 1.36116337, -0.78789619, -0.25063130…
$ lbph    &lt;dbl&gt; -1.0247058, -1.0247058, -1.0247058, -1.0247058, -1.0247058, -1…
$ svi     &lt;dbl&gt; -0.5229409, -0.5229409, -0.5229409, -0.5229409, -0.5229409, -0…
$ lcp     &lt;dbl&gt; -0.8631712, -0.8631712, -0.8631712, -0.8631712, -0.8631712, -0…
$ gleason &lt;dbl&gt; -1.0421573, -1.0421573, 0.3426271, -1.0421573, -1.0421573, -1.…
$ pgg45   &lt;dbl&gt; -0.8644665, -0.8644665, -0.1553481, -0.8644665, -0.8644665, -0…
$ lpsa    &lt;dbl&gt; -0.4307829, -0.1625189, -0.1625189, -0.1625189, 0.3715636, 0.7…
$ train   &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE,…</code></pre>
</div>
</div>
</div>
</div>
</div>
<!-- -   The variable `train` splits the data into a training and test set, -->
<!--     as in the textbook. -->
</section>
<section id="correlation-matrix-of-prostate" class="level2">
<h2 class="anchored" data-anchor-id="correlation-matrix-of-prostate">Correlation matrix of <code>prostate</code></h2>
<div class="cell" data-layout-align="center" data-hash="un_C_cache/html/unnamed-chunk-4_afadf368c60f0be4fc80de117f79030a">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_C_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="3000"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="the-regression-framework" class="level2">
<h2 class="anchored" data-anchor-id="the-regression-framework">The regression framework</h2>
<div class="incremental">
<ul class="incremental">
<li><p>In this unit we will assume that the response variables <span class="math inline">Y_i</span> (<code>lpsa</code>) are obtained as <span class="math display">
Y_i = f(\bm{x}_i) + \epsilon_i, \qquad
</span> where <span class="math inline">\epsilon_i</span> are <span class="orange">iid</span> random variables with <span class="math inline">\mathbb{E}(\epsilon_i) = 0</span> and <span class="math inline">\text{var}(\epsilon_i) = \sigma^2</span>.</p></li>
<li><p>Unless specifically stated, we will <span class="orange">not</span> assume the <span class="orange">Gaussianity</span> of the errors <span class="math inline">\epsilon_i</span> nor make any specific assumption about <span class="math inline">f(\bm{x})</span>, which could be <span class="blue">non-linear</span>.</p></li>
<li><p>In practice, we <span class="orange">approximate</span> the true <span class="math inline">f(\bm{x})</span> using a <span class="blue">linear model</span>, e.g.&nbsp;by considering the following function<span class="math display">
    f(\bm{x}_i; \beta_0, \beta) = \beta_0+ \beta_1 x_{i1} + \cdots + \beta_p x_{ip} =\beta_0 + \bm{x}_i^T\beta,
    </span> in which the regression coefficients must be estimated.</p></li>
<li><p>In this unit the <span class="blue">intercept</span> <span class="math inline">\beta_0</span> will often play a special role, therefore we use a slightly different notation compared to <a href="unit_A.html">Unit A</a>.</p></li>
</ul>
</div>
</section>
<section id="the-variable-selection-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-variable-selection-problem">The variable selection problem</h2>
<ul>
<li>Including a lot of covariates into the model is not necessarily a good thing!</li>
</ul>
<ul>
<li><p>Indeed, some variables are likely to be <span class="blue">irrelevant</span>:</p>
<ul>
<li>they might be <span class="orange">correlated</span> with other covariates and therefore <span class="orange">redundant</span>;</li>
<li>they could be uncorrelated with the response <code>lpsa</code>.</li>
</ul></li>
</ul>
<ul>
<li><p>If we use all the <span class="math inline">p = 8</span> available covariates, the estimated <span class="math inline">f(\bm{x}; \hat{\beta_0}, \hat{\beta})</span> might have a <span class="orange">high variance</span>, without important gain in term of bias, i.e.&nbsp;a <span class="blue">large mean squared error</span>.</p></li>
<li><p>We are looking for a <span class="orange">simpler model</span> having, hopefully, a lower mean squared error.</p></li>
</ul>
<ul>
<li>These considerations are particularly relevant in cases in which <span class="math inline">p &gt; n</span>!</li>
</ul>
</section>
<section id="a-naïve-approach-abusing-p-values" class="level2">
<h2 class="anchored" data-anchor-id="a-naïve-approach-abusing-p-values">A naïve approach: (ab)using p-values</h2>
<div style="font-size: 75%;">
<div class="cell" data-hash="un_C_cache/html/unnamed-chunk-5_d8596f2d7bd7cb3a9ff23042addefb64">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<colgroup>
<col style="width: 13%">
<col style="width: 16%">
<col style="width: 9%">
<col style="width: 10%">
<col style="width: 8%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 8%">
<col style="width: 10%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">(Intercept)</th>
<th style="text-align: right;">lcavol</th>
<th style="text-align: right;">lweight</th>
<th style="text-align: right;">age</th>
<th style="text-align: right;">lbph</th>
<th style="text-align: right;">svi</th>
<th style="text-align: right;">lcp</th>
<th style="text-align: right;">gleason</th>
<th style="text-align: right;">pgg45</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">estimate</td>
<td style="text-align: right;">2.46</td>
<td style="text-align: right;">0.68</td>
<td style="text-align: right;">0.26</td>
<td style="text-align: right;">-0.14</td>
<td style="text-align: right;">0.21</td>
<td style="text-align: right;">0.31</td>
<td style="text-align: right;">-0.29</td>
<td style="text-align: right;">-0.02</td>
<td style="text-align: right;">0.27</td>
</tr>
<tr class="even">
<td style="text-align: left;">std.error</td>
<td style="text-align: right;">0.09</td>
<td style="text-align: right;">0.13</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.12</td>
<td style="text-align: right;">0.15</td>
<td style="text-align: right;">0.15</td>
<td style="text-align: right;">0.15</td>
</tr>
<tr class="odd">
<td style="text-align: left;">statistic</td>
<td style="text-align: right;">27.60</td>
<td style="text-align: right;">5.37</td>
<td style="text-align: right;">2.75</td>
<td style="text-align: right;">-1.40</td>
<td style="text-align: right;">2.06</td>
<td style="text-align: right;">2.47</td>
<td style="text-align: right;">-1.87</td>
<td style="text-align: right;">-0.15</td>
<td style="text-align: right;">1.74</td>
</tr>
<tr class="even">
<td style="text-align: left;">p.value</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">0.17</td>
<td style="text-align: right;">0.04</td>
<td style="text-align: right;">0.02</td>
<td style="text-align: right;">0.07</td>
<td style="text-align: right;">0.88</td>
<td style="text-align: right;">0.09</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<ul>
<li><p>It is common practice to use the <span class="orange">p-values</span> to perform <span class="blue">model selection</span> in a stepwise fashion.</p></li>
<li><p>However, what if the true <span class="math inline">f(\bm{x})</span> were not linear?</p></li>
<li><p>In many data mining problems, a <span class="blue">linear model</span> is simply an approximation of the unknown <span class="math inline">f(\bm{x})</span> and hypothesis testing procedures are ill-posed.</p></li>
</ul>
<ul>
<li><p>Even if the true function were linear, using p-values would <span class="orange">not be a good idea</span>, at least if done without appropriate <span class="blue">multiplicity corrections</span>.</p></li>
<li><p>The above p-values are meant to be used in the context of a single hypothesis testing problem, <span class="orange">not</span> to make <span class="orange">iterative choices</span>.</p></li>
</ul>
</section>
<section id="the-predictive-culture" class="level2">
<h2 class="anchored" data-anchor-id="the-predictive-culture">The predictive culture</h2>
<div class="columns">
<div class="column" style="width:25%;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="img/box.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">George E. P. Box</figcaption>
</figure>
</div>
</div><div class="column" style="width:75%;">
<ul>
<li><p>“<em>All models are approximations. Essentially, all models are wrong, but some are useful</em>.”</p>
<p><span class="grey">George E. P. Box</span></p></li>
<li><p>If the <span class="blue">focus</span> is on <span class="blue">prediction</span>, we do not necessarily care about selecting the “true” set of parameters.</p></li>
<li><p>In many data mining problems, the focus is on <span class="orange">minimizing</span> the <span class="orange">prediction errors</span>.</p></li>
<li><p>Hence, often times we may <span class="blue">accept some bias</span> (i.e.&nbsp;we use a “wrong” but useful model), if this leads to a <span class="orange">reduction in variance</span>.</p></li>
</ul>
</div>
</div>
</section>
<section id="overview-of-this-unit" class="level2">
<h2 class="anchored" data-anchor-id="overview-of-this-unit">Overview of this unit</h2>
<ul>
<li><p>In this unit we will discuss two “discrete” methods:</p>
<ul>
<li>Best subset selection and its greedy approximations: forward / backward regression;</li>
<li>Principal components regression (PCR).</li>
</ul></li>
<li><p>Best subset selection perform <span class="orange">variable selection</span>, whereas principal components regression <span class="orange">reduces the variance</span> of the coefficients.</p></li>
<li><p>These “discrete” methods can be seen as the naïve counterpart of more advanced and <span class="blue">continuous</span> ideas, that are presented in the second part of the Unit.</p></li>
</ul>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 43%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><span class="orange">Shrinkage</span></th>
<th><span class="orange">Variable selection</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="blue">Discrete</span></td>
<td>Principal component regression</td>
<td>Best subset selection, stepwise</td>
</tr>
<tr class="even">
<td><span class="blue">Continuous</span></td>
<td>Ridge regression</td>
<td>Relaxed Lasso</td>
</tr>
</tbody>
</table>
<ul>
<li>Finally, the <span class="blue">lasso</span> and the <span class="orange">elastic-net</span> perform both shrinkage and variable selection.</li>
</ul>
</section>
<section id="overview-of-the-final-results" class="level2">
<h2 class="anchored" data-anchor-id="overview-of-the-final-results">Overview of the final results</h2>
<table class="table">
<colgroup>
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 19%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">Least squares</th>
<th style="text-align: right;">Best subset</th>
<th style="text-align: right;">PCR</th>
<th style="text-align: right;">Ridge</th>
<th style="text-align: right;">Lasso</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>(Intercept)</code></td>
<td style="text-align: right;">2.465</td>
<td style="text-align: right;">2.477</td>
<td style="text-align: right;">2.455</td>
<td style="text-align: right;">2.467</td>
<td style="text-align: right;">2.468</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>lcavol</code></td>
<td style="text-align: right;">0.680</td>
<td style="text-align: right;">0.740</td>
<td style="text-align: right;">0.287</td>
<td style="text-align: right;">0.588</td>
<td style="text-align: right;">0.532</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>lweight</code></td>
<td style="text-align: right;">0.263</td>
<td style="text-align: right;">0.316</td>
<td style="text-align: right;">0.339</td>
<td style="text-align: right;">0.258</td>
<td style="text-align: right;">0.169</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>age</code></td>
<td style="text-align: right;">-0.141</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">0.056</td>
<td style="text-align: right;">-0.113</td>
<td style="text-align: right;">.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>lbph</code></td>
<td style="text-align: right;">0.210</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">0.102</td>
<td style="text-align: right;">0.201</td>
<td style="text-align: right;">.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>svi</code></td>
<td style="text-align: right;">0.305</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">0.261</td>
<td style="text-align: right;">0.283</td>
<td style="text-align: right;">0.092</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>lcp</code></td>
<td style="text-align: right;">-0.288</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">0.219</td>
<td style="text-align: right;">-0.172</td>
<td style="text-align: right;">.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>gleason</code></td>
<td style="text-align: right;">-0.021</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">-0.016</td>
<td style="text-align: right;">0.010</td>
<td style="text-align: right;">.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>pgg45</code></td>
<td style="text-align: right;">0.267</td>
<td style="text-align: right;">.</td>
<td style="text-align: right;">0.062</td>
<td style="text-align: right;">0.204</td>
<td style="text-align: right;">.</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="best-subset-selection" class="level1">
<h1>Best subset selection</h1>
<section id="best-subset-selection-1" class="level2">
<h2 class="anchored" data-anchor-id="best-subset-selection-1">Best subset selection</h2>
<ul>
<li><p>Let us get back to our <span class="blue">variable selection problem</span>.</p></li>
<li><p>In principle, we could perform an <span class="orange">exhaustive search</span> considering all the <span class="math inline">2^p</span> possible models and then selecting the one having the best out-of-sample predictive performance.</p></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Best subset procedure
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>Let <span class="math inline">\mathcal{M}_0</span> be the <span class="blue">null model</span>, which contains no predictors, i.e.&nbsp;set <span class="math inline">\hat{y}_i = \hat{\beta}_0 = \bar{y}</span>.</p></li>
<li><p>For <span class="math inline">k =1,\dots,p</span>, do:</p>
<ol type="i">
<li><p>Estimate <span class="orange">all</span> the <span class="math inline">\binom{p}{k}</span> models that contain exactly <span class="math inline">k</span> covariates;</p></li>
<li><p>Identify the “best” model with <span class="math inline">k</span> covariates having the smallest <span class="math inline">\text{MSE}_{k, \text{train}}</span>; call it <span class="math inline">\mathcal{M}_k</span>.</p></li>
</ol></li>
</ol>
</div>
</div>
<ul>
<li>A model with more variables has lower <span class="orange">training</span> error, namely <span class="math inline">\text{MSE}_{k + 1, \text{train}} \le \text{MSE}_{k, \text{train}}</span> by construction. Hence, the optimal subset size <span class="math inline">k</span> must be chosen e.g.&nbsp;via <span class="blue">cross-validation</span>.</li>
</ul>
</section>
<section id="step-1.-and-2.-of-best-subset-selection" class="level2">
<h2 class="anchored" data-anchor-id="step-1.-and-2.-of-best-subset-selection">Step 1. and 2. of best subset selection</h2>
<div class="cell" data-layout-align="center" data-hash="un_C_cache/html/unnamed-chunk-8_7c607631f69b486f8cd553cac97c1d57">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_C_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="2000"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="the-best-models-mathcalm_1dots-mathcalm_p" class="level2">
<h2 class="anchored" data-anchor-id="the-best-models-mathcalm_1dots-mathcalm_p">The “best” models <span class="math inline">\mathcal{M}_1,\dots, \mathcal{M}_p</span></h2>
<ul>
<li>The output of the <span class="orange">best subset selection</span>, on the training set is:</li>
</ul>
<div class="cell" data-hash="un_C_cache/html/unnamed-chunk-9_af388b1179312cdb7a159fce35e96e5c">
<div class="cell-output cell-output-stdout">
<pre><code>         lcavol lweight age lbph svi lcp gleason pgg45
1  ( 1 ) "*"    " "     " " " "  " " " " " "     " "  
2  ( 1 ) "*"    "*"     " " " "  " " " " " "     " "  
3  ( 1 ) "*"    "*"     " " " "  "*" " " " "     " "  
4  ( 1 ) "*"    "*"     " " "*"  "*" " " " "     " "  
5  ( 1 ) "*"    "*"     " " "*"  "*" " " " "     "*"  
6  ( 1 ) "*"    "*"     " " "*"  "*" "*" " "     "*"  
7  ( 1 ) "*"    "*"     "*" "*"  "*" "*" " "     "*"  
8  ( 1 ) "*"    "*"     "*" "*"  "*" "*" "*"     "*"  </code></pre>
</div>
</div>
<div class="incremental">
<ul class="incremental">
<li><p>The above table means that the best model with <span class="math inline">k = 1</span> uses the variable <code>lcavol</code>, whereas when <span class="math inline">k = 2</span> the selected variables are <code>lcavol</code> and <code>lweight</code>, and so on and so forth.</p></li>
<li><p>Note that, in general, these models are <span class="orange">not</span> necessarily <span class="orange">nested</span>, i.e.&nbsp;a variable selected at step <span class="math inline">k</span> is not necessarily included at step <span class="math inline">k +1</span>. Here they are, but it is a coincidence.</p></li>
</ul>
</div>
<ul>
<li>What is the <span class="orange">optimal subset size</span> <span class="math inline">k</span> in terms of out-of-sample mean squared error?</li>
</ul>
</section>
<section id="the-wrong-way-of-doing-cross-validation" class="level2">
<h2 class="anchored" data-anchor-id="the-wrong-way-of-doing-cross-validation">The wrong way of doing cross-validation</h2>
<div class="incremental">
<ul class="incremental">
<li><p>Consider a regression problem with a <span class="orange">large number of predictors</span> (relative to <span class="math inline">n</span>) such as the <code>prostate</code> dataset.</p></li>
<li><p>A typical strategy for analysis might be as follows:</p>
<ol class="incremental" type="1">
<li><p>Screen the predictors: find a subset of “good” predictors that show fairly strong correlation with the response;</p></li>
<li><p>Using this subset of predictors (e.g.&nbsp;<code>lcavol</code>, <code>lweight</code> and <code>svi</code>), build a regression model;</p></li>
<li><p>Use cross-validation to estimate the prediction error of the model of the step 2.</p></li>
</ol></li>
<li><p>Is this a correct application of cross-validation?</p></li>
<li><p>If your reaction was “<span class="orange">this is absolutely wrong!</span>”, it means you correctly understood the principles of cross-validation.</p></li>
<li><p>If you though this was an ok-ish idea, you may want to read <span class="blue">Section 7.10.2</span> of HTF (2009), called “the wrong way of doing cross-validation”.</p></li>
</ul>
</div>
</section>
<section id="step-3.-of-best-subset-selection-via-cross-validation" class="level2">
<h2 class="anchored" data-anchor-id="step-3.-of-best-subset-selection-via-cross-validation">Step 3. of best subset selection via cross-validation</h2>
<div class="cell" data-layout-align="center" data-hash="un_C_cache/html/unnamed-chunk-11_22a13d5bf98536f225c2213fd83e61a2">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_C_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="2000"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>By applying the “1 standard error rule”, we select <span class="math inline">k = 2</span>, i.e. <code>lcavol</code> and <code>lweight</code>.</li>
</ul>
</section>
<section id="comments-and-computations" class="level2">
<h2 class="anchored" data-anchor-id="comments-and-computations">Comments and computations</h2>
<div class="incremental">
<ul class="incremental">
<li><p>The correct way of doing cross-validation requires that the <span class="blue">best subset selection</span> is performed on <span class="orange">every fold</span>, possibly obtaining different “best” models with the same size.</p></li>
<li><p>Best subset selection is conceptually appealing, but it has a <span class="orange">major limitation</span>. There are <span class="math display">
\sum_{k=1}^p \binom{n}{k} = 2^p
</span> models to consider, which is <span class="orange">computationally prohibitive</span>!</p></li>
<li><p>There exists algorithms (i.e.&nbsp;<span class="blue">leaps and bounds</span>) that makes this feasible for <span class="math inline">p \approx 30</span>.</p></li>
<li><p>Recently, <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-44/issue-2/Best-subset-selection-via-a-modern-optimization-lens/10.1214/15-AOS1388.full">Bertsimas et al., 2016</a> proposed the usage of a mixed integer optimization formulation, allowing <span class="math inline">p</span> to be in the order of hundreds.</p></li>
<li><p>Despite these advances, this problem remains <span class="orange">computationally very expensive</span>. See also the recent paper <a href="https://projecteuclid.org/journals/statistical-science/volume-35/issue-4/Best-Subset-Forward-Stepwise-or-Lasso-Analysis-and-Recommendations-Based/10.1214/19-STS733.full">Hastie et al. (2020)</a> for additional considerations and comparisons.</p></li>
</ul>
</div>
</section>
<section id="forward-regression" class="level2">
<h2 class="anchored" data-anchor-id="forward-regression">Forward regression</h2>
<ul>
<li>Forward regression is <span class="orange">greedy approximation</span> of best subset selection, that produces a sequence of <span class="blue">nested</span> models. It is computationally feasible and can be applied when <span class="math inline">p &gt; n</span>.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Forward regression
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>Let <span class="math inline">\mathcal{M}_0</span> be the <span class="blue">null model</span>, which contains no predictors, i.e.&nbsp;set <span class="math inline">\hat{y}_i = \hat{\beta}_0 = \bar{y}</span>.</p></li>
<li><p>For <span class="math inline">k = 0,\dots, \min(n - 1, p - 1)</span>, do:</p>
<ol type="i">
<li><p>Consider the <span class="math inline">p − k</span> models that augment the predictors in <span class="math inline">\mathcal{M}_k</span> with <span class="orange">one additional covariate</span>.</p></li>
<li><p>Identify the “best” model among the above <span class="math inline">p - k</span> competitors having the smallest <span class="math inline">\text{MSE}_{k, \text{train}}</span> and call it <span class="math inline">\mathcal{M}_k</span>.</p></li>
</ol></li>
</ol>
</div>
</div>
<ul>
<li>It can be shown that the identification of the <span class="blue">optimal new predictor</span> can be efficiently computed using the <span class="orange">QR decomposition</span> (see Exercises).</li>
</ul>
</section>
<section id="backward-regression" class="level2">
<h2 class="anchored" data-anchor-id="backward-regression">Backward regression</h2>
<ul>
<li>When <span class="math inline">p &lt; n</span>, an alternative greedy approach is <span class="orange">backward regression</span>, which also produces a sequence of <span class="blue">nested</span> models.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Backward regression
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>Let <span class="math inline">\mathcal{M}_p</span> be the <span class="blue">full model</span>, which contains all the predictors.</p></li>
<li><p>For <span class="math inline">k = p, p - 1,\dots, 1</span>, do:</p>
<ol type="i">
<li><p>Consider the <span class="math inline">k</span> models that contain <span class="orange">all but one</span> of the predictors in <span class="math inline">\mathcal{M}_k</span>, for a total of <span class="math inline">k − 1</span> predictors.</p></li>
<li><p>Identify the “best” model <span class="math inline">\mathcal{M}_k</span> among these <span class="math inline">k</span> models having the smallest <span class="math inline">\text{MSE}_{k, \text{train}}</span>.</p></li>
</ol></li>
</ol>
</div>
</div>
<ul>
<li>It can be shown that the <span class="blue">dropped predictor</span> is the one with the lowest absolute <span class="math inline">Z</span>-score or, equivalently, the <span class="orange">highest p-value</span> (see Exercises).</li>
</ul>
</section>
<section id="forward-backward-and-best-subset" class="level2">
<h2 class="anchored" data-anchor-id="forward-backward-and-best-subset">Forward, backward and best subset</h2>
<div class="cell" data-layout-align="center" data-hash="un_C_cache/html/unnamed-chunk-13_6d3a83970ad20cd715a64c1d759ef814">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_C_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="2000"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>In the <code>prostate</code> dataset, forward, backward and best subset selection all gave exactly the <span class="orange">same path of solutions</span> on the full training set.</li>
</ul>
</section>
<section id="pros-and-cons-of-subset-selection-strategies" class="level2">
<h2 class="anchored" data-anchor-id="pros-and-cons-of-subset-selection-strategies">Pros and cons of subset selection strategies</h2>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pros
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Best subset selection is appealing because of its <span class="blue">conceptual simplicity</span>.</p></li>
<li><p>Best subset and forward regression can be used, when computations are not problematic, even when <span class="math inline">p &gt; n</span>.</p></li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cons
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Subset strategies tend to select models that are “<span class="orange">too simple</span>”, especially in presence of correlated variables.</p></li>
<li><p>Despite the recent advances, when <span class="math inline">p</span> is large best subset selection is <span class="orange">computationally unfeasible</span>.</p></li>
<li><p>Leaps and bounds computational strategies can not be easily generalized to GLMs.</p></li>
</ul>
</div>
</div>
</section>
</section>
<section id="principal-components-regression" class="level1">
<h1>Principal components regression</h1>
<section id="data-compression" class="level2">
<h2 class="anchored" data-anchor-id="data-compression">Data compression</h2>
<div class="columns">
<div class="column" style="width:30%;">
<p><img src="img/cowboy_pixel.jpg" class="img-fluid"></p>
</div><div class="column" style="width:70%;">
<ul>
<li><p>At this point we established that <span class="orange">many covariates = many problems</span>.</p></li>
<li><p>Instead of selecting the “best” variables, let us consider a different perspective.</p></li>
<li><p>We consider a <span class="blue">compressed</span> version of the covariates that has smaller dimension <span class="math inline">k</span> but retains most information.</p></li>
<li><p>Intuitively, we want to <span class="orange">reduce the variance</span> by finding a good compression, without sacrificing too much bias.</p></li>
<li><p>The main statistical tool, unsurprisingly, will be the celebrated <span class="blue">principal components analysis</span> (PCA).</p></li>
<li><p>We will compress the covariate information <span class="math inline">\bm{X}</span> using a smaller set of variables <span class="math inline">\bm{Z}</span>, i.e.&nbsp;the principal components.</p></li>
</ul>
</div>
</div>
</section>
<section id="the-intercept-term" class="level2">
<h2 class="anchored" data-anchor-id="the-intercept-term">The intercept term</h2>
<div class="incremental">
<ul class="incremental">
<li><p>In principal component regression and in other related methods (ridge, lasso and elastic-net), we do <span class="orange">not</span> wish to <span class="orange">compress</span> the <span class="orange">intercept</span> term <span class="math inline">\beta_0</span>. We would like to “remove it”.</p></li>
<li><p>Let us consider a <span class="blue">reparametrization</span> of the linear model, in which <span class="math inline">\alpha = \beta_0 + \bar{\bm{x}}^T\beta</span>. This is equivalent to a linear model with <span class="orange">centered predictors</span>: <span class="math display">
\begin{aligned}
f(\bm{x}_i; \alpha, \beta) &amp; = \beta_0 + \bm{x}_i^T\beta = \alpha - \bar{\bm{x}}^T\beta + \bm{x}_i^T\beta  = \alpha + (\bm{x}_i -\bar{\bm{x}})^T\beta. \\
\end{aligned}
</span></p></li>
<li><p>The estimates for <span class="math inline">(\alpha, \beta)</span> can be now computed separately and <span class="orange">in two steps</span>.</p></li>
<li><p>The <span class="orange">estimate</span> of the <span class="orange">intercept</span> with centered predictors is <span class="math inline">\hat{\alpha} = \bar{y}</span>. In fact: <span class="math display">
\hat{\alpha} = \arg\min_{\alpha \in \mathbb{R}}\sum_{i=1}^n\{y_i - \alpha - (\bm{x}_i -\bar{\bm{x}})^T\beta\}^2 = \frac{1}{n}\sum_{i=1}^n\{y_i - (\bm{x}_i -\bar{\bm{x}})^T\beta\} = \frac{1}{n}\sum_{i=1}^ny_i.
</span></p></li>
<li><p>Then, the <span class="blue">estimate of <span class="math inline">\beta</span></span> can be obtained considering a linear model <span class="orange">without intercept</span>: <span class="math display">
    f(\bm{x}_i; \beta) = (\bm{x}_i -\bar{\bm{x}})^T\beta,
</span> employed to predict the <span class="blue">centered responses</span> <span class="math inline">y_i - \bar{y}</span>.</p></li>
</ul>
</div>
</section>
<section id="centering-the-predictors" class="level2">
<h2 class="anchored" data-anchor-id="centering-the-predictors">Centering the predictors</h2>
<ul>
<li>In principal components regression, we replace <span class="orange">original data</span> <span class="math inline">Y_i = f(\bm{x}_i) + \epsilon_i</span> with their <span class="blue">centered</span> version: <span class="math display">
x_{ij} - \bar{x}_j, \qquad y_i - \bar{y}, \qquad i=1,\dots,n; \ \ j=1,\dots,p.
</span></li>
</ul>
<ul>
<li>In the end, we will make predictions in the <span class="blue">original scale</span>, which requires a simple <span class="orange">final adjustment</span>. One simply need to compute the intercept term <span class="math display">
\hat{\beta}_0 = \bar{y} - \bar{\bm{x}}\hat{\beta},
</span> and then compute the predictions via the formula <span class="math inline">\hat{\beta}_0 + \bm{x}_i^T\hat{\beta} = \hat{\alpha} + \bm{x}_{i}^T\hat{\beta}</span>.</li>
</ul>
<ul>
<li><span class="orange">Remark</span>. The centering operation is a mathematical trick that facilitate the exposition, but is unconsequential from an estimation point of view.</li>
</ul>
</section>
<section id="centering-the-predictors-ii" class="level2">
<h2 class="anchored" data-anchor-id="centering-the-predictors-ii">Centering the predictors II</h2>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Centering assumption
</div>
</div>
<div class="callout-body-container callout-body">
<p>In principal components regression, we assume the data have been previously <span class="orange">centered</span>: <span class="math display">
    \frac{1}{n}\sum_{i=1}^n y_{i} = 0, \qquad \frac{1}{n}\sum_{i=1}^nx_{ij} = 0, \qquad j=1,\dots,p.
    </span></p>
</div>
</div>
<ul>
<li>Using centered predictor means that we can focus on linear models <span class="blue">without intercept</span>: <span class="math display">
f(\bm{x}_{i}; \beta) = x_{i1}\beta_1 + \cdots + x_{ip}\beta_p = \bm{x}_{i}^T\beta.
</span></li>
</ul>
<ul>
<li>Under the centering assumption the <span class="orange">covariance matrix</span> of the data is simply <span class="math display">
S = \frac{1}{n} \bm{X}^T\bm{X}.
</span></li>
</ul>
<!-- -   If in addition each variable has been [scaled]{.blue} by their -->
<!--     standard deviations, then $n^{-1} \bm{X}^T\bm{X}$ corresponds to the -->
<!--     [correlation]{.blue} matrix. -->
</section>
<section id="singular-value-decomposition-svd" class="level2">
<h2 class="anchored" data-anchor-id="singular-value-decomposition-svd">Singular value decomposition (SVD)</h2>
<ul>
<li><p>Let <span class="math inline">\bm{X}</span> be a <span class="math inline">n \times p</span> matrix. Then, its full form <span class="orange">singular value decomposition</span> is: <span class="math display">
\bm{X} = \bm{U} \bm{D} \bm{V}^T = \sum_{j=1}^m d_j \tilde{\bm{u}}_j \tilde{\bm{v}}_j^T,
</span> with <span class="math inline">m =\min\{n, p\}</span> and where:</p>
<ul>
<li>the <span class="math inline">n \times n</span> matrix <span class="math inline">\bm{U} = (\tilde{\bm{u}}_1, \dots, \tilde{\bm{u}}_n)</span> is <span class="orange">orthogonal</span>, namely: <span class="math inline">\bm{U}^T \bm{U} = \bm{U}\bm{U}^T= I_n</span>;</li>
<li>the <span class="math inline">p \times p</span> matrix <span class="math inline">\bm{V} = (\tilde{\bm{v}}_1,\dots,\tilde{\bm{v}}_p)</span> is <span class="orange">orthogonal</span>, namely: <span class="math inline">\bm{V}^T \bm{V} = \bm{V}\bm{V}^T= I_p</span>;</li>
<li>the <span class="math inline">n \times p</span> matrix <span class="math inline">\bm{D}</span> has <span class="blue">diagonal</span> entries <span class="math inline">[\bm{D}]_{jj} = d_j</span>, for <span class="math inline">j=1,\dots,m</span>, and zero entries elsewhere;</li>
</ul></li>
<li><p>The real numbers <span class="math inline">d_1 \ge d_2 \ge \cdots \ge d_m \ge 0</span> are called <span class="blue">singular values</span>.</p></li>
<li><p>If one or more <span class="math inline">d_j = 0</span>, then the matrix <span class="math inline">\bm{X}</span> is singular.</p></li>
</ul>
</section>
<section id="principal-component-analysis-i" class="level2">
<h2 class="anchored" data-anchor-id="principal-component-analysis-i">Principal component analysis I</h2>
<div class="incremental">
<ul class="incremental">
<li><p>Le us assume that <span class="math inline">p &lt; n</span> and that <span class="math inline">\text{rk}(\bm{X}) = p</span>, recalling that <span class="math inline">\bm{X}</span> is a <span class="orange">centered</span> matrix.</p></li>
<li><p>Using SVD, the matrix <span class="math inline">\bm{X}^T\bm{X}</span> can be expressed as <span class="math display">
\bm{X}^T\bm{X} = (\bm{U} \bm{D} \bm{V}^T)^T \bm{U} \bm{D} \bm{V}^T = \bm{V} \bm{D}^T \textcolor{red}{\bm{U}^T \bm{U}} \bm{D} \bm{V}^T = \bm{V} \bm{\Delta}^2 \bm{V}^T,
</span> where <span class="math inline">\bm{\Delta}^2 = \bm{D}^T\bm{D}</span> is a <span class="math inline">p \times p</span> <span class="blue">diagonal</span> matrix with entries <span class="math inline">d_1^2,\dots,d_p^2</span>.</p></li>
<li><p>This equation is at the heart of <span class="blue">principal component analysis</span> (PCA). Define the matrix <span class="math display">
\bm{Z} = \bm{X}\bm{V} = \bm{U}\bm{D},
</span> whose columns <span class="math inline">\tilde{\bm{z}}_1,\dots,\tilde{\bm{z}}_p</span> are called <span class="orange">principal components</span>.</p></li>
<li><p>The matrix <span class="math inline">\bm{Z}</span> is orthogonal, because <span class="math inline">\bm{Z}^T\bm{Z} = \bm{D}^T\textcolor{red}{\bm{U}^T \bm{U}} \bm{D} = \bm{\Delta}^2</span>, which is diagonal.</p></li>
<li><p>Moreover, by definition the entries of <span class="math inline">\bm{Z}</span> are linear combination of the original variables: <span class="math display">
z_{ij} = x_{i1}v_{i1} + \cdots + x_{ip} v_{ip} = \bm{x}_{i}^T\tilde{\bm{v}}_j.
</span> The columns <span class="math inline">\tilde{\bm{v}}_1,\dots,\tilde{\bm{v}}_p</span> of <span class="math inline">\bm{V}</span> are sometimes called <span class="blue">loadings</span>.</p></li>
</ul>
</div>
</section>
<section id="principal-component-analysis-ii" class="level2">
<h2 class="anchored" data-anchor-id="principal-component-analysis-ii">Principal component analysis II</h2>
<div class="incremental">
<ul class="incremental">
<li><p>Principal components form an orthogonal basis of <span class="math inline">\bm{X}</span>, but they are not a “random” choice and they do <span class="orange">not</span> coincide with them <span class="orange">Gram-Schmidt</span> basis of <a href="un_A.html#the-qr-decomposition-i">Unit A</a>.</p></li>
<li><p>Indeed, the <span class="blue">first principal component</span> is the linear combination having <span class="orange">maximal variance</span>: <span class="math display">
\tilde{\bm{v}}_1 = \arg\max_{\bm{v} \in \mathbb{R}^p} \text{var}(\bm{X}\bm{v})= \arg\max_{\bm{v} \in \mathbb{R}^p} \frac{1}{n} \bm{v}^T\bm{X}^T\bm{X} \bm{v}, \quad \text{ subject to } \quad \bm{v}^T \bm{v} = 1.
</span></p></li>
<li><p>The <span class="blue">second principal component</span> maximizes the variance under the additional constraint of being <span class="orange">orthogonal</span> to the former. And so on and so forth.</p></li>
<li><p>The values <span class="math inline">d_1^2 \ge d_2^2 \ge \dots \ge d_p^2 &gt; 0</span> are the <span class="orange">eigenvalues</span> of <span class="math inline">\bm{X}^T\bm{X}</span> and correspond to the rescaled <span class="blue">variances</span> of each principal component, that is <span class="math inline">\text{var}(\tilde{\bm{z}}_j) = \tilde{\bm{z}}_j^T \tilde{\bm{z}}_j/n = d^2_j / n</span>.</p></li>
<li><p>Hence, the quantity <span class="math inline">d_j^2 / \sum_{j'=1}^p d_{j'}^2</span> measures the amount of total variance captured by principal components.</p></li>
</ul>
</div>
</section>
<section id="principal-component-analysis-prostate-data" class="level2">
<h2 class="anchored" data-anchor-id="principal-component-analysis-prostate-data">Principal component analysis: <code>prostate</code> data</h2>
<div class="cell" data-layout-align="center" data-hash="un_C_cache/html/unnamed-chunk-14_1fa0d50049fe804f1823923ca81cec4b">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_C_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="2000"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="principal-components-regression-pcr" class="level2">
<h2 class="anchored" data-anchor-id="principal-components-regression-pcr">Principal components regression (PCR)</h2>
<div class="incremental">
<ul class="incremental">
<li><p>We use the first <span class="math inline">k \le p</span> <span class="blue">principal components</span> to predict the responses <span class="math inline">y_{i}</span> via <span class="math display">
f(\bm{z}_i; \gamma) = \gamma_1 z_{i1} + \cdots + \gamma_kz_{ik}, \qquad i=1,\dots,n,
</span></p></li>
<li><p>Because of orthogonality, the least squares solution is straightforward to compute: <span class="math display">
\hat{\gamma}_j = \frac{\tilde{\bm{z}}_j^T\bm{y}}{\tilde{\bm{z}}_j^T\tilde{\bm{z}}_j} = \frac{1}{d_j^2}\tilde{\bm{z}}_j^T\bm{y}, \qquad j=1,\dots,k.
</span></p></li>
<li><p>The principal components are in <span class="orange">order of importance</span> and effectively <span class="blue">compressing the information</span> contained in <span class="math inline">\bm{X}</span> using only <span class="math inline">k \le p</span> variables.</p></li>
<li><p>When <span class="math inline">k = p</span> we are simply rotating the original matrix <span class="math inline">\bm{X} = \bm{Z}\bm{V}</span>, i.e.&nbsp;performing <span class="orange">no compression</span>. The predicted values coincide with OLS.</p></li>
<li><p>The number <span class="math inline">k</span> is a <span class="blue">complexity parameter</span> which should be chosen via information criteria or cross-validation.</p></li>
</ul>
</div>
</section>
<section id="selection-of-k-cross-validation" class="level2">
<h2 class="anchored" data-anchor-id="selection-of-k-cross-validation">Selection of <span class="math inline">k</span>: cross-validation</h2>
<div class="cell" data-layout-align="center" data-hash="un_C_cache/html/unnamed-chunk-16_9f986946e70995b44dc0d71722e91deb">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_C_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="2000"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="shrinkage-effect-of-principal-components-i" class="level2">
<h2 class="anchored" data-anchor-id="shrinkage-effect-of-principal-components-i">Shrinkage effect of principal components I</h2>
<div class="incremental">
<ul class="incremental">
<li><p>A closer look to the PCR solution reveals some interesting aspects. Recall that: <span class="math display">
\tilde{\bm{z}}_j = \bm{X}\tilde{\bm{v}}_j = d_j \tilde{\bm{u}}_j,  \qquad j=1,\dots,p.
</span></p></li>
<li><p>The <span class="orange">predicted values</span> for the <span class="blue">centered responses</span> <span class="math inline">\bm{y}</span> of the PCR with <span class="math inline">k</span> components are: <span class="math display">
\bm{X}\hat{\beta}_\text{pcr} = \sum_{j=1}^k \tilde{\bm{z}}_j \hat{\gamma}_j = \bm{X} \sum_{j=1}^k \tilde{\bm{v}}_j \hat{\gamma}_j, \qquad \text{ where } \qquad \hat{\beta}_\text{pcr} = \sum_{j=1}^k \tilde{\bm{v}}_j \hat{\gamma}_j.
</span></p></li>
<li><p>This representation highlights two important aspects:</p>
<ul class="incremental">
<li>It is possible to express the PCR solution in the original scale, for better <span class="orange">interpretability</span>;</li>
<li>The vector <span class="math inline">\hat{\beta}_\text{pcr}</span> is a <span class="blue">constrained solution</span>, being a combination of <span class="math inline">k \le p</span> coefficients, therefore <span class="orange">reducing</span> the <span class="orange">complexity</span> of the model and <span class="blue">shrinking</span> the coefficients.</li>
</ul></li>
<li><p>When <span class="math inline">k = 1</span>, then the <span class="math inline">\hat{\beta}_\text{pcr}</span> estimate coincide with the scaled loading vector <span class="math inline">\hat{\beta}_\text{pcr} = \hat{\gamma}_1 \tilde{\bm{v}}_1</span>;</p></li>
<li><p>When <span class="math inline">k = p</span> then the <span class="math inline">\hat{\beta}_\text{pcr}</span> coincides with <span class="blue">ordinary least squares</span> (see Exercises).</p></li>
</ul>
</div>
</section>
<section id="shrinkage-effect-of-principal-components-ii" class="level2">
<h2 class="anchored" data-anchor-id="shrinkage-effect-of-principal-components-ii">Shrinkage effect of principal components II</h2>
<div class="incremental">
<ul class="incremental">
<li><p>The <span class="orange">variance</span> of <span class="math inline">\hat{\beta}_\text{pcr}</span>, assuming iid errors <span class="math inline">\epsilon_i</span> in the <span class="blue">original data</span>, is: <span class="math display">
\text{var}(\hat{\beta}_\text{pcr}) = \sigma^2\sum_{j=1}^k \frac{1}{d_j^2} \tilde{\bm{v}}_j\tilde{\bm{v}}_j^T.
</span></p></li>
<li><p>Thus, if a <span class="blue">multicollinearity</span> exists, then it appears as a principal component with very small variance, i.e.&nbsp;a small <span class="math inline">d_j^2</span>. Its removal therefore drastically <span class="orange">reduces</span> the <span class="orange">variance</span> of <span class="math inline">\hat{\beta}_\text{pcr}</span>.</p></li>
<li><p>Furthermore, the predicted values for the <span class="orange">centered data</span> can be expressed as <span class="math display">
\bm{X}\hat{\beta}_\text{pcr} = \sum_{j=1}^k \tilde{\bm{z}}_j \hat{\gamma}_j = \sum_{j=1}^k \tilde{\bm{z}}_j  \frac{\tilde{\bm{z}}_j^T\bm{y}}{\tilde{\bm{z}}_j^T\tilde{\bm{z}}_j} = \sum_{j=1}^k \textcolor{darkblue}{d_j} \tilde{\bm{u}}_j  \frac{\textcolor{darkblue}{d_j}}{\textcolor{darkblue}{d_j^2}} \frac{\tilde{\bm{u}}_j^T\bm{y}}{\textcolor{red}{\tilde{\bm{u}}_j^T\tilde{\bm{u}}_j}} = \sum_{j=1}^k \tilde{\bm{u}}_j \tilde{\bm{u}}_j^T \bm{y}.
</span></p></li>
<li><p>The columns of <span class="math inline">\bm{U}</span>, namely the vectors <span class="math inline">\tilde{\bm{u}}_j</span> are the <span class="blue">normalized principal components</span>.</p></li>
<li><p>Hence, we are shrinking the predictions towards the main <span class="orange">principal directions</span>.</p></li>
</ul>
</div>
</section>
<section id="shrinkage-effect-of-principal-components-iii" class="level2">
<h2 class="anchored" data-anchor-id="shrinkage-effect-of-principal-components-iii">Shrinkage effect of principal components III</h2>
<div class="cell" data-layout-align="center" data-hash="un_C_cache/html/unnamed-chunk-17_ad55a1663f7566dcd082973cb8734f9f">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_C_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="1800"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="pros-and-cons-of-pcr" class="level2">
<h2 class="anchored" data-anchor-id="pros-and-cons-of-pcr">Pros and cons of PCR</h2>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pros
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Principal components are a natural tool to <span class="blue">reduce the complexity</span> of the data, especially in presence of <span class="orange">highly correlated</span> variables.</p></li>
<li><p>If you transform back the coefficients, there is a <span class="blue">clean interpretation</span> of the impact of the covariates on the response.</p></li>
<li><p>Principal components might be interesting in their own right, as they describe the <span class="blue">dependence structure</span> among covariates.</p></li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cons
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><span class="orange">All the variables</span> are used for predictions, which could be computationally demanding.</p></li>
<li><p>The shrinkage effect on the regression coefficients is somewhat indirect and not smooth.</p></li>
<li><p>Principal component regression can not be applied when <span class="math inline">p &gt; n</span>.</p></li>
</ul>
</div>
</div>
</section>
</section>
<section id="ridge-regression" class="level1">
<h1>Ridge regression</h1>
<section id="shrinkage-methods" class="level2">
<h2 class="anchored" data-anchor-id="shrinkage-methods">Shrinkage methods</h2>
<div class="columns">
<div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="img/ridge.jpg" class="img-fluid figure-img"></p>
</figure>
</div>
</div><div class="column" style="width:70%;">
<ul>
<li><p>Shrinkage methods are popular tools for handling the issue of multiple variables.</p></li>
<li><p>Shrinkage <span class="blue">regularizes</span> the estimates, <span class="blue">constraining</span> the <span class="blue">size</span> of the regression coefficients.</p></li>
<li><p>This lead to <span class="orange">biased estimator</span> with, hopefully, lower variance.</p></li>
<li><p>As a byproduct, the induced regularization procedure enables estimation even when <span class="math inline">p &gt; n</span>.</p></li>
<li><p>The first method that has been proposed is called <span class="orange">ridge regression</span>. The lasso and the elastic-net are other examples.</p></li>
</ul>
</div>
</div>
</section>
<section id="the-ridge-regularization-method" class="level2">
<h2 class="anchored" data-anchor-id="the-ridge-regularization-method">The ridge regularization method</h2>
<ul>
<li>The ridge estimator is the most common <span class="blue">shrinkage method</span> and is the <span class="orange">minimizer</span> of <span class="math display">
\sum_{i=1}^n(y_i - \beta_0 -  \bm{x}_i^T\beta)^2 \qquad \text{subject to} \qquad \sum_{j=1}^p \beta_j^2 \le s.
</span></li>
</ul>
<ul>
<li><p>When the <span class="blue">complexity parameter</span> <span class="math inline">s</span> is small, the coefficients are explicitly <span class="orange">shrinked</span>, i.e.&nbsp;biased, <span class="orange">towards zero</span>.</p></li>
<li><p>On the other hand, if <span class="math inline">s</span> is large enough, then the ridge estimator coincides with ordinary least squares.</p></li>
</ul>
<ul>
<li>In ridge regression, the <span class="orange">variability</span> of the estimator is explicitly <span class="orange">bounded</span>, although this comes with some <span class="blue">bias</span>. The parameter <span class="math inline">s</span> controls the bias-variance trade-off.</li>
</ul>
<ul>
<li>The intercept term <span class="math inline">\beta_0</span> is <span class="orange">not penalized</span>, because there are no strong reasons to believe that the mean of <span class="math inline">y_i</span> equals zero. However, as before, we would like to “remove the intercept”.</li>
</ul>
</section>
<section id="centering-and-scaling-the-predictors-i" class="level2">
<h2 class="anchored" data-anchor-id="centering-and-scaling-the-predictors-i">Centering and scaling the predictors I</h2>
<ul>
<li>The ridge solutions are <span class="orange">not equivariant</span> under <span class="orange">scalings of the input</span>, so one normally <span class="blue">standardizes</span> the input to have unit variance, if they are not in the same scale.</li>
</ul>
<ul>
<li>Moreover, as for PCR, we can estimate the intercept using a <span class="orange">two-step procedure</span>:
<ul>
<li>The reparametrization <span class="math inline">\alpha = \beta_0 + \bar{\bm{x}}^T\beta</span> is equivalent to centering the predictors;</li>
<li>The estimate for the centered intercept is <span class="math inline">\hat{\alpha} = \bar{y}</span>;</li>
<li>The ridge estimate can be obtained considering a model without intercept, using centered responses and predictors.</li>
</ul></li>
</ul>
<ul>
<li>Hence, in ridge regression we replace <span class="orange">original data</span> <span class="math inline">Y_i = f(\bm{x}_i) + \epsilon_i</span> with their <span class="blue">standardized</span> version: <span class="math display">
\frac{x_{ij} - \bar{x}_j}{s_j}, \qquad y_i - \bar{y}, \qquad i=1,\dots,n; \ \ j=1,\dots,p.
</span> where <span class="math inline">s_j^2 = n^{-1}\sum_{i=1}^n (x_{ij} - \bar{x}_j)^2</span> is the <span class="blue">sample variance</span>.</li>
</ul>
</section>
<section id="centering-and-scaling-the-predictors-ii" class="level2">
<h2 class="anchored" data-anchor-id="centering-and-scaling-the-predictors-ii">Centering and scaling the predictors II</h2>
<ul>
<li>It is easy to show (see Exercises) that the <span class="orange">coefficients</span> expressed in the <span class="blue">original scale</span> are <span class="math display">
\hat{\beta}_0 = \bar{y} - \bar{\bm{x}}\hat{\beta}_\text{ridge}, \qquad \hat{\beta}_\text{scaled-ridge} = \text{diag}(1 / s_1,\dots, 1/s_p) \hat{\beta}_\text{ridge}.
</span> Thus, the <span class="orange">predictions</span> on the <span class="blue">original scale</span> are <span class="math inline">\hat{\beta}_0 + \bm{x}_i^T\hat{\beta}_\text{scaled-ridge} = \bar{y} + \bm{x}_{i}^T\hat{\beta}_\text{ridge}</span>.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>For ridge problems we will assume the data have been previously <span class="orange">standardized</span>, namely <span class="math display">
    \frac{1}{n}\sum_{i=1}^ny_{i} = 0, \qquad \frac{1}{n}\sum_{i=1}^nx_{ij} = 0, \qquad \frac{1}{n}\sum_{i=1}^n x_{i}^2 = 1\qquad j=1,\dots,p.
    </span></p>
</div>
</div>
</div>
<ul>
<li>We will say that the <span class="blue">ridge estimator</span> <span class="math inline">\hat{\beta}_\text{ridge}</span> is the <span class="orange">minimizer</span> of following system <span class="math display">
\sum_{i=1}^n(y_i - \bm{x}_{i}^T\beta)^2 \qquad \text{subject to} \qquad \sum_{j=1}^p \beta_j^2 \le s.
</span></li>
</ul>
</section>
<section id="lagrange-multipliers-and-ridge-solution" class="level2">
<h2 class="anchored" data-anchor-id="lagrange-multipliers-and-ridge-solution">Lagrange multipliers and ridge solution</h2>
<ul>
<li>The ridge regression problem can be <span class="orange">equivalently expressed</span> in its <a href="https://en.wikipedia.org/wiki/Karush–Kuhn–Tucker_conditions">Lagrangian form</a>, which greatly facilitates computations. The ridge estimator <span class="math inline">\hat{\beta}_\text{ridge}</span> is the <span class="orange">minimizer</span> of <span class="math display">
\sum_{i=1}^n(y_{i} - \bm{x}_{i}^T\beta)^2 + \lambda \sum_{j=1}^p\beta_j^2 = \underbrace{||\bm{y} - \bm{X}\beta||^2}_{\text{least squares}} + \underbrace{\lambda ||\beta||^2}_{\text{\textcolor{red}{ridge penalty}}},
</span> where <span class="math inline">\lambda &gt; 0</span> is a <span class="blue">complexity parameter</span> controlling the <span class="orange">penalty</span>. It holds that <span class="math inline">s = ||\hat{\beta}_\text{ridge} ||^2</span>.</li>
</ul>
<ul>
<li>When <span class="math inline">\lambda = 0</span> then <span class="math inline">\hat{\beta}_\text{ridge} = \hat{\beta}_\text{ols}</span> whereas when <span class="math inline">\lambda \rightarrow \infty</span> we get <span class="math inline">\hat{\beta}_\text{ridge} = 0</span>.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ridge regression estimator
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any <span class="math inline">n\times p</span> design matrix <span class="math inline">\bm{X}</span>, not necessarily of full-rank, the ridge estimator is <span class="math display">
\hat{\beta}_\text{ridge} = (\bm{X}^T\bm{X} + \lambda I_p)^{-1}\bm{X}^T\bm{y}.
</span> Such an estimator <span class="blue">always exists</span> and is <span class="orange">unique</span> (even when <span class="math inline">p &gt; n</span>).</p>
</div>
</div>
<!-- ## Mathematical details -->
<!-- ::: incremental -->
<!-- - The matrix $\bm{X}^T\bm{X} + \lambda I_p$ is [invertible]{.blue} for any $\lambda > 0$.  -->
<!-- - For any $\lambda >0$, the ridge solution is always at the [boundary]{.orange}, that is, is never an interior point. Indeed, if $\bm{X}$ is full rank, it can be shown that -->
<!-- $$ -->
<!-- ||\hat{\beta}_\text{ridge}||^2 \le ||\hat{\beta}||^2. -->
<!-- $$ -->
<!-- - The size of the spherical ridge parameter constraint [shrinks monotonously]{.orange} as $\lambda$ increases: -->
<!-- $$ -->
<!-- \frac{\partial}{\partial \lambda} ||\hat{\beta}_\text{ridge}||^2 < 0, -->
<!-- $$ -->
<!-- ::: -->
</section>
<section id="the-geometry-of-the-ridge-solution" class="level2">
<h2 class="anchored" data-anchor-id="the-geometry-of-the-ridge-solution">The geometry of the ridge solution</h2>
<div class="cell" data-hash="un_C_cache/html/unnamed-chunk-18_0864496ed4fdee83020d3572f6a4bf36">
<div class="cell-output-display">
<p><img src="un_C_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid" width="1400"></p>
</div>
</div>
</section>
<section id="the-ridge-path" class="level2">
<h2 class="anchored" data-anchor-id="the-ridge-path">The ridge path</h2>
<div class="cell" data-layout-align="center" data-hash="un_C_cache/html/unnamed-chunk-21_946a4f625e2aed4ae231be26de0243dd">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_C_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid figure-img" width="1800"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="comments-on-the-ridge-path" class="level2">
<h2 class="anchored" data-anchor-id="comments-on-the-ridge-path">Comments on the ridge path</h2>
<ul>
<li><p>The values of <span class="math inline">\lambda</span> are in somewhat <span class="orange">arbitrary scale</span>. The ridge penalty has a concrete effect starting from <span class="math inline">\lambda / n &gt; 0.1</span> or so.</p></li>
<li><p>The variable <code>lcavol</code> is arguably the most important, followed by <code>lweight</code> and <code>svi</code>, being the one that receive less shrinkage compared to the others.</p></li>
</ul>
<ul>
<li><p>The coefficient of <code>age</code>, <code>gleason</code> and <code>lcp</code>, is negative at the beginning and then become positive for large values of <span class="math inline">\lambda</span>.</p></li>
<li><p>This indicate that their negative value in <span class="math inline">\hat{\beta}_\text{ols}</span> was probably a consequence of their <span class="orange">correlation</span> with other variables.</p></li>
</ul>
<ul>
<li>There is an interesting similarity between this plot and the one of principal component regression… is it a coincidence?</li>
</ul>
</section>
<section id="shrinkage-effect-of-ridge-regression-i" class="level2">
<h2 class="anchored" data-anchor-id="shrinkage-effect-of-ridge-regression-i">Shrinkage effect of ridge regression I</h2>
<ul>
<li>Considering, once again, the <span class="orange">singular value decomposition</span>, we get: <span class="math display">\begin{aligned}
\bm{X}\hat{\beta}_\text{ridge} &amp;= \bm{X}(\bm{X}^T\bm{X} + \lambda I_p)^{-1}\bm{X}^T\bm{y} \\
&amp; = \bm{U}\bm{D} \bm{V}^T[\bm{V}(\bm{D}^T\bm{D} + \lambda I_p)\bm{V}^T]^{-1}(\bm{U}\bm{D}\bm{V})^T\bm{y}  \\
&amp; = \bm{U}\bm{D} \textcolor{red}{\bm{V}^T\bm{V}}(\bm{D}^T\bm{D} + \lambda I_p)^{-1} \textcolor{red}{\bm{V}^T \bm{V}} \bm{D}^T \bm{U} ^T\bm{y} \\
&amp; = \bm{U}\bm{D}(\bm{D}^T\bm{D} + \lambda I_p)^{-1}\bm{D}^T\bm{U}^T\bm{y} \\
&amp; = \bm{H}_\text{ridge}\bm{y} = \sum_{j=1}^p \tilde{\bm{u}}_j \frac{d_j^2}{d_j^2 + \lambda}\tilde{\bm{u}}_j^T \bm{y},
\end{aligned}
</span> where <span class="math inline">\bm{H}_\text{ridge} = \bm{X}(\bm{X}^T\bm{X} + \lambda I_p)^{-1}\bm{X}^T</span> is the so-called <span class="blue">hat matrix</span> of ridge regression.</li>
</ul>
<ul>
<li><p>This means that ridge regression <span class="blue">shrinks</span> the <span class="blue">principal directions</span> by an amount that depends on the <span class="orange">eigenvalues</span> <span class="math inline">d_j^2</span>.</p></li>
<li><p>In other words, it <span class="orange">smoothly reduces</span> the impact of the <span class="orange">redundant</span> information.</p></li>
</ul>
</section>
<section id="shrinkage-effect-of-ridge-regression-ii" class="level2">
<h2 class="anchored" data-anchor-id="shrinkage-effect-of-ridge-regression-ii">Shrinkage effect of ridge regression II</h2>
<ul>
<li><p>A sharp <span class="blue">connection</span> with <span class="blue">principal components regression</span> is therefore revealed.</p></li>
<li><p>Compare the previous formula for <span class="math inline">\bm{X}\hat{\beta}_\text{ridge}</span> with <a href="#shrinkage-effect-of-principal-components-ii">the one we previously obtained</a> for <span class="math inline">\bm{X}\hat{\beta}_\text{pcr}</span>.</p></li>
</ul>
<ul>
<li>More explicitly, for <span class="orange">ridge regression</span> we will have that <span class="math display">
\hat{\beta}_\text{ridge} = \bm{V}\text{diag}\left(\frac{d_1}{d_1^2 + \lambda}, \dots, \frac{d_p}{d_p^2 + \lambda}\right)\bm{U}^T\bm{y}.
</span> whereas for <span class="blue">principal components regression</span> with <span class="math inline">k</span> components we get <span class="math display">
\hat{\beta}_\text{pcr} = \bm{V}\text{diag}\left(\frac{1}{d_1}, \dots, \frac{1}{d_k}, 0, \dots, 0\right)\bm{U}^T\bm{y}.
</span></li>
</ul>
<ul>
<li>Both operate on the singular values, but where principal component regression <span class="blue">thresholds</span> the singular values, <span class="orange">ridge regression</span> shrinks them.</li>
</ul>
</section>
<section id="bias-variance-trade-off" class="level2">
<h2 class="anchored" data-anchor-id="bias-variance-trade-off">Bias-variance trade-off</h2>
<ul>
<li>The ridge regression <span class="orange">add some bias</span> to the estimates, but it <span class="blue">reduces</span> their <span class="blue">variance</span>.</li>
</ul>
<ul>
<li>The <span class="orange">variance</span> of <span class="math inline">\hat{\beta}_\text{ridge}</span>, assuming iid errors <span class="math inline">\epsilon_i</span> in the <span class="blue">original scale</span> with variance <span class="math inline">\sigma^2</span>, results: <span class="math display">
\text{var}(\hat{\beta}_\text{ridge}) = \sigma^2\sum_{j=1}^p \frac{d_j^2}{(d_j^2 + \lambda)^2} \tilde{\bm{v}}_j\tilde{\bm{v}}_j^T,
</span> whose diagonal elements are always smaller than those of <span class="math inline">\text{var}(\hat{\beta}_\text{ols})</span>.</li>
</ul>
<ul>
<li><p>The above formula highlights that ridge will be very <span class="blue">effective</span> in presence highly <span class="blue">correlated variables</span>, as they will be “shrunk” away by the penalty.</p></li>
<li><p>What typically happens is that such a reduction in variance <span class="orange">compensate</span> the increase in bias, especially when <span class="math inline">p</span> is large relative to <span class="math inline">n</span>.</p></li>
</ul>
</section>
<section id="a-historical-perspective-i" class="level2">
<h2 class="anchored" data-anchor-id="a-historical-perspective-i">☠️ - A historical perspective I</h2>
<div class="incremental">
<ul class="incremental">
<li><p>The ridge regression estimator was originally proposed by Hoerl and Kennard (1970) with a quite different motivation in mind.</p></li>
<li><p>In linear models, the estimate of <span class="math inline">\beta</span> is obtained by solving the <span class="blue">normal equations</span> <span class="math display">
(\bm{X}^T\bm{X})\beta = \bm{X}^T\bm{y},
</span> which could be <span class="orange">ill-conditioned</span>.</p></li>
<li><p>In other words, the <span class="blue">condition number</span> <span class="math display">
\kappa(\bm{X}^T\bm{X}) = \frac{d_1^2}{d_p^2},
</span> might be very large, leading to <span class="orange">numerical inaccuracies</span>, since the matrix <span class="math inline">\bm{X}^T\bm{X}</span> is <span class="blue">numerically singular</span> and therefore not invertible in practice.</p></li>
</ul>
</div>
</section>
<section id="a-historical-perspective-ii" class="level2">
<h2 class="anchored" data-anchor-id="a-historical-perspective-ii">☠️ - A historical perspective II</h2>
<div class="incremental">
<ul class="incremental">
<li><p>Ridge provides a <span class="blue">remedy</span> for <span class="orange">ill-conditioning</span>, by adding a “ridge” to the diagonal of <span class="math inline">\bm{X}^T\bm{X}</span>, obtaining the modified normal equations <span class="math display">
(\bm{X}^T\bm{X} + \lambda I_p)\beta = \bm{X}^T\bm{y}.
</span></p></li>
<li><p>The <span class="orange">condition number</span> of the modified <span class="math inline">(\bm{X}^T\bm{X} + \lambda I_p)</span> matrix becomes <span class="math display">
\kappa(\bm{X}^T\bm{X} + \lambda I_p) = \frac{\lambda + d_1^2}{\lambda + d_p^2}.
</span></p></li>
<li><p>Notice that even if <span class="math inline">d_p = 0</span>, i.e.&nbsp;the matrix <span class="math inline">\bm{X}</span> is <span class="blue">singular</span>, then condition number will be finite as long as <span class="math inline">\lambda &gt; 0</span>.</p></li>
<li><p>This technique is known as <span class="orange">Tikhonov regularization</span>, after the Russian mathematician Andrey Tikhonov.</p></li>
</ul>
</div>
</section>
<section id="a-historical-perspective-iii" class="level2">
<h2 class="anchored" data-anchor-id="a-historical-perspective-iii">☠️ - A historical perspective III</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/ridge_paper.png" class="img-fluid figure-img" style="width:40.0%"></p>
</figure>
</div>
<ul>
<li>Figure 1 of the <span class="orange">original paper</span> by Hoerl and Kennard (1970), displaying the bias-variance <span class="blue">trade-off</span>.</li>
</ul>
</section>
<section id="on-the-choice-of-lambda" class="level2">
<h2 class="anchored" data-anchor-id="on-the-choice-of-lambda">On the choice of <span class="math inline">\lambda</span></h2>
<ul>
<li>The <span class="orange">penalty parameter</span> <span class="math inline">\lambda</span> determines the amount of bias and variance of <span class="math inline">\hat{\beta}_\text{ridge}</span> and therefore it must be carefully <span class="blue">estimated</span>.</li>
</ul>
<ul>
<li><p><span class="blue">Minimizing</span> the loss <span class="math inline">||\bm{y} - \bm{X}\hat{\beta}_\text{ridge}||^2</span> over <span class="math inline">\lambda</span> is a <span class="orange">bad idea</span>, because it would always lead to <span class="math inline">\lambda = 0</span>, corresponding to <span class="math inline">\hat{\beta}_\text{ridge} = \hat{\beta}_\text{ols}</span>.</p></li>
<li><p>Indeed, <span class="math inline">\lambda</span> is a <span class="blue">complexity</span> parameter and, like the number of covariates, should be selected using <span class="orange">information criteria</span> or training/test and <span class="orange">cross-validation</span>.</p></li>
</ul>
<ul>
<li><p>Suppose we wish to use an <span class="blue">information criteria</span> such as the AIC or BIC, of the form <span class="math display">
  \text{IC}(p) = -2 \ell(\hat{\beta}_\text{ridge}) + \text{penalty}(\text{``degrees of freedom"}).
  </span> We need a careful definition of <span class="blue">degrees of freedom</span> that is appropriate in this context.</p></li>
<li><p>The current definition of degrees of freedom, i.e.&nbsp;the number of <span class="orange">non-zero coefficients</span>, is <span class="orange">not appropriate</span> for ridge regression, because it would be equal to <span class="math inline">p</span> for any value of <span class="math inline">\lambda</span>.</p></li>
</ul>
</section>
<section id="effective-degrees-of-freedom-i" class="level2">
<h2 class="anchored" data-anchor-id="effective-degrees-of-freedom-i">Effective degrees of freedom I</h2>
<ul>
<li>Let us recall that the original data are <span class="math inline">Y_i = f(\bm{x}_i) + \epsilon_i</span> and that the <span class="orange">optimism</span> for a generic estimator <span class="math inline">\hat{f}(\bm{x})</span> is defined as the following average of covariances <span class="math display">
\text{Opt} = \frac{2}{n}\sum_{i=1}^n\text{cov}(Y_i, \hat{f}(\bm{x}_i)),
</span> which is equal to <span class="math inline">\text{Opt}_\text{ols} = (2\sigma^2p)/ n</span> in <span class="blue">ordinary least squares</span>.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Effective degrees of freedom
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\hat{f}(\bm{x})</span> be an estimate for the regression function <span class="math inline">f(\bm{x})</span> based on the data <span class="math inline">Y_1,\dots,Y_n</span>. The <span class="orange">effective degrees of freedom</span> are defined as <span class="math display">
\text{df} = \frac{1}{\sigma^2}\sum_{i=1}^n\text{cov}(Y_i, \hat{f}(\bm{x}_i)).
</span></p>
</div>
</div>
</section>
<section id="effective-degrees-of-freedom-ii" class="level2">
<h2 class="anchored" data-anchor-id="effective-degrees-of-freedom-ii">Effective degrees of freedom II</h2>
<div class="incremental">
<ul class="incremental">
<li><p>The effective degrees of freedom of <span class="blue">ordinary least squares</span> and <span class="blue">principal component regression</span> are <span class="math display">
\text{df}_\text{ols} = p + 1, \qquad \text{df}_\text{pcr} = k + 1,
</span> where the additional term correspond to the <span class="orange">intercept</span>.</p></li>
<li><p>After some algebra, one finds that the effective degrees of freedom of <span class="orange">ridge regression</span> are <span class="math display">
\text{df}_\text{ridge} = 1 + \text{tr}(\bm{H}_\text{ridge}) = 1 + \sum_{j=1}^p \frac{d_j^2}{d_j^2 + \lambda}.
</span></p></li>
<li><p>Using the above result, we can <span class="orange">plug-in</span> <span class="math inline">\text{df}_\text{ridge}</span> into the formula of the <span class="blue"><span class="math inline">C_p</span> of Mallows</span>: <span class="math display">
  \widehat{\mathrm{ErrF}} = \frac{1}{n}\sum_{i=1}^n(y_i - \bm{x}_i^T\hat{\beta}_\text{scaled-ridge})^2 + \frac{2 \hat{\sigma}^2}{n} \text{df}_\text{ridge}.
  </span> where the residual variance is estimated as <span class="math inline">\hat{\sigma}^2 = (n - \text{df}_\text{ridge})^{-1} \sum_{i=1}^n(y_i - \bm{x}_i^T\hat{\beta}_\text{scaled-ridge})^2</span>.</p></li>
</ul>
</div>
</section>
<section id="effective-degrees-of-freedom-iii" class="level2">
<h2 class="anchored" data-anchor-id="effective-degrees-of-freedom-iii">Effective degrees of freedom III</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="cell" data-hash="un_C_cache/html/unnamed-chunk-22_f0b39d0fc664711316ec125773252c81">
<div class="cell-output-display">
<p><img src="un_C_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid" width="1000"></p>
</div>
</div>
</div><div class="column" style="width:50%;">
<div class="cell" data-hash="un_C_cache/html/unnamed-chunk-23_532b63763643f37f90a7bcf42786bec4">
<div class="cell-output-display">
<p><img src="un_C_files/figure-html/unnamed-chunk-23-1.png" class="img-fluid" width="1000"></p>
</div>
</div>
</div>
</div>
</section>
<section id="cross-validation-for-ridge-regression-i" class="level2">
<h2 class="anchored" data-anchor-id="cross-validation-for-ridge-regression-i">Cross-validation for ridge regression I</h2>
<ul>
<li><p>Training / test strategies and <span class="blue">cross-validation</span> are also valid tools for selecting <span class="math inline">\lambda</span>.</p></li>
<li><p>Most statistical software packages use a slightly <span class="orange">different parametrization</span> for <span class="math inline">\lambda</span>, as they minimize <span class="math display">
\textcolor{red}{\frac{1}{n}}\sum_{i=1}^n(y_{i} - \bm{x}_{i}^T\beta)^2 + \tilde{\lambda} \sum_{j=1}^p\beta_j^2,
</span> where the penalty parameter <span class="math inline">\tilde{\lambda} = \lambda / n</span>.</p></li>
</ul>
<ul>
<li>This parametrization does not alter the estimate of <span class="math inline">\hat{\beta}_\text{ridge}</span> but is more amenable for <span class="blue">cross-validation</span> as the values of <span class="math inline">\tilde{\lambda}</span> can be compared across dataset with <span class="orange">different sample sizes</span>.</li>
</ul>
<ul>
<li><p>Different R packages have <span class="blue">different defaults</span> about other aspects too.</p></li>
<li><p>For instance, the R package <code>glmnet</code> uses <span class="math inline">\tilde{\lambda}</span> and also <span class="blue">standardizes the response</span> <span class="math inline">\bm{y}</span> and then <span class="orange">transforms back</span> the estimated coefficients into the <span class="orange">original scale</span>.</p></li>
</ul>
</section>
<section id="cross-validation-for-ridge-regression-ii" class="level2">
<h2 class="anchored" data-anchor-id="cross-validation-for-ridge-regression-ii">Cross-validation for ridge regression II</h2>
<div class="cell" data-layout-align="center" data-hash="un_C_cache/html/unnamed-chunk-25_1ab1b9b40c703aa69acbabe31c01ff29">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_C_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid figure-img" width="2000"></p>
</figure>
</div>
</div>
</div>
<!-- ## Leave-one-out cross-validation -->
<!-- -   Ridge regression is, like ordinary least squares, a [linear smoother]{.blue}, namely: -->
<!-- $$ -->
<!-- \bm{X}\hat{\beta}_\text{ridge} = \bm{X}(\bm{X}^T\bm{X} + \lambda I_p)^{-1}\bm{X}^T\bm{y} = \bm{H}_\text{ridge}\bm{y}, -->
<!-- $$ -->
<!-- meaning that a [computational shortcut]{.orange} for LOO-CV can be exploited. -->
<!-- . . . -->
<!-- ::: callout-note -->
<!-- #### LOO-CV (Ridge regression) -->
<!-- Let $\hat{y}_{-i, \text{ridge}} = \bm{x}_i^T\hat{\beta}_{-i, \text{ridge}}$ be the leave-one-out -->
<!-- predictions of a [linear model]{.blue} with a [ridge penalty]{.orange} and let $h_{i, \text{ridge}} = [\bm{H}_\text{ridge}]_{ii}$ and -->
<!-- $\hat{y}_i$ be the leverages and the predictions. Then: $$ -->
<!-- y_i - \hat{y}_{-i, \text{ridge}} = \frac{y_i - \hat{y}_{i, \text{ridge}}}{1 - h_{i, \text{ridge}}}, \qquad i=1,\dots,n, -->
<!-- $$ -->
<!-- recalling that $\bar{y} = 0$. Therefore, the leave-one-out mean squared error is -->
<!-- $$\widehat{\mathrm{Err}} = \frac{1}{n}\sum_{i=1}^n \left(\frac{y_i - \hat{y}_{i, \text{ridge}}}{1 - h_{i, \text{ridge}}}\right)^2.$$ -->
<!-- ::: -->
</section>
<section id="the-ridge-estimate" class="level2">
<h2 class="anchored" data-anchor-id="the-ridge-estimate">The ridge estimate</h2>
<div class="cell" data-layout-align="center" data-hash="un_C_cache/html/unnamed-chunk-27_9c11b9e076ff56b4a400f0a2ff113d78">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_C_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid figure-img" width="1800"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="further-properties-of-ridge-regression" class="level2">
<h2 class="anchored" data-anchor-id="further-properties-of-ridge-regression">Further properties of ridge regression</h2>
<div class="incremental">
<ul class="incremental">
<li><p>Ridge regression has a transparent <span class="orange">Bayesian interpretation</span>, since the penalty can be interpreted as a Gaussian prior on <span class="math inline">\beta</span>.</p></li>
<li><p>If two variables are identical copies <span class="math inline">\tilde{\bm{x}}_j = \tilde{\bm{x}}_\ell</span>, so are the corresponding ridge coefficients <span class="math inline">\hat{\beta}_{j,\text{ridge}} = \hat{\beta}_{\ell,\text{ridge}}</span>.</p></li>
<li><p>Adding <span class="math inline">p</span> <span class="orange">fake observations</span> all equal to <span class="math inline">0</span> to the response and then fitting ordinary least squares leads to the ridge estimator. This procedure is called <span class="blue">data augmentation</span>.</p></li>
<li><p>A computationally convenient formula for <span class="blue">LOO cross-validation</span> is available, which requires the model to be estimated only once, as in least squares.</p></li>
<li><p>In the <span class="orange"><span class="math inline">p &gt; n</span> case</span> there are specific <span class="orange">computational strategies</span> that can be employed; see Section 18.3.5 of Hastie, Tibshirani and Friedman (2011).</p></li>
</ul>
</div>
</section>
<section id="pros-and-cons-of-ridge-regression" class="level2">
<h2 class="anchored" data-anchor-id="pros-and-cons-of-ridge-regression">Pros and cons of ridge regression</h2>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pros
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Ridge regression trades some <span class="orange">bias</span> in exchange of a <span class="blue">lower variance</span>, often resulting in more accurate predictions.</p></li>
<li><p>The ridge solution <span class="orange">always exists</span> and is <span class="blue">unique</span>, even when <span class="math inline">p &gt; n</span> or in presence of perfect <span class="orange">collinearity</span>.</p></li>
<li><p>For fixed values of <span class="math inline">\lambda</span>, <span class="blue">efficient computations</span> are available using QR and Cholesky decompositions. <!-- -   At the end of this unit we will describe a [general optimization --> <!--     strategy]{.blue} that recovers the entire "ridge path", i.e. the --> <!--     estimate $\hat{\beta}_\text{ridge}$ for several values of $\lambda$. --></p></li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cons
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>In ridge regression, <span class="orange">all variables</span> are used. This is in contrast with best subset selection.</li>
</ul>
</div>
</div>
</section>
</section>
<section id="the-lasso" class="level1">
<h1>The lasso</h1>
<section id="looking-for-sparsity" class="level2">
<h2 class="anchored" data-anchor-id="looking-for-sparsity">Looking for sparsity</h2>
<div class="columns">
<div class="column" style="width:25%;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="img/tibshirani.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Robert Tibshirani</figcaption>
</figure>
</div>
</div><div class="column" style="width:75%;">
<ul>
<li>Signal <span class="orange">sparsity</span> is the assumption that only a small number of predictors have an effect, i.e.&nbsp;<span class="math display">
\beta_j = 0, \qquad \text{for most} \qquad j \in \{1,\dots,p\}.</span></li>
<li>In this case we would like our estimator <span class="math inline">\hat{\beta}</span> to be <span class="blue">sparse</span>, meaning that <span class="math inline">\hat{\beta}_j = 0</span> for many <span class="math inline">j \in \{1,\dots,p\}</span>.</li>
<li>Sparse estimators are desirable because:
<ul>
<li>perform <span class="blue">variable selection</span> and improve the <span class="orange">interpretability</span> of the results;</li>
<li>Speed up the <span class="blue">computations</span> of the predictions, because less variables are needed.</li>
</ul></li>
<li>Best subset selection is sparse (but computationally unfeasible), the ridge estimator is not.</li>
</ul>
</div>
</div>
</section>
<section id="the-least-absolute-selection-and-shrinkage-operator" class="level2">
<h2 class="anchored" data-anchor-id="the-least-absolute-selection-and-shrinkage-operator">The <span class="orange">l</span>east <span class="orange">a</span>bsolute <span class="orange">s</span>election and <span class="orange">s</span>hrinkage <span class="orange">o</span>perator</h2>
<ul>
<li>The <span class="blue">lasso</span> appeared in the highly influential paper of Tibshirani (1996). It is a method that performs both <span class="blue">shrinkage</span> and <span class="orange">variable selection</span></li>
</ul>
<ul>
<li>The lasso estimator is the <span class="orange">minimizer</span> of the following system <span class="math display">
\sum_{i=1}^n(y_i - \beta_0- \bm{x}_i^T\beta)^2 \qquad \text{subject to} \qquad \sum_{j=1}^p |\beta_j| \le s.
</span> therefore when the <span class="blue">complexity parameter</span> <span class="math inline">s</span> is small, the coefficients of <span class="math inline">\hat{\beta}_\text{lasso}</span> are <span class="orange">shrinked</span> and when <span class="math inline">s</span> is large enough <span class="math inline">\hat{\beta}_\text{lasso} = \hat{\beta}_\text{ols}</span>, as in ridge regression.</li>
</ul>
<ul>
<li>The lasso is <span class="orange">deceptively similar</span> to ridge. However, the change from a quadratic penalty to the absolute value has crucial <span class="blue">sparsity</span> implications.</li>
</ul>
<ul>
<li>The intercept term <span class="math inline">\beta_0</span> is <span class="orange">not penalized</span>, as for ridge, because we can remove it by centering the predictors.</li>
</ul>
<!-- ## Centering and scaling the predictors I -->
<!-- -   The lasso solutions are [not equivariant]{.orange} under [scalings -->
<!--     of the input]{.orange}, so one normally [standardizes]{.blue} the -->
<!--     input to have unit variance, if they are not in the same scale. -->
<!-- . . . -->
<!-- - Moreover, as for PCR, we can estimate the intercept using a [two-step procedure]{.orange}: -->
<!--   - The reparametrization $\alpha = \beta_0 + \bar{\bm{x}}^T\beta$ is equivalent to centering the predictors; -->
<!--   - The estimate for the centered intercept is $\hat{\alpha} = \bar{y}$; -->
<!--   - The ridge estimate can be obtained considering a model without intercept, using centered responses and predictors.  -->
<!-- . . . -->
<!-- - Hence, the [original data]{.orange} $Y_i = f(\bm{x}_i) + \epsilon_i$ are [standardized]{.blue}, using the "[*]{.blue}" to denote it: $$  -->
<!-- x_{ij} = \frac{x_{ij} - \bar{x}_j}{s_j}, \qquad y_{i} = y_i - \bar{y}, \qquad i=1,\dots,n; \ \ j=1,\dots,p. -->
<!-- $$ -->
<!-- where $s_j^2 = n^{-1}\sum_{i=1}^n (x_{ij} - \bar{x}_j)^2$ is the [sample variance]{.blue}. -->
</section>
<section id="centering-and-scaling-the-predictors" class="level2">
<h2 class="anchored" data-anchor-id="centering-and-scaling-the-predictors">Centering and scaling the predictors</h2>
<ul>
<li><p>Thus, as for ridge regression, we will center and scale predictors and response.</p></li>
<li><p>It is easy to show that the <span class="orange">coefficients</span> expressed in the <span class="blue">original scale</span> are <span class="math display">
\hat{\beta}_0 = \bar{y} - \bar{\bm{x}}\hat{\beta}_\text{lasso}, \qquad \hat{\beta}_\text{scaled-lasso} = \text{diag}(1 / s_1,\dots, 1/s_p) \hat{\beta}_\text{lasso}.
</span> Thus, the <span class="orange">predictions</span> on the <span class="blue">original scale</span> are <span class="math inline">\hat{\beta}_0 + \bm{x}_i^T\hat{\beta}_\text{scaled-lasso} = \bar{y} + \bm{x}_{i}^T\hat{\beta}_\text{lasso}</span>.</p></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>For lasso problems we will assume the data have been previously <span class="orange">standardized</span>, namely <span class="math display">
    \frac{1}{n}\sum_{i=1}^ny_{i} = 0, \qquad \frac{1}{n}\sum_{i=1}^nx_{ij} = 0, \qquad \frac{1}{n}\sum_{i=1}^n x_{i}^2 = 1\qquad j=1,\dots,p.
    </span></p>
</div>
</div>
</div>
<ul>
<li>We will say that the <span class="blue">lasso estimator</span> <span class="math inline">\hat{\beta}_\text{lasso}</span> is the <span class="orange">minimizer</span> of following system <span class="math display">
\sum_{i=1}^n(y_i - \bm{x}_{i}^T\beta)^2 \qquad \text{subject to} \qquad \sum_{j=1}^p| \beta_j| \le s.
</span></li>
</ul>
</section>
<section id="lagrange-multipliers-and-lasso-solution" class="level2">
<h2 class="anchored" data-anchor-id="lagrange-multipliers-and-lasso-solution">Lagrange multipliers and lasso solution</h2>
<ul>
<li><p>The lasso problem can be <span class="blue">equivalently expressed</span> in its Lagrangian form, which is more amenable for computations.</p></li>
<li><p>Having remove the intercept, the lasso estimator <span class="math inline">\hat{\beta}_\text{lasso}</span> is the <span class="orange">minimizer</span> of <span class="math display">
  \underbrace{\textcolor{darkblue}{\frac{1}{2 n}}\sum_{i=1}^n(y_{i} - \bm{x}_i^T\beta)^2}_{\text{least squares}} + \underbrace{\lambda \sum_{j=1}^p|\beta_j|}_{\text{\textcolor{red}{lasso penalty}}}
  </span> where <span class="math inline">\lambda &gt; 0</span> is a <span class="blue">complexity parameter</span> controlling the <span class="orange">penalty</span>.</p></li>
</ul>
<ul>
<li><p>When <span class="math inline">\lambda = 0</span> the penalty term disappears and <span class="math inline">\hat{\beta}_\text{lasso} = \hat{\beta}_\text{ols}</span>. On the other hand, there exists a finite value of <span class="math inline">\lambda_0 &lt; \infty</span> such that <span class="math inline">\hat{\beta}_\text{lasso} = 0</span>.</p></li>
<li><p>For any intermediate value <span class="math inline">0 &lt; \lambda &lt; \lambda_0</span> we get a combination of <span class="orange">shrinked</span> but positive coefficients, and a set of coefficients whose value is <span class="blue">exactly zero</span>.</p></li>
</ul>
<ul>
<li>Unfortunately, there is <span class="orange">no closed-form expression</span> for the lasso solution.</li>
</ul>
</section>
<section id="the-geometry-of-the-lasso-solution" class="level2">
<h2 class="anchored" data-anchor-id="the-geometry-of-the-lasso-solution">The geometry of the lasso solution</h2>
<div class="cell" data-hash="un_C_cache/html/unnamed-chunk-28_6ccf19d6893a54497d67e1752d65b5ef">
<div class="cell-output-display">
<p><img src="un_C_files/figure-html/unnamed-chunk-28-1.png" class="img-fluid" width="1400"></p>
</div>
</div>
</section>
<section id="lasso-with-a-single-predictor-i" class="level2">
<h2 class="anchored" data-anchor-id="lasso-with-a-single-predictor-i">Lasso with a single predictor I</h2>
<div class="incremental">
<ul class="incremental">
<li><p>To gain some understanding, let us consider the <span class="blue">single-predictor</span> scenario, in which<span class="math display">
\hat{\beta}_\text{lasso} = \arg\min_{\beta}\frac{1}{2n}\sum_{i=1}^n(y_{i} - x_{i}\beta)^2 + \lambda |\beta|.
  </span></p></li>
<li><p>This simple problem admits an <span class="orange">explicit expression</span> (see Exercises), which is <span class="math display">
\hat{\beta}_\text{lasso} = \begin{cases} \text{cov}(x,y) - \lambda, \qquad &amp;\text{if} \quad \text{cov}(x,y) &gt; \lambda \\
0 \qquad &amp;\text{if} \quad \text{cov}(x,y) \le \lambda\\
\text{cov}(x,y) + \lambda, \qquad &amp;\text{if} \quad \text{cov}(x,y) &lt; -\lambda \\
\end{cases}
</span></p></li>
<li><p>The above solution can be written as <span class="math inline">\hat{\beta}_\text{lasso} = \mathcal{S}_\lambda(\hat{\beta}_\text{ols})</span>, where <span class="math inline">\mathcal{S}_\lambda(x) = \text{sign}(x)(|x| - \lambda)_+</span> is the <span class="blue">soft-thresholding</span> operator and <span class="math inline">(\cdot)_+</span> is the <span class="orange">positive part</span> of a number (<code>pmax(0, x)</code>).</p></li>
<li><p>For <span class="orange">ridge regression</span> with one predictor we obtain, instead: <span class="math display">
\hat{\beta}_\text{ridge} = \frac{1}{\lambda + 1}\text{cov}(x,y) =\frac{1}{\lambda + 1}\hat{\beta}_\text{ols} = \frac{1}{\lambda + 1}\frac{1}{n}\sum_{i=1}^n x_{i}y_{i}.
</span></p></li>
</ul>
</div>
</section>
<section id="lasso-with-a-single-predictor-ii" class="level2">
<h2 class="anchored" data-anchor-id="lasso-with-a-single-predictor-ii">Lasso with a single predictor II</h2>
<div class="cell" data-hash="un_C_cache/html/unnamed-chunk-29_10d8f081755eefc71a0d8361c15c0150">
<div class="cell-output-display">
<p><img src="un_C_files/figure-html/unnamed-chunk-29-1.png" class="img-fluid" width="1400"></p>
</div>
</div>
</section>
<section id="soft-thresholding-and-lasso-solution" class="level2">
<h2 class="anchored" data-anchor-id="soft-thresholding-and-lasso-solution">Soft-thresholding and lasso solution</h2>
<ul>
<li>The single predictor special cases provides further intuition of why the lasso perform <span class="blue">variable selection</span> and <span class="orange">shrinkage</span>.</li>
</ul>
<ul>
<li><p>Ridge regression induce shrinkage in a <span class="orange">multiplicative</span> fashion and the regression coefficients reach zero as <span class="math inline">\lambda \rightarrow \infty</span>.</p></li>
<li><p>Conversely, lasso shrink the ordinary least squares in an <span class="blue">additive</span> manner, <span class="orange">truncating</span> them at <span class="orange">zero</span> after a certain threshold.</p></li>
</ul>
<ul>
<li>Even though we do not have a closed-form expression for the lasso solution <span class="math inline">\hat{\beta}_\text{lasso}</span> when the covariates <span class="math inline">p &gt; 1</span>, the main intuition is preserved: lasso induces <span class="orange">sparsity</span>!</li>
</ul>
</section>
<section id="the-lasso-path" class="level2">
<h2 class="anchored" data-anchor-id="the-lasso-path">The lasso path</h2>
<div class="cell" data-layout-align="center" data-hash="un_C_cache/html/unnamed-chunk-31_9c2be54f9b1aa243c1e894079aedbc13">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_C_files/figure-html/unnamed-chunk-31-1.png" class="img-fluid figure-img" width="1800"></p>
</figure>
</div>
</div>
</div>
<!-- ## Comments on the lasso path -->
<!-- -   As for ridge regression, the values of $\lambda$ are expressed in -->
<!--     somewhat [arbitrary scale]{.orange}. -->
<!-- -   There exist a [maximum value]{.blue} $\lambda_0$, after -->
<!--     which all the coefficients are zero: $$ -->
<!--     \lambda_0 = \max_{j}\left| \text{cov}(\tilde{\bm{x}}_j, \bm{y})\right| = \max_{j}\left| \frac{1}{n}\sum_{i=1}^n x_{ij} y_i\right|. -->
<!--     $$ -->
<!-- . . . -->
<!-- -   It is confirmed that the variable `lcavol` is arguably the most -->
<!--     important, followed by `lweight` and `svi`, being the last variables -->
<!--     to be set equal to zero. -->
<!-- . . . -->
<!-- -   The coefficient of `age`, `gleason` and `lcp` are negative and then -->
<!--     are set equal to zero for higher values of $\lambda$. Compare this -->
<!--     with the [ridge path]{.orange}. -->
</section>
<section id="least-angle-regression-i" class="level2">
<h2 class="anchored" data-anchor-id="least-angle-regression-i">Least angle regression I</h2>
<ul>
<li>Least angle regression (LAR) is a “<span class="orange">democratic</span>” version of <span class="blue">forward stepwise regression</span>.</li>
</ul>
<ul>
<li><p>Forward stepwise builds a model <span class="blue">sequentially</span>, adding one variable at a time. At each step, the best variable is included in the <span class="orange">active set</span> and then the least square fit is updated.</p></li>
<li><p>LAR uses a similar strategy, but any new variable contributes to the predictions only “as much” as it deserves.</p></li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Main result of LAR
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>The LAR algorithm provides a way to <span class="blue">compute</span> the entire <span class="orange">lasso path</span> efficiently at the cost of a full least-squares fit.</p></li>
<li><p>LAR sheds light on important <span class="orange">statistical aspects</span> of the lasso. A nice <span class="blue">LAR - lasso - boosting</span> relationship is established, which is computationally and conceptually useful.</p></li>
</ul>
</div>
</div>
</section>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section"></h2>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Least angle regression algorithm (LAR)
</div>
</div>
<div class="callout-body-container callout-body">
<div class="incremental">
<ol class="incremental" type="1">
<li><p>After centering and standardization, define the residuals <span class="math inline">\bm{r}_0 = \bm{y}</span> and let <span class="math inline">\hat{\beta}^{(0)} = 0</span>.</p></li>
<li><p>Find the predictor <span class="math inline">\tilde{\bm{x}}_j</span> <span class="blue">most correlated</span> with the residuals <span class="math inline">\bm{r}_0</span>, i.e.&nbsp;having the largest value for <span class="math inline">\text{cov}(\tilde{\bm{x}}_j, \bm{r}_0) = \text{cov}(\tilde{\bm{x}}_j, \bm{y})</span>. Call this value <span class="math inline">\lambda_0</span> and let <span class="math inline">\mathcal{A} = \{j\}</span> be the <span class="orange">active set</span>.</p>
<ol class="incremental" type="i">
<li><p><span class="blue">Move</span> <span class="math inline">\beta_j(\lambda)</span> from <span class="math inline">\hat{\beta}_j^{(0)} = 0</span> <span class="blue">towards</span> its <span class="orange">least squares solution</span> by decreasing <span class="math inline">\lambda</span>, i.e.&nbsp;<span class="math display">
  \beta_j(\lambda) = \frac{\lambda_0 - \lambda}{\lambda_0} \text{cov}(\tilde{\bm{x}}_j, \bm{y}), \qquad 0 &lt; \lambda \le \lambda_0,
  </span> keeping track of the residuals <span class="math inline">\bm{r}(\lambda) = \bm{y} - \tilde{\bm{x}}_j\beta_j(\lambda)</span>. It can be shown that <span class="math display">
  |\text{cov}(\tilde{\bm{x}}_j, \bm{r}(\lambda))| = \lambda.
  </span></p></li>
<li><p>Identify the value <span class="math inline">\lambda &gt; 0</span> such that <span class="blue">another variable</span> <span class="math inline">\bm{x}_{\ell}</span> has <span class="orange">as much correlation</span> with the residuals as <span class="math inline">\bm{x}_{j}</span>. Call this value <span class="math inline">\lambda_1</span>, obtaining: <span class="math inline">|\text{cov}(\tilde{\bm{x}}_{\ell}, \bm{r}(\lambda_1))| = \lambda_1</span>.</p></li>
<li><p>Obtain the <span class="blue">estimate</span> <span class="math inline">\hat{\beta}^{(1)} = (0,\dots,\beta_j(\lambda_1), \dots, 0)</span> and set <span class="math inline">\bm{r}_1 = \bm{r}(\lambda_1)</span>. Define the new <span class="orange">active set</span> <span class="math inline">\mathcal{A} = \{j, \ell\}</span> and let <span class="math inline">\bm{X}_\mathcal{A}</span> be the corresponding matrix.</p></li>
</ol></li>
</ol>
</div>
</div>
</div>
</section>
<section id="section-1" class="level2">
<h2 class="anchored" data-anchor-id="section-1"></h2>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Least angle regression algorithm (LAR)
</div>
</div>
<div class="callout-body-container callout-body">
<div class="incremental">
<ol class="incremental" start="3" type="1">
<li><p>For <span class="math inline">k =2,\dots,K = \min(n-1,p)</span>, do:</p>
<ol class="incremental" type="i">
<li><p><span class="blue">Move</span> the coefficients <span class="math inline">\beta_\mathcal{A}(\lambda)</span> from <span class="math inline">\hat{\beta}_\mathcal{A}^{(k-1)}</span> <span class="blue">towards</span> their <span class="orange">least squares solution</span>: <span class="math display">
\beta_\mathcal{A}(\lambda) = \hat{\beta}_\mathcal{A}^{(k-1)} + \frac{\lambda_{k-1} - \lambda}{\lambda_{k-1}}(\bm{X}_\mathcal{A}^T\bm{X}_\mathcal{A})^{-1}\bm{X}_\mathcal{A}^T\bm{r}_{k-1}, \qquad 0 &lt; \lambda \le \lambda_{k-1},
</span> keeping track of <span class="math inline">\bm{r}(\lambda) = \bm{y} - \bm{X}_\mathcal{A}\beta_\mathcal{A}(\lambda)</span>. The covariances with the residuals are <span class="orange">tied</span>: <span class="math display">
|\text{cov}(\tilde{\bm{x}}_j, \bm{r}(\lambda))| = \lambda, \qquad j \in \mathcal{A}.
</span></p></li>
<li><p>Identify the largest value <span class="math inline">\lambda &gt; 0</span> such that <span class="blue">another variable</span> <span class="math inline">\bm{x}_{\ell}</span> has <span class="orange">as much correlation</span> with the residuals. Call this value <span class="math inline">\lambda_k</span>, so that <span class="math inline">|\text{cov}(\tilde{\bm{x}}_{\ell}, \bm{r}(\lambda_k))| = \lambda_k</span>.</p></li>
<li><p>Set the <span class="blue">estimate</span> <span class="math inline">\hat{\beta}^{(k)}</span> with entries <span class="math inline">\hat{\beta}_\mathcal{A}^{(k)} = \beta_\mathcal{A}(\lambda_k)</span> and zero otherwise. Let <span class="math inline">\bm{r}_k = \bm{r}(\lambda_k)</span>. Define the new <span class="orange">active set</span> <span class="math inline">\mathcal{A} \leftarrow \mathcal{A} \cup \{\ell \}</span> and design matrix <span class="math inline">\bm{X}_\mathcal{A}</span>.</p></li>
</ol></li>
<li><p>Return the pairs <span class="math inline">\{\lambda_k, \hat{\beta}^{(k)}\}_0^K</span>.</p></li>
</ol>
</div>
</div>
</div>
</section>
<section id="least-angle-regression-remarks" class="level2">
<h2 class="anchored" data-anchor-id="least-angle-regression-remarks">Least angle regression: remarks</h2>
<ul>
<li>The coefficients in LAR change in a <span class="orange">piecewise</span> fashion, with knots in <span class="math inline">\lambda_k</span>. The LAR path coincides <span class="blue">almost always</span> with the lasso. Otherwise, a simple modification is required:</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
LAR: lasso modification
</div>
</div>
<div class="callout-body-container callout-body">
<p>3.ii+. If a nonzero coefficient crosses zero before the next variable enters, drop it from <span class="math inline">\mathcal{A}</span> and recompute the joint least-squares direction using the reduced set.</p>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical details
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>In <span class="blue">Step 3.ii</span>, we do <span class="orange">not</span> take small steps and then recheck the covariances. Instead, the new variable <span class="math inline">\bm{x}_\ell</span> “catching up” and the value <span class="math inline">\lambda_k</span> can be identified with some algebra.</p></li>
<li><p>The LAR algorithm is <span class="orange">extremely efficient</span>, requiring the same order of computation of least squares. The main bottleneck is <span class="blue">Step 3.i</span>, but <span class="blue">QR decomposition</span> can be exploited.</p></li>
</ul>
</div>
</div>
</section>
<section id="lasso-and-lar-relationship" class="level2">
<h2 class="anchored" data-anchor-id="lasso-and-lar-relationship">☠️ - Lasso and LAR relationship</h2>
<ul>
<li>What follows is <span class="orange">heuristic intuition</span> for why LAR and lasso are so similar. By construction, at any stage of the LAR algorithm, we have that: <span class="math display">
\text{cov}(\tilde{\bm{x}}_j, \bm{r}(\lambda)) = \frac{1}{n}\sum_{i=1}^nx_{ij}\{y_i - \bm{x}_i^T\beta(\lambda)\} = \lambda s_j, \qquad j \in \mathcal{A},
</span> where <span class="math inline">s_j \in \{-1, 1\}</span> indicates the <span class="blue">sign of the covariance</span>.</li>
</ul>
<ul>
<li>On the other hand, let <span class="math inline">\mathcal{A}_\text{lasso}</span> be the active set of the lasso. For these variables, the penalized lasso loss is differentiable, obtaining: <span class="math display">
\text{cov}(\tilde{\bm{x}}_j, \bm{r}(\lambda)) = \frac{1}{n}\sum_{i=1}^nx_{ij}\{y_i - \bm{x}_i^T\beta(\lambda)\} = \lambda \text{sign}(\beta_j), \qquad j \in \mathcal{A}_\text{lasso},
</span> which <span class="orange">coincide</span> with the LAR solution if <span class="math inline">s_j = \text{sign}(\beta_j)</span>, which is <span class="blue">almost always the case</span>.</li>
</ul>
</section>
<section id="uniqueness-of-the-lasso-solution" class="level2">
<h2 class="anchored" data-anchor-id="uniqueness-of-the-lasso-solution">Uniqueness of the lasso solution</h2>
<ul>
<li>The lasso can be computed even when <span class="math inline">p &gt; n</span>. In these cases, will it be <span class="orange">unique</span>?</li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Three uniqueness results (Tibshirani, 2013)
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>If <span class="math inline">\bm{X}</span> has <span class="blue">full rank</span> <span class="math inline">\text{rk}(\bm{X}) = p</span>, which implies <span class="math inline">p \le n</span>, then <span class="math inline">\hat{\beta}_\text{lasso}</span> is uniquely determined.</p></li>
<li><p>If <span class="orange">all</span> the values of <span class="math inline">\bm{X}</span> are <span class="orange">different</span>, then <span class="math inline">\hat{\beta}_\text{lasso}</span> is uniquely determined, even when <span class="math inline">p &gt; n</span>.</p></li>
<li><p>The <span class="blue">predictions</span> <span class="math inline">\bm{X}\hat{\beta}_\text{lasso}</span> are always uniquely determined.</p></li>
</ul>
</div>
</div>
<ul>
<li><p>Non-uniqueness may occur in presence of <span class="orange">discrete-valued</span> data. It is of practical concern only whenever <span class="math inline">p &gt; n</span> and if we are interested in interpreting the coefficients.</p></li>
<li><p>Much more <span class="blue">general</span> sufficient conditions for the uniqueness of <span class="math inline">\hat{\beta}_\text{lasso}</span> are known, but they are quite technical and hard to check in practice.</p></li>
</ul>
</section>
<section id="the-degrees-of-freedom-of-the-lasso" class="level2">
<h2 class="anchored" data-anchor-id="the-degrees-of-freedom-of-the-lasso">The degrees of freedom of the lasso</h2>
<ul>
<li>In ridge regression, the <span class="blue">effective degrees of freedom</span> have a simple formula.</li>
</ul>
<ul>
<li><span class="orange">Miraculously</span>, for the lasso with a fixed penalty parameter <span class="math inline">\lambda</span>, the number of nonzero coefficients <span class="math inline">|\mathcal{A}_\text{lasso}(\lambda)|</span> is an <span class="blue">unbiased estimate</span> of the degrees of freedom.</li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Degrees of freedom (Zhou, Hastie and Tibshirani, 2007, Tibshirani and Taylor, 2012)
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Suppose <span class="math inline">\bm{X}</span> has <span class="orange">full rank</span> <span class="math inline">\text{rk}(\bm{X}) = p</span> and <span class="math inline">\bm{y}</span> follows a Gaussian law. Then: <span class="math display">
\text{df}_\text{lasso} = 1 + \mathbb{E}|\mathcal{A}_\text{lasso}(\lambda)|.
</span></p></li>
<li><p>Under further regularity conditions, the above relationship is <span class="blue">exact</span> if we consider the <span class="orange">LAR active set</span>, therefore implicitly using a different set of <span class="math inline">\lambda</span> values for any fit: <span class="math display">
\text{df}_\text{lar} = 1 + |\mathcal{A}|.
</span></p></li>
</ul>
</div>
</div>
</section>
<section id="effective-degrees-of-freedom-of-lar-and-best-subset" class="level2">
<h2 class="anchored" data-anchor-id="effective-degrees-of-freedom-of-lar-and-best-subset">☠️ - Effective degrees of freedom of LAR and best subset</h2>
<div class="cell" data-layout-align="center" data-hash="un_C_cache/html/unnamed-chunk-33_9e7613c6c680ba6d509d96ab275278c2">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_C_files/figure-html/unnamed-chunk-33-1.png" class="img-fluid figure-img" width="1800"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="cross-validation-for-lasso" class="level2">
<h2 class="anchored" data-anchor-id="cross-validation-for-lasso">Cross-validation for lasso</h2>
<div class="cell" data-layout-align="center" data-hash="un_C_cache/html/unnamed-chunk-35_51f5eec32431cbab5df390197bf05909">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_C_files/figure-html/unnamed-chunk-35-1.png" class="img-fluid figure-img" width="2000"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="the-lar-lasso-estimate" class="level2">
<h2 class="anchored" data-anchor-id="the-lar-lasso-estimate">The LAR (lasso) estimate</h2>
<div class="cell" data-layout-align="center" data-hash="un_C_cache/html/unnamed-chunk-37_01571b78dba2495ca6d56929ffb072d1">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_C_files/figure-html/unnamed-chunk-37-1.png" class="img-fluid figure-img" width="1800"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="other-properties-of-lar-and-lasso" class="level2">
<h2 class="anchored" data-anchor-id="other-properties-of-lar-and-lasso">Other properties of LAR and lasso</h2>
<div class="incremental">
<ul class="incremental">
<li><p><span class="orange">Bayesian interpretation</span>: the penalty can be interpreted as a Laplace prior on <span class="math inline">\beta</span>.</p></li>
<li><p>As mentioned, under certain conditions the LAR algorithm can be seen as the limiting case of <span class="blue">boosting procedure</span>, in which small corrections to predictions are iteratively performed.</p></li>
<li><p>The <span class="orange">nonnegative garrote</span> (Breiman, 1995) is a two-stage procedure, with a close relationship to the lasso. Breiman’s paper was the inspiration for Tibshirani (1996).</p></li>
<li><p>There is a large body of theoretical work on the behavior of the lasso, focused on:</p>
<ul class="incremental">
<li>the mean-squared-error consistency of the lasso;</li>
<li>the recovery of the nonzero support set of the true regression parameters, sometimes called <span class="orange">sparsistency</span>.</li>
</ul></li>
<li><p>The interested reader work may have a look at the (very technical) Chapter 11 of Hastie, Tibshirani and Wainwright (2015)</p></li>
</ul>
</div>
</section>
<section id="summary-of-lars-and-lasso" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-lars-and-lasso">Summary of LARS and lasso</h2>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pros
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>LAR and Lasso are extremely efficient approaches that perform both <span class="blue">variable selection</span> and <span class="orange">shrinkage</span> at the same time.</p></li>
<li><p>Lasso produces a <span class="blue">parsimonious</span> model.</p></li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cons
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Lasso can be applied when <span class="math inline">p &gt; n</span>, but there might be <span class="orange">uniqueness</span> issues. Moreover, the lasso selects at most <span class="math inline">n</span> variables.</p></li>
<li><p>If there is a group of variables with high pairwise correlations, the lasso tends to “randomly” select only one variable from the group.</p></li>
<li><p>When <span class="math inline">p &lt; n</span>, if there are <span class="orange">high correlations</span> between predictors, it has been empirically observed that the prediction performance of the lasso is dominated by ridge regression.</p></li>
</ul>
</div>
</div>
</section>
<section id="the-prostate-dataset.-a-summary-of-the-estimates" class="level2">
<h2 class="anchored" data-anchor-id="the-prostate-dataset.-a-summary-of-the-estimates">The <code>prostate</code> dataset. A summary of the estimates</h2>
<div style="font-size: 70%;">
<div class="cell" data-hash="un_C_cache/html/unnamed-chunk-38_6932a7a0e7fb3c9071d4c300d88738f0">
<div class="cell-output-display">

<div class="datatables html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-244140500d26ffd552d8" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-244140500d26ffd552d8">{"x":{"filter":"none","vertical":false,"data":[["(Intercept)","lcavol","lweight","age","lbph","svi","lcp","gleason","pgg45"],[2.464932922123746,0.6795281412379752,0.2630530657325436,-0.141464833536172,0.2101465572218274,0.3052005971250976,-0.2884927724535462,-0.02130503880294779,0.2669557621199234],[2.477357342449561,0.7397136698758429,0.3163281888271424,0,0,0,0,0,0],[2.455021592228733,0.2866612827183799,0.3391036880563064,0.05628528949356439,0.1015283806908212,0.2614850538133575,0.2186806183758072,-0.01605594153820449,0.06170971163528302],[2.466944796713691,0.5882288487041071,0.2581760179326364,-0.1128882084901707,0.2011682585074609,0.2832605085528289,-0.1721357517114825,0.01032273985298153,0.2040346251751614],[2.468314602273341,0.5321289780171822,0.1687891416769021,0,0,0.09162985645647548,0,0,0]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>OLS<\/th>\n      <th>Best subset<\/th>\n      <th>PCR<\/th>\n      <th>Ridge<\/th>\n      <th>Lasso<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":9,"dom":"t","columnDefs":[{"targets":1,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 3, 3, \",\", \".\", null);\n  }"},{"targets":2,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 3, 3, \",\", \".\", null);\n  }"},{"targets":3,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 3, 3, \",\", \".\", null);\n  }"},{"targets":4,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 3, 3, \",\", \".\", null);\n  }"},{"targets":5,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 3, 3, \",\", \".\", null);\n  }"},{"className":"dt-right","targets":[1,2,3,4,5]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[9,10,25,50,100],"rowCallback":"function(row, data, displayNum, displayIndex, dataIndex) {\nvar value=data[0]; $(this.api().cell(row, 0).node()).css({'font-weight':'bold'});\nvar value=data[1]; $(this.api().cell(row, 1).node()).css({'background-color':isNaN(parseFloat(value)) ? '' : value <= 0 ? \"#FED8B1\" : \"#DBE9FA\"});\nvar value=data[2]; $(this.api().cell(row, 2).node()).css({'background-color':isNaN(parseFloat(value)) ? '' : value <= 0 ? \"#FED8B1\" : \"#DBE9FA\"});\nvar value=data[3]; $(this.api().cell(row, 3).node()).css({'background-color':isNaN(parseFloat(value)) ? '' : value <= 0 ? \"#FED8B1\" : \"#DBE9FA\"});\nvar value=data[4]; $(this.api().cell(row, 4).node()).css({'background-color':isNaN(parseFloat(value)) ? '' : value <= 0 ? \"#FED8B1\" : \"#DBE9FA\"});\nvar value=data[5]; $(this.api().cell(row, 5).node()).css({'background-color':isNaN(parseFloat(value)) ? '' : value <= 0 ? \"#FED8B1\" : \"#DBE9FA\"});\nvar value=data[1]; $(this.api().cell(row, 1).node()).css({'background-color':value == 0 ? \"white\" : null});\nvar value=data[2]; $(this.api().cell(row, 2).node()).css({'background-color':value == 0 ? \"white\" : null});\nvar value=data[3]; $(this.api().cell(row, 3).node()).css({'background-color':value == 0 ? \"white\" : null});\nvar value=data[4]; $(this.api().cell(row, 4).node()).css({'background-color':value == 0 ? \"white\" : null});\nvar value=data[5]; $(this.api().cell(row, 5).node()).css({'background-color':value == 0 ? \"white\" : null});\n}"}},"evals":["options.columnDefs.0.render","options.columnDefs.1.render","options.columnDefs.2.render","options.columnDefs.3.render","options.columnDefs.4.render","options.rowCallback"],"jsHooks":[]}</script>
</div>
</div>
</div>
</section>
<section id="the-results-on-the-test-set" class="level2">
<h2 class="anchored" data-anchor-id="the-results-on-the-test-set">The results on the test set</h2>
<ul>
<li><p>At the beginning of this unit, we split the data into <span class="blue">training</span> set and <span class="orange">test</span> set. Using the training, we selected <span class="math inline">\lambda</span> via cross-validation or the <span class="math inline">C_p</span> index.</p></li>
<li><p>Using final test set with <span class="math inline">30</span> observations, we will assess which model is <span class="orange">preferable</span>.</p></li>
</ul>
<div class="cell" data-hash="un_C_cache/html/unnamed-chunk-39_2effd04705e71d1bf61ce12b5ea1a1e3">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">OLS</th>
<th style="text-align: right;">Best subset</th>
<th style="text-align: right;">PCR</th>
<th style="text-align: right;">Ridge</th>
<th style="text-align: right;">Lasso</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Test error (MSE)</td>
<td style="text-align: right;">0.521</td>
<td style="text-align: right;">0.492</td>
<td style="text-align: right;">0.496</td>
<td style="text-align: right;">0.496</td>
<td style="text-align: right;">0.48</td>
</tr>
</tbody>
</table>
</div>
</div>
<ul>
<li><p>All the approaches presented in this unit have better performance than ordinary least squares.</p></li>
<li><p>The <span class="blue">lasso</span> is the approach with <span class="orange">lowest mean squared error</span>. At the same time, it is also a <span class="blue">parsimonious</span> choice.</p></li>
<li><p>Best subset is the second best, doing a good job in this example… but here <span class="math inline">p = 8</span>, so there were no computational difficulties!</p></li>
</ul>
</section>
</section>
<section id="elastic-net-and-pathwise-algorithms" class="level1">
<h1>Elastic-net and pathwise algorithms</h1>
<section id="elastic-net" class="level2">
<h2 class="anchored" data-anchor-id="elastic-net">Elastic-net</h2>
<ul>
<li>The <span class="blue">elastic-net</span> is a compromise between ridge and lasso. It selects variables like the lasso, and shrinks together the coefficients of correlated predictors like ridge.</li>
</ul>
<ul>
<li>Having removed the intercept, the elastic-net estimator <span class="math inline">\hat{\beta}_\text{en}</span> is the <span class="orange">minimizer</span> of: <span class="math display">
\frac{1}{2n}\sum_{i=1}^n(y_i - \bm{x}_i^T\beta)^2 + \lambda\sum_{j=1}^p\left(\alpha  |\beta_j| + \frac{(1 - \alpha)}{2}\beta_j^2\right),
</span> where <span class="math inline">0 &lt; \alpha &lt; 1</span> and the <span class="orange">complexity parameter</span> <span class="math inline">\lambda &gt; 0</span>.</li>
</ul>
<ul>
<li><p><span class="blue">Ridge regression</span> is a special case, when <span class="math inline">\alpha = 0</span>. <span class="blue">Lasso</span> is also a special case, when <span class="math inline">\alpha = 1</span>.</p></li>
<li><p>It is often <span class="orange">not worthwhile</span> to estimate <span class="math inline">\alpha</span> using cross-validation. A typical choice is <span class="math inline">\alpha = 0.5</span>.</p></li>
</ul>
<ul>
<li><p>An advantage of the elastic-net is that it has a <span class="orange">unique solution</span>, even when <span class="math inline">p &gt; n</span>.</p></li>
<li><p>Another nice property, shared by ridge, is that whenever <span class="math inline">\tilde{\bm{x}}_j = \tilde{\bm{x}}_\ell</span>, then <span class="math inline">\hat{\beta}_{j,\text{en}} = \hat{\beta}_{\ell,\text{en}}</span>. On the other hand, the <span class="orange">lasso</span> estimator would be <span class="orange">undefined</span>.</p></li>
</ul>
</section>
<section id="convex-optimization" class="level2">
<h2 class="anchored" data-anchor-id="convex-optimization">Convex optimization</h2>
<ul>
<li>The estimators OLS, ridge, lasso and elastic-net have a huge <span class="orange">computational advantage</span> compared, e.g., to best subset: they are all <span class="blue">convex optimization problems</span>.</li>
</ul>
<ul>
<li>A function <span class="math inline">f: \mathbb{R}^p \rightarrow \mathbb{R}</span> is <span class="blue">convex</span> if for any values <span class="math inline">\bm{b}_1, \bm{b}_2 \in \mathbb{R}^p</span> and <span class="math inline">t \in [0, 1]</span> it holds that <span class="math display">
f(t \bm{b}_1 + (1-t)\bm{b}_2) \le  t f(\bm{b}_1) + (1-t) f(\bm{b}_2).
</span> Replacing <span class="math inline">\le</span> with <span class="math inline">&lt;</span> for <span class="math inline">t \in(0,1)</span> gives the definition of <span class="orange">strict convexity</span>.</li>
</ul>
<ul>
<li><span class="blue">OLS</span> and the <span class="blue">lasso</span> are, for a general design matrix <span class="math inline">\bm{X}</span>, convex problems. On the other hand, <span class="orange">ridge</span> and <span class="orange">elastic net</span> are strictly convex, as well as OLS and lasso when <span class="math inline">\text{rk}(\bm{X}) = p</span>.</li>
</ul>
<div class="callout callout-style-simple callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Properties of convex optimization
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>In a <span class="blue">convex</span> optimization problem every local minimum is a <span class="blue">global minimum</span>;</li>
<li>In a <span class="orange">strictly convex</span> optimization problem there exists a <span class="orange">unique</span> global minimum.</li>
</ul>
</div>
</div>
</section>
<section id="elastic-net-with-a-single-predictor" class="level2">
<h2 class="anchored" data-anchor-id="elastic-net-with-a-single-predictor">Elastic-net with a single predictor</h2>
<ul>
<li>The elastic-net estimate <span class="math inline">\hat{\beta}_\text{en}</span> is typically obtained through the <span class="blue">coordinate descent</span> algorithm, which works well here due <span class="blue">convexity</span> and the following <span class="orange">property</span>.</li>
</ul>
<ul>
<li>In the <span class="blue">single-predictor</span> scenario the elastic-net minimization problem simplifies to <span class="math display">
   \hat{\beta}_\text{en} = \arg\min_{\beta}\frac{1}{2n}\sum_{i=1}^n(y_{i} - x_{i}\beta)^2 + \lambda \left( \alpha |\beta| + \frac{(1-\alpha)}{2} \beta^2\right).
    </span></li>
</ul>
<ul>
<li>It can be shown that an <span class="orange">explicit expression</span> for <span class="math inline">\hat{\beta}_\text{en}</span> is available, which is <span class="math display">\hat{\beta}_\text{en} = \frac{1}{1 + (1 - \alpha)\lambda}\mathcal{S}_{\alpha\lambda}(\hat{\beta}_\text{ols}),</span> where <span class="math inline">\mathcal{S}_\lambda(x) = \text{sign}(x)(|x| - \lambda)_+</span> is the <span class="blue">soft-thresholding</span> operator and the <span class="orange">least square estimate</span> is <span class="math inline">\hat{\beta}_\text{ols} = n^{-1}\sum_{i=1}^n x_i y_i</span>.</li>
</ul>
</section>
<section id="coordinate-descent" class="level2">
<h2 class="anchored" data-anchor-id="coordinate-descent">Coordinate descent</h2>
<ul>
<li><p>The <span class="blue">coordinate descent</span> algorithm is based on a simple principle: optimize one coefficient (coordinate) at a time, keeping the others fixed.</p></li>
<li><p>We can <span class="orange">re-write</span> the objective function of the elastic-net in a more convenient form: <span class="math display">
\frac{1}{2n}\sum_{i=1}^n \left(\textcolor{red}{y_i - \sum_{j \neq k} x_{ij}\beta_j} - x_{ik} \beta_k\right)^2 + \lambda\left(\alpha  |\beta_k| + \frac{(1 - \alpha)}{2} \beta_k^2\right) + \underbrace{\lambda  \sum_{j\neq k}^p \{\alpha |\beta_j| + \frac{(1 - \alpha)}{2}\beta_j^2\}}_{\text{does not depend on } \beta_k},
</span></p></li>
</ul>
<ul>
<li>Let us define the <span class="blue">partial residuals</span> <span class="math inline">r_i^{(j)} = y_i - \sum_{j \neq k} x_{ij}\beta_j</span>. Then the updated <span class="math inline">\beta_k</span> is <span class="math display">
\beta_k \leftarrow \frac{1}{1 + (1 - \alpha)\lambda}\mathcal{S}_{\alpha\lambda}\left(\frac{1}{n}\sum_{i=1}^n x_{ik} r_i^{(j)}\right).
</span> We <span class="orange">cycle</span> this update for <span class="math inline">k=1,\dots,p</span>, over and over until convergence.</li>
</ul>
</section>
<section id="coordinate-descent---example" class="level2">
<h2 class="anchored" data-anchor-id="coordinate-descent---example">Coordinate descent - Example</h2>
<p><span class="orange">Objective function</span>: <span class="math inline">(1 - \beta_1 - 2 \beta_2)^2 + (3 - \beta_1 - 2 \beta_2)^2 + 5 (|\beta_1| + |\beta_2|)</span>.</p>
<div class="cell" data-hash="un_C_cache/html/unnamed-chunk-40_82774fba735af11ba8815bd065e5ab33">
<div class="cell-output-display">
<p><img src="un_C_files/figure-html/unnamed-chunk-40-1.png" class="img-fluid" width="1400"></p>
</div>
</div>
</section>
<section id="pathwise-coordinate-optimization" class="level2">
<h2 class="anchored" data-anchor-id="pathwise-coordinate-optimization">Pathwise coordinate optimization</h2>
<div class="incremental">
<ul class="incremental">
<li><p>The coordinate descent, in a regression model with elastic-net penalty, is <span class="orange">theoretically guaranteed</span> to reach global minimum.</p></li>
<li><p>The coordinate descent algorithm is implemented in the <code>glmnet</code> package. It is, de facto, the default algorithm for penalized generalized linear models.</p></li>
<li><p>The <code>glmnet</code> implementation is very efficient due to several additional tricks:</p>
<ul class="incremental">
<li>The <span class="blue">warm start</span>. The algorithm for <span class="math inline">\lambda_k</span> is initialized in the previously obtained solution using <span class="math inline">\lambda_{k-1}</span>.</li>
<li>Partial residuals can be efficiently obtained <span class="orange">without re-computing</span> the whole linear predictor.</li>
<li>The code is written in <span class="blue">Fortran</span>.</li>
<li>Many other additional tricks are employed: active set convergence, tools for sparse <span class="math inline">\bm{X}</span> matrices, etc.</li>
</ul></li>
<li><p>The same algorithm can be used to fit ridge regression and lasso as well, which is often convenient even though alternatives would be available.</p></li>
</ul>
</div>
</section>
</section>
<section id="generalized-linear-models" class="level1">
<h1>Generalized linear models</h1>
<section id="generalized-linear-models-1" class="level2">
<h2 class="anchored" data-anchor-id="generalized-linear-models-1">Generalized linear models</h2>
<ul>
<li>Almost everything we discussed in this unit for <span class="orange">regression</span> problems can be <span class="blue">extended to GLMs</span> and, in particular, to classification problems.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Best subset selection for GLMs
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Best subset selection, together with its forward and backward greedy approximations, are conceptually straightforward to extend to GLMs (using log-likelihood and ML).</p></li>
<li><p>The only difficulty is about computations, because leaps-and-bound approaches can not be applied here.</p></li>
</ul>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Principal components for GLMs
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Principal components can be applied to GLMs in a straightforward manner.</p></li>
<li><p>The shrinkage effect and their ability to control the variance remain unaltered, but the theory (e.g.&nbsp;variance of the estimator) holds only in an approximate sense.</p></li>
</ul>
</div>
</div>
</section>
<section id="shrinkage-methods-for-glms" class="level2">
<h2 class="anchored" data-anchor-id="shrinkage-methods-for-glms">Shrinkage methods for GLMs</h2>
<ul>
<li><p>Shrinkage methods such as ridge and lasso can be also generalized to GLMs.</p></li>
<li><p>The elastic-net approach, which covers ridge and lasso as special cases, becomes: <span class="math display">
(\hat{\beta}_0,\hat{\beta})= \arg\min_{(\beta_0, \beta)}\left\{ -\frac{1}{n}\ell(y_i; \beta_0 + \bm{x}_i^T\beta) + \lambda\sum_{j=1}^p\left(\alpha  |\beta_j| + \frac{(1 - \alpha)}{2}\beta_j^2\right)\right\},
</span> which is an instance of <span class="orange">penalized log-likelihood</span>.</p></li>
<li><p><span class="blue">Most of the properties</span> (e.g.&nbsp;ridge = variance reduction and shrinkage, lasso = variable selection) and other high-level considerations we made so far are <span class="blue">still valid</span>.</p></li>
<li><p>Computations are somewhat more cumbersome. The <code>glmnet</code> package provides the numerical routines for fitting this kind of model, using variants of <span class="blue">coordinate descent</span>.</p></li>
<li><p>The core idea is to obtain a <span class="blue">quadratic approximation</span> of the log-likelihood, as for IWLS. Then, the approximated loss becomes a (<span class="orange">weighted</span>) penalized regression problem.</p></li>
</ul>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<section id="references-i" class="level2">
<h2 class="anchored" data-anchor-id="references-i">References I</h2>
<ul>
<li><span class="blue">Main references</span>
<ul>
<li><strong>Chapter 3</strong> of Azzalini, A. and Scarpa, B. (2011), <a href="http://azzalini.stat.unipd.it/Book-DM/"><em>Data Analysis and Data Mining</em></a>, Oxford University Press.</li>
<li><strong>Chapter 3 and 4</strong> of Hastie, T., Tibshirani, R. and Friedman, J. (2009), <a href="https://hastie.su.domains/ElemStatLearn/"><em>The Elements of Statistical Learning</em></a>, Second Edition, Springer.</li>
<li><strong>Chapter 16</strong> of Efron, B. and Hastie, T. (2016), <a href="https://hastie.su.domains/CASI/"><em>Computer Age Statistical Inference</em></a>, Cambridge University Press.</li>
<li><strong>Chapter 2,3 and 5</strong> of Hastie, T., Tibshirani, R. and Wainwright, M. (2015). <a href="https://hastie.su.domains/StatLearnSparsity_files/SLS_corrected_1.4.16.pdf"><em>Statistical Learning with Sparsity: The Lasso and Generalizations</em></a>. CRC Press.</li>
</ul></li>
<li><span class="orange">Best subset selection</span>
<ul>
<li>Hastie, T., Tibshirani, R., and Tibshirani, R.J. (2020). Best subset, forward stepwise or lasso? Analysis and recommendations based on extensive comparisons. <em>Statistical Science</em> <strong>35</strong>(4): 579–592.</li>
</ul></li>
</ul>
</section>
<section id="references-ii" class="level2">
<h2 class="anchored" data-anchor-id="references-ii">References II</h2>
<ul>
<li><span class="blue">Ridge regression</span>
<ul>
<li>Hoerl, A. E., and Kennard, R. W. (1970). Ridge regression: biased estimation for nonorthogonal problems. <em>Technometrics</em> <strong>12</strong>(1), 55–67.</li>
<li>Hastie, T. (2020). Ridge regularization: an essential concept in data science. <em>Technometrics</em>, <strong>62</strong>(4), 426-433.</li>
</ul></li>
<li><span class="orange">Lasso</span>
<ul>
<li>Tibshirani, R. (1996). Regression selection and shrinkage via the lasso. <em>Journal of the Royal Statistical Society. Series B: Statistical Methodology</em>, <strong>58</strong>(1), 267-288.</li>
<li>Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. (2004). Least Angle Regression. <em>Annals of Statistics</em> <strong>32</strong>(2), 407-499.</li>
<li>Zou, H., Hastie, T., and Tibshirani, R. (2007). On the ‘degrees of freedom’ of the lasso. <em>Annals of Statistics</em> <strong>35</strong>(5), 2173-2192.</li>
<li>Tibshirani, R. J. (2013). The lasso problem and uniqueness. <em>Electronic Journal of Statistics</em> <strong>7</strong>(1), 1456-1490.</li>
</ul></li>
</ul>
</section>
<section id="references-iii" class="level2">
<h2 class="anchored" data-anchor-id="references-iii">References III</h2>
<ul>
<li><span class="blue">Elastic-net</span>
<ul>
<li>Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. <em>Journal of the Royal Statistical Society. Series B: Statistical Methodology</em> <strong>67</strong>(2), 301-320.</li>
</ul></li>
<li><span class="orange">Computations for penalized methods</span>
<ul>
<li>Friedman, J., Hastie, T., Höfling, H. and Tibshirani, R. (2007). Pathwise coordinate optimization. <em>The Annals of Applied Statistics</em> <strong>1</strong>(2), 302-32.</li>
<li>Tay, J. K., Narasimhan, B. and Hastie, T. (2023). Elastic net regularization paths for all generalized linear models. <em>Journal of Statistical Software</em> <strong>106</strong> (1).</li>
</ul></li>
</ul>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



<script src="un_C_files/libs/quarto-html/zenscroll-min.js"></script>
</body></html>