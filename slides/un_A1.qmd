---
title: "A-B-C"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Universit√† degli Studi di Milano-Bicocca_"
execute:
  cache: true
format:
  revealjs:
    auto-stretch: false
    html-math-method: katex
    transition: slide
    output-file: un_A1_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 300
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: true
    code-summary: "Show the code"
    fig-dpi: 300
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## About this unit

::: columns
::: {.column width="40%"}
![](img/ABC.png) *"Everything should be made as simple as possible, but
not simpler"* Attributed to Albert Einstein
:::

::: {.column width="60%"}
-   In this unit we will cover the following [topics]{.orange}:

    -   Linear models and the modelling process
    -   Cholesky factorization
    -   Orthogonalization and QR decomposition
    -   Iterative methods

-   The [computational aspects]{.blue} of linear models will be novel to
    most of you...

-   ... but you should be already [very familiar]{.orange} with linear
    models !

-   If do not remember much about them, use this first week of lectures
    to catch up (or study) from the material of previous courses.

<!-- -   A [short introduction]{.blue} to the topic is also offered in -->

<!--     Azzalini & Scarpa (2011), Chapter 2 and Appendix A.3. -->
:::
:::

# Old friends: linear models

## Car data

-   We consider data for $n = 203$ models of cars in circulation in 1985
    in the USA.
-   We want to identify a relationship that allows to [predict]{.blue}
    the distance covered per unit of fuel, as a function of the vehicle
    characteristics.
-   We consider the following [continuous variables]{.orange}:
    -   The city distance per unit of fuel (km/L, `city.distance`)
    -   The engine size (L, `engine.size`)
    -   The number of cylinders (`n.cylinders`)
    -   The curb weight (kg, `curb.weight`)
-   We also considered the [categorical variable]{.orange} fuel type
    (gasoline or diesel, `fuel`).

## Car data ([diesel]{.blue} or [gas]{.orange})

```{r, message=FALSE}
library(tidyverse)
library(ggplot2)
library(GGally)
library(ggthemes)

auto <- read.table("../data/auto.txt", header = TRUE) %>% select(city.distance, engine.size, n.cylinders, curb.weight, fuel)

p0 <- ggpairs(auto,
  columns = 1:4, aes(colour = fuel),
  lower = list(continuous = wrap("points", size = 0.9)),
  upper = list(continuous = wrap("points", size = 0.9)),
  diag = "blank"
) +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("") +
  ylab("")
p0
```

## Linear regression

-   At the moment, let us focus on `city.distance` ($y$), `engine.size`
    ($x$) and `fuel` ($z$).

-   The simplest model we can come up with is a [linear
    regression]{.orange} line: $$
    y_i = \beta_1 + \beta_2 x_i + \epsilon_i, \qquad i=1,\dots,n,
    $$ where the "errors" $\epsilon_i$ are iid random variables, having
    zero mean and variance $\sigma^2$.

. . .

-   We are looking for an estimate for the [unknown regression
    parameters]{.blue} $\beta_0$ and $\beta_1$.

-   Such an estimate could be obtained by ordinary least squares
    (OLS)...

. . .

-   ... but the next plot clearly suggests that the relationship between
    `city.distance` and `engine.size` is [not]{.orange} well
    approximated by a [linear]{.orange} function.

-   ... and also that `fuel` has an non-negligible effect on the
    response.

## Scatterplot of the data

```{r}
ggplot(data = auto, aes(x = engine.size, y = city.distance, col = fuel)) +
  geom_point() +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Engine size (L)") +
  ylab("Urban distance (km/L)")
```

## Regression models

::: incremental
-   A more [general formulation]{.orange} for modeling the relationship
    between a vector of covariates
    $(x_{i1},\dots,x_{ip})^T \in \mathbb{R}^p$ and a response
    $y \in \mathbb{R}$ is $$
    y_i = f(x_{i1},\dots,x_{ip}; \beta) + \epsilon_i, \qquad i=1,\dots,n.
    $$

-   To estimate the unknown parameters $\beta$, a possibility is to rely
    on the [least squares criterion]{.blue}: we seek the
    [minimum]{.orange} of the objective function $$
    D(\beta) = \sum_{i=1}^n\{y_i - f(x_{i1},\dots, x_{ip}; \beta)\}^2,
    $$ using $n$ pairs of observations $(x_{i1},\dots,x_{ip})^T$ and
    $y_i$, for $i = 1,\dots,n$.

-   The solution to this minimization problem is denoted by
    $\hat{\beta}$.

-   The [predicted values]{.blue} $\hat{y}_i$ are then obtained as
    $\hat{y}_i = f(x_{i1},\dots,x_{ip}; \hat{\beta})$, for
    $i=1,\dots,n.$
:::

## Linear models

-   We can more flexibly model `city.distance` ($y$), `engine.size`
    ($x$) and `fuel` ($z$).

-   For instance, we could consider a [polynomial term]{.orange}
    combined with a [dummy variable]{.blue} $$
    f(x, z; \beta) = \beta_1 + \beta_2 x + \beta_3 x^2 + \beta_4 x^3 + \beta_5 I(z = \texttt{gas}).
    $$

-   [Remark]{.orange}. This model is [linear in the parameters]{.blue},
    but it can capture non-linear patterns!

. . .

::: callout-note
#### Definition (Linear model)

In a [linear model]{.blue} the response variable $y$ is related to the
covariates through the function$$
    f(x_1,\dots,x_p; \beta) = \beta_1 x_1 + \cdots + \beta_p x_p = \tilde{\bm{x}}^T\beta,
    $$ where $\tilde{\bm{x}} = (x_1, \dots,x_p)^T$ is a vector of
[covariates]{.orange} and $\beta = (\beta_1,\dots,\beta_p)^T$ is the
corresponding vector of [coefficients]{.orange}.
:::

## Matrix notation

::: incremental
-   The [response values]{.orange} are collected in the vector
    $\bm{y} = (y_1,\dots,y_n)^T$.

-   The [design matrix]{.blue} is a $n \times p$ matrix, comprising the
    covariate's values, defined by $$
    \bm{X} = 
    \begin{bmatrix} 
    x_{11} & \cdots & x_{1p}\\
    \vdots & \ddots & \vdots \\
    x_{n1} & \cdots & x_{np}
    \end{bmatrix}.
    $$

-   The linear model can be written using the [compact
    notation]{.orange}: $$
    \bm{y} = \bm{X}\beta + \bm{\epsilon},
    $$ where $\bm{\epsilon} = (\epsilon_1,\dots,\epsilon_n)^T$ is a
    vector of iid error terms with zero mean and variance $\sigma^2$.

-   The $j$th variable (column) is denoted with $\bm{x}_j$, whereas the
    $i$th observation (row) is $\tilde{\bm{x}}_i$: $$
    \bm{X} = (\bm{x}_1,\dots,\bm{x}_p) = (\tilde{\bm{x}}_1, \dots,\tilde{\bm{x}}_n)^T.
    $$
:::

## Linear regression: estimation I

-   The optimal set of coefficients $\hat{\beta}$ is the minimizer of
    the least squared criterion $$
    D(\beta) = (\bm{y} - \bm{X}\beta)^T(\bm{y} - \bm{X}\beta) = ||\bm{y} - \bm{X}\beta||^2,
    $$ known also as [residual sum of squares (RSS)]{.orange}, where $$
    ||\bm{y}|| = \sqrt{y_1^2 + \cdots + y_n^2}.$$

::: callout-note
#### Least square estimate (OLS)

If the design matrix has [full rank]{.blue}, that is if
$\text{rk}(\bm{X}^T\bm{X}) = p$, then the [least square
estimate]{.orange} has an explicit solution: $$
    \hat{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T \bm{y}.
    $$
:::

## Linear regression: estimation II

::: incremental
-   Consequently, the predicted values are $$
    \hat{\bm{y}} = \bm{X}\hat{\beta} = \bm{H}\bm{y}, \qquad \bm{H} = \bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T.
    $$

-   $\bm{H}$ is a $n \times n$ [projection matrix]{.orange} matrix
    sometimes called [hat matrix]{.blue}.

-   It can be shown that $\text{tr}(\bm{H}) = \text{rk}(\bm{H}) = p$.
    Moreover, it holds $\bm{H} = \bm{H}^T$ and $\bm{H}^2 = \bm{H}$.

-   The quantity $D(\hat{\beta})$ is the [deviance]{.blue}, which is
    equal to $$
    D(\hat{\beta}) = ||\bm{y} - \hat{\bm{y}}||^2 = \bm{y}^T(I_n - \bm{H})\bm{y}.
    $$

-   A typical estimate for the [residual variance]{.orange} $\sigma^2$
    is then given by $$
    s^2 = \frac{D(\hat{\beta})}{n - p} = \frac{1}{n-p}\sum_{i=1}^n(y_i - \tilde{\bm{x}}_i^T\hat{\beta})^2.
    $$
:::

## Linear regression: inference

::: incremental
-   Let us additionally assume that the errors follow a Gaussian
    distribution:
    $\epsilon_i \overset{\text{iid}}{\sim} N(0, \sigma^2)$.

-   This implies that the distribution of the estimator $\hat{\beta}$ is
    $$
    \hat{\beta} \sim N_p(\beta, \sigma^2 (\bm{X}^T\bm{X})^{-1}).
    $$

-   Hence, the estimator $\hat{\beta}$ is [unbiased]{.orange} and its
    [variance]{.blue} can be estimated by $$
    \widehat{\text{var}}(\hat{\beta}) = s^2 (\bm{X}^T\bm{X})^{-1}.
    $$

-   The [standard errors]{.orange} of the components of beta correspond
    to the square root of the diagonal of the above covariance matrix.

-   Confidence interval and Wald's tests can be obtained through
    classical inferential theory.
:::

## Car data, a preliminary model

::: incremental
-   A first model to predict `city.distance` ($y$) via `engine.size`
    ($x$) and `fuel` ($z$) is $$
      f(x, z; \beta) = \beta_1 + \beta_2 x + \beta_3 x^2 + \beta_4 x^3 + \beta_5 I(z = \texttt{gas}).
      $$

-   By looking at the plot of the data, it is plausible that we need a
    [polynomial]{.orange} of degree $3$ or $4$

-   It is also clear from the plot that `fuel` is a relevant variable.
    Categorical variable should be [encoded]{.orange} using [indicator
    variables]{.blue}.

-   To evaluate the goodness of fit, we can calculate the [coefficient
    of determination]{.orange}: $$
    R^2 = 1 - \frac{\text{(``Residual deviance'')}}{\text{(``Total deviance'')}} = 1 - \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{\sum_{i=1}^n(y_i - \bar{y})^2}.
    $$
:::

## A first model: estimated coefficients

-   We obtain the following [summary]{.orange} for the regression
    coefficients $\hat{\beta}$.

```{r}
library(broom)
library(knitr)
m1 <- lm(city.distance ~ engine.size + I(engine.size^2) + I(engine.size^3) + fuel, data = auto)
kable(tidy(m1, conf.int = TRUE), digits = 3)
```

-   The coefficient $R^2$ and $s$ are estimated as follows:

```{r}
kable(glance(m1)[c(1, 3, 10)])
```

## A first model: fitted values

```{r}
augmented_m1 <- augment(m1)
ggplot(data = augmented_m1, aes(x = engine.size, y = city.distance, col = fuel)) +
  geom_point() +
  geom_line(aes(y = .fitted)) +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Engine size (L)") +
  ylab("Urban distance (km/L)")
```

## A first model: graphical diagnostics

```{r}
ggplot(data = augmented_m1, aes(x = .fitted, y = .resid, col = fuel)) +
  geom_point() +
  geom_hline(aes(yintercept = 0), linetype = "dotted") +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Fitted values") +
  ylab("Residuals")
```

## Comments and criticisms

::: incremental
-   The overall fit [seems satisfactory]{.blue} at first glance,
    especially if we aim at predicting the urban distance of cars when
    average engine size (i.e., between $1.5L$ and $3L$).

-   However, the plot of the [residuals]{.orange}
    $r_i = y_i - \hat{y}_i$ suggests that there the homoschedasticity
    assumption, i.e. $\text{var}(\epsilon_i) = \sigma^2$, might be
    violated.

-   Also, this model in not suitable for [extrapolation]{.orange}.
    Indeed:

-   It has no grounding in physics or engineering, which leads to
    difficulties in the interpretation of the trend and/or paradoxical
    situations.

-   For example, the curve of the set of gasoline cars shows a local
    minimum around $4.6 L$ and then rises again!
:::

## Variable transformation

-   A major advantage of linear models is that they can exploit
    non-linear relationship via [variable transformations]{.blue}.

-   This gives the statistician a lot of modelling flexibility, for
    instance:

$$
\log{y_i} = \beta_1 + \beta_2 \log{x_i} + \beta_3 I(z_i = \texttt{gas}) + \epsilon_i, \qquad i=1,\dots,n.
$$

. . .

-   This specification is [linear in the parameters]{.orange}, it fixes
    the domain issues, and imposes a monotone relationship between
    engine size and consumption.

```{r}
m2 <- lm(log(city.distance) ~ I(log(engine.size)) + fuel, data = auto)
kable(tidy(m2, conf.int = TRUE), digits = 3)
```

## Second model: fitted values

```{r}
augmented_m2 <- augment(m2, data = auto)
ggplot(data = augmented_m2, aes(x = engine.size, y = city.distance, col = fuel)) +
  geom_point() +
  geom_line(aes(y = exp(.fitted))) +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Engine size (L)") +
  ylab("Urban distance (km/L)")
```

## Second model: graphical diagnostics

```{r}
ggplot(data = augmented_m2, aes(x = .fitted, y = .resid, col = fuel)) +
  geom_point() +
  geom_hline(aes(yintercept = 0), linetype = "dotted") +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Fitted values") +
  ylab("Residuals")
```

## Comments and criticisms

-   The [goodness of fit]{.blue} indices are the following:

```{r}
r.squared.original <- 1 - sum(mean((auto$city.distance - exp(predict(m2)))^2)) / sum(mean((auto$city.distance - mean(auto$city.distance))^2))
kable(data.frame(r.squared.original = r.squared.original, glance(m2)[c(1, 3, 10)]))
```

-   [Apple]{.blue} and [oranges]{.orange}: the $R^2$ must be computed
    and compared using the same scale!

. . .

-   This second model is [more parsimonious]{.blue} and yet it reaches
    similar predictive performance.

-   It is also more coherent with the nature of the data: the
    predictions cannot be negative and the relationship between engine
    size and the consumption is monotone.

-   There is still some heteroschedasticity in the residuals --- is this
    due to a missing covariate that has not been included into the
    model?

## A third model: additional variables

-   Let us consider [two additional variables]{.orange}: `curb.weight`
    ($w$) and `n.cylinders` ($v$). A modified model could be: $$
    \log{y_i} = \beta_1 + \beta_2 \log{x_i} +  \beta_3 \log{w_i} + \beta_4 I(z_i = \texttt{gas}) + \beta_5 I(v_i = 2) + \epsilon_i,
    $$ for $i=1,\dots,n$.

. . .

```{r}
auto$cylinders2 <- factor(auto$n.cylinders == 2)
m3 <- lm(log(city.distance) ~ I(log(engine.size)) + I(log(curb.weight)) + fuel + cylinders2, data = auto)
kable(tidy(m3, conf.int = TRUE), digits = 3)
```

## A third model: graphical diagnostics

```{r}
augmented_m3 <- augment(m3, data = auto)
ggplot(data = augmented_m3, aes(x = .fitted, y = .resid, col = fuel)) +
  geom_point() +
  geom_hline(aes(yintercept = 0), linetype = "dotted") +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Fitted values") +
  ylab("Residuals")
```

## Comments and criticisms

-   The goodness of fit greatly [improved]{.blue}:

```{r}
r.squared.original <- 1 - sum(mean((auto$city.distance - exp(predict(m3)))^2)) / sum(mean((auto$city.distance - mean(auto$city.distance))^2))
kable(data.frame(r.squared.original = r.squared.original, glance(m3)[c(1, 3, 10)]))
```

-   We took care of the [outliers]{.orange}, which it turns out are
    identified by the group of cars having 2 cylinders.

-   The diagnostic plots are also very much improved, although still not
    perfect.

-   The estimates are coherent with our expectations, based on common
    knowledge. Have a look at the textbook (A&S) for a detailed
    explaination about $\beta_4$!

# Cholesky factorization

## How to obtain the least squares estimate?

-   In undergraduate courses it is often suggested that the least square
    estimate should be computed using the formula $$
    \hat{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T \bm{y},
    $$ that is, using the R code `solve(t(X) %*% X) %*% t(X) %*% y`.

-   This is [theoretically correct]{.orange} and it works reasonably
    well in many simple cases.

. . .

-   However, in realistic problems, when we have a lot of data (large
    $n$) and correlated variables, the above code is [computationally
    inefficient]{.orange} and [numerically inaccurate]{.blue}.

-   The main computational bottleneck is about obtaining the inverse of
    $\bm{X}^T\bm{X}$, which is very costly and often numerically
    unstable, especially when the predictors are almost collinear.

## The normal equations

-   Consider the following system of equations ([normal
    equations]{.orange}):$$
    \bm{X}^T\bm{X} \beta = \bm{X}^T \bm{y}.
    $$

-   This system could be solved using
    `solve(crossprod(X), crossprod(X, y))`.

-   The above R command avoids the computation of
    $(\bm{X}^T\bm{X})^{-1}$ and it is preferable compared to the "direct
    solution". However, it does not exploit the properties of the matrix
    $\bm{X}^T\bm{X}$.

. . .

::: callout-note
#### Proposition A.1

Suppose $\bm{X} \in \mathbb{R}^{n \times p}$ with $n \ge p$ has full
rank, that is $\text{rk}(\bm{X}) = p$. Then, the matrix $$
\bm{X}^T\bm{X}
$$ is [symmetric]{.blue} and [positive definite]{.orange}.
:::

## Cholesky factorization

-   Recall (from your favorite linear algebra textbook) that a
    [symmetric]{.blue} matrix $\bm{A} \in \mathbb{R}^{p \times p}$ is
    [positive definite]{.orange} if and only if one the following
    properties is satisfied
    -   The quadratic form $\bm{x}^T \bm{A} \bm{x} > 0$ for all
        $\bm{x} \in \mathbb{R}^p \neq 0$.
    -   The eigenvalues $\lambda_1,\dots,\lambda_p$ of $\bm{A}$ are all
        strictly positive.

::: callout-note
#### Theorem (Cholesky factorization)

Let $\bm{A} \in \mathbb{R}^{p \times p}$ be a symmetric and positive
definite matrix. Then, there exists a unique [upper triangular]{.orange}
$p \times p$ matrix $\bm{R}$ with positive entries such that $$
\bm{A} = \bm{R}^T\bm{R}.
$$
:::

## Cholesky factorization and least squares

::: incremental
-   Let $\bm{R}^T\bm{R}$ be the Cholesky factorization of the matrix
    $\bm{X}^T\bm{X}$. Then, the [normal equations]{.orange} can be
    written as $$
    \bm{R}^T\bm{R} \beta = \bm{X}^T \bm{y},
    $$ which can be can be solved in two steps.

-   [Step 1 (Forwardsolve)]{.blue}. Solve with respect to $z$ the system
    of equations $$
    \bm{R}^T z = \bm{X}^T \bm{y}.
    $$

-   [Step 2 (Backsolve)]{.blue}. Given $z$, now solve with respect to
    $\beta$ the system of equations $$
    \bm{R} \beta = z.
    $$
:::

## Forward and backward substitutions

::: incremental
-   The solution of [triangular systems]{.blue} is
    [straightforward]{.orange}.

-   As an example, consider the following $3 \times 3$ lower triangular
    system: $$
    \begin{bmatrix} 
    l_{11} & 0 & 0 \\
    l_{21} & l_{22} & 0 \\
    l_{31} & l_{32} & l_{33} \\
    \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix}
    b_1 \\ b_2 \\ b_3
    \end{bmatrix}.
    $$

-   The solution for $x_1,x_2,x_3$ can be found sequentially: $$
    x_1 = \frac{b_1}{l_{11}}, \qquad x_2 = \frac{b_2 -  l_{21}x_1}{l_{22}}, \qquad x_3 = \frac{b_3 - l_{31}x_1 - l_{32}x_2}{l_{33}}.
    $$

-   Finding the inverse $\bm{R}^{-1}$ is simple, again because $\bm{R}$
    is upper triangular. Also note that $$
    (\bm{X}^T \bm{X})^{-1} = (\bm{R}^T \bm{R})^{-1} = \bm{R}^{-1} (\bm{R}^{-1})^T.
    $$
:::

## Computational complexity

::: incremental
-   The solution via Cholesky factorization is a [fast direct
    approach]{.blue} for finding $\hat{\beta}$.

-   The expensive steps are:

    -   The formation of the matrix $\bm{X}^T\bm{X}$ requires
        $\sim n p^2$ elementary operations
    -   The Cholesky factorization of $\bm{X}^T\bm{X}$ requires
        $\sim p^3 / 3$ elementary operations.

-   This gives an overall computational complexity of order $$
    \sim n p^2 + p^3 /3,
    $$ which corrects the typographical error of the A&S textbook.

-   This means that in [high-dimensional]{.orange} settings (large $p$)
    computations become rapidly [very costly]{.orange}, since the
    complexity is cubic in $p$.
:::

## ‚ò†Ô∏è - Error propagation in normal equations

::: incremental
-   The normal equations method is typically [quicker]{.blue} than other
    algorithms, as it removes the dependency on $n$, but it is in
    general numerically more [unstable]{.orange}.

-   Consider for example the following matrix: $$
    \bm{X} = \begin{bmatrix}1 & 1 \\
    \epsilon & 0 \\
    0 & \epsilon \end{bmatrix},
    $$ for a small value $\epsilon > 0$. Then, we obtain that
    $$\bm{X}^T \bm{X} = \begin{bmatrix}1 + \epsilon^2& 1 \\
    1 & 1 + \epsilon^2 \end{bmatrix}.
    $$

-   The numerical computation of $\epsilon^2$ in $\bm{X}^T\bm{X}$
    requires a higher precision compared to $\epsilon$, leading to
    numerical instabilities and/or a [loss in accuracy]{.orange}.
:::

## ‚ò†Ô∏è - Condition numbers and normal equations

::: incremental
-   Suppose $\bm{X} \in \mathbb{R}^{n \times p}$ with $n \ge p$ has full
    rank and singular values $d_1 \ge d_2 \ge \dots \ge d_p$. Then its
    [condition number]{.orange} is $$
    \kappa(\bm{X}) = ||\bm{X}|| \cdot ||\bm{X}^+|| = \frac{d_1}{d_p},
    $$ where $\bm{X}^+$ is the Moore-Penrose pseudo-inverse. Note that
    $\kappa(\bm{X}) \ge 1$.

-   If $\kappa(\bm{X})$ is small, the matrix $\bm{X}$ is [well
    conditioned]{.blue}. Otherwise, we say it is [ill
    conditioned]{.orange}.

-   The condition number determines how accurately we can solve linear
    systems.

-   An important fact is: $$
    \kappa(\bm{X}^T\bm{X}) = \kappa(\bm{X})^2, 
    $$ implying that there is a clear loss of numerical accuracy when
    using normal equations.
:::

# The QR decomposition

## Orthogonal predictors

-   When the [predictors]{.blue} are mutually [orthogonal]{.orange}, the
    problem is significantly simpler.

-   In other words, consider a linear model of the form $$
    \bm{y} = \bm{Z}\bm{\beta} + \bm{\epsilon},
    $$ where $\bm{Z} = (\bm{z}_1,\dots,\bm{z}_p)$. [Orthogonality]{.orange} means that
    $\bm{Z}^T\bm{Z} = \text{diag}(\bm{z}_1^T\bm{z}_1,\dots,\bm{z}_p^T\bm{z}_p)$.

::: callout-note
#### Proposition A.2. OLS with orthogonal predictors

The least square estimate
$\hat{\bm{\beta}} = (\hat{\beta}_1,\dots,\hat{\beta}_p)^T$ of a
linear model with orthogonal predictors is

$$
\hat{\gamma}_j = \frac{\bm{z}_j^T\bm{y}}{\bm{z}_j^T\bm{z}_j}, \qquad j=1,\dots,p.
$$
:::

## Regression by successive orthogonalization

-   The predictors of $\bm{X}$ are generally not orthogonal. We
    want to find a suitable transformation $\bm{Z} = \bm{X} \bm{\Gamma}^{-1}$ that
    [orthogonalize]{.blue} the [predictors]{.orange}.

. . .

- Suppose for example that $p = 2$. We set [first orthogonal predictor]{.orange} $\bm{z}_1 = \bm{x}_1$.

- We then consider the following [univariate]{.blue} regression problem
$$
\bm{x}_2 = \gamma \bm{z}_1 + \bm{\epsilon}, \qquad \text{which leads} \qquad \hat{\gamma} = \frac{\bm{z}_1^T\bm{x}_2}{\bm{z}_1^T\bm{z}_1}.
$$
-  The [second orthogonal predictor]{.orange} is obtained as the [residual term]{.blue}: $$
\bm{z}_2 = \bm{x}_2 - \hat{\gamma}\bm{z}_1.
$$
- The geometry of linear models guarantees that $\bm{z}_1^T\bm{z}_2 = 0$.

## Gram-Schmidt algorithm

- Let us now consider the [general case]{.orange}, for any value of $p$.

- [Initialization]{.blue}. Set $\bm{z}_1 = \bm{x}_1$.

- [For $j= 2,\dots,p$]{.blue}. Consider the regression problem with $j-1$ orthogonal predictors
$$
\bm{x}_j = \sum_{k=1}^{j-1}\gamma_{kj} \bm{z}_k + \bm{\epsilon}_j, \quad \text{which leads} \quad \hat{\gamma}_{kj} = \frac{\bm{z}_k^T\bm{x}_j}{\bm{z}_k^T\bm{z}_k}, \quad k=1,\dots,j-1,
$$
Then, compute the new vector $\bm{z}_j$ as the [residual]{.orange} term
  $$
  \bm{z}_j = \bm{x}_j - \sum_{k=1}^{j-1}\hat{\gamma}_{kj} \bm{z}_k
  $$
- The geometry of linear models guarantees [orthogonality]{.orange}, that is $\bm{z}_j^T \bm{z}_{j'} = 0$ for any $j \neq j'$. 


## The QR decomposition I

::: incremental

- By construction, the Gram-Schmidt algorithm produces the following decomposition $$
\bm{X} = \bm{Z} \bm{\Gamma}, \qquad \bm{\Gamma} = 
\begin{bmatrix}
1 & \hat{\gamma}_{12} & \hat{\gamma}_{13} &\cdots & \hat{\gamma}_{1p} \\
0 &  1 & \hat{\gamma}_{23} &\cdots & \hat{\gamma}_{2p} \\
\vdots & \vdots & \vdots &\ddots & \vdots\\
0 & 0 & 0 & \cdots & 1
\end{bmatrix}, \qquad \bm{Z} = (\bm{z}_1,\dots,\bm{z}_p).
$$
- The $p \times p$ matrix $\bm{\Gamma}$ is [upper triangular]{.orange}, whereas the columns of the $n \times p$ matrix $\bm{Z}$ are [mutually orthogonal]{.blue}.

- It is often convenient to [standardize]{.orange} the columns of $\bm{Z}$, dividing them by their norm $||\bm{z}_j||$. Let $\bm{D} = \text{diag}(||\bm{z}_1||, \dots, ||\bm{z}_p||)$, then in matrix notation:
$$
\bm{X} = \bm{Z} \bm{\Gamma} = \bm{Z} \bm{D}^{-1} \bm{D} \bm{\Gamma} = \bm{Q} \bm{R}, \quad \text{with} \quad \bm{Q} = \bm{Z}\bm{D}^{-1}.
$$
- [Remark]{.orange}. Note that $\bm{Q}^T \bm{Q} = I_p$, i.e. the columns of $\bm{Q}$ are [orthonormal]{.blue}.

:::

## The QR decomposition II

::: callout-note
#### Theorem (QR factorization)
Suppose $\bm{X} \in \mathbb{R}^{n \times p}$ with $n \ge p$ has full
rank, that is $\text{rk}(\bm{X}) = p$. Then there exists a factorization of the form
$$
\bm{X} = \bm{Q} \bm{R},
$$
where $\bm{Q} \in \mathbb{R}^{n \times p}$ has [orthonormal columns]{.orange} and $\bm{R} \in \mathbb{R}^{p \times p}$ is an [upper triangular]{.blue} matrix. 
:::

::: callout-tip
#### Corollary (QR factorization)
The QR decomposition is [unique]{.blue} up to [sign flips]{.blue} of the columns of $\bm{Q}$ and the rows of $\bm{R}$. Moreover, if $\bm{R}$ has positive diagonal entries, as the one obtained using Gram-Schmidt, then it coincides with the [Cholesky factor]{.orange} of $\bm{X}^T\bm{X}$.
:::

## The QR decomposition and least squares

::: incremental

- The QR decomposition greatly facilitates computations for linear models. Indeed: 
$$
\begin{aligned}
\hat{\beta} &= (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y} = [(\bm{Q}\bm{R})^T\bm{Q}\bm{R}]^{-1}(\bm{Q}\bm{R})^T\bm{y}\\
&= (\bm{R}^T\bm{Q}^T\bm{Q}\bm{R})^{-1}\bm{R}^T\bm{Q}^T\bm{y} \\
&= \bm{R}^{-1} (\bm{R}^T)^{-1} \bm{R}^T\bm{Q}^T\bm{y} \\
&= \bm{R}^{-1}\bm{Q}^T \bm{y}.
\end{aligned}
$$
- Hence, the least square estimate is obtained as the solution of the [triangular system]{.blue}
$$
\bm{R}\beta = \bm{Q}^T\bm{y},
$$
which can be easily solved via [backward substitution]{.orange}. 

- As a special case of the above equation, one gets $\hat{\beta}_p = (\bm{z}_p^T\bm{y}) / (\bm{z}_p^T \bm{z}_p)$.

:::

## The QR decomposition and linear models

::: incremental

- An important advantage of the QR factorization is that many other useful quantities can be readily computed. For example, the [covariance matrix]{.orange} is obtained as:
$$
    s^2 (\bm{X}^T \bm{X})^{-1} = s^2 \bm{R}^{-1} (\bm{R}^{-1})^T.
    $$

- The [predicted values]{.orange} and the [projection matrix]{.blue} are also easily obtained as
$$
\hat{\bm{y}} = \bm{H}\bm{y} = \bm{Q}\bm{Q}^T\bm{y}.
$$

- The diagonal elements $h_i = [\bm{H}]_{ii}$ of the hat matrix $\bm{H}$ are called [leverages]{.orange} and one may want to compute them without evaluating the full $n \times n$ matrix, using
$$
h_i = \sum_{j=1}^p q_{ij}^2, \qquad i=1,\dots,n,
$$
where $q_{ij}$ are the entries of $\bm{Q}$. 

:::

## Computational complexity

::: incremental
-   The solution via QR factorization is [numerically reliable]{.orange} and it facilitates the computation of other quantities of interest. 

- In practice, the QR is computed via a [modified Gram-Schmidt]{.blue}, that fixes the instabilities of the na√Øve Gram-Schmidt algorithm, or via [Householder reflections]{.orange}.

-   The expensive step is the QR factorization itself. This gives an overall computational complexity of order $$
    \sim 2 n p^2$$
    which is about twice that of the Cholesky, when $n$ is much larger than $p$, and about the same when $p \approx n$. 

- Depending on the context, we may prefer the Cholesky or the QR. The default approach in R, i.e. the one implemented in the `lm` function, is the QR factorization.  
:::

## ‚ò†Ô∏è - Pivoting and rank deficiencies

::: incremental
- If $\text{rk}(\bm{X}) = k <  p$ ([rank deficiency]{.orange}) then it is still possibile to obtain a "QR" factorization of the form
$$
\bm{X}\bm{P} = \bm{Q}\begin{bmatrix}\bm{R}_{11} & \bm{R}_{12} \\
0 & 0\end{bmatrix},
$$
where $\bm{P}$ is a $p √ó p$ permutation matrix and $\bm{R}_{11}$ is an $k \times k$ upper triangular and non-singular matrix.

- This operation is sometimes called [pivoting]{.blue} and it is particularly important even when $\text{rk}(\bm{X}) = p$ to prevent numerical issues when the condition number $\kappa(\bm{X})$ is high. 

- In presence of perfect collinarity, the implementation of the QR decomposition in R (`qr`) relies on  pivoting. This is why the `lm` function is able to automatically "omit" a predictor. 

:::

# Iterative methods
