---
title: "A-B-C"
subtitle: "Data mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Universit√† degli Studi di Milano-Bicocca_"
execute:
  cache: false
format:
  revealjs:
    auto-stretch: false
    html-math-method: katex
    transition: slide
    output-file: un_A1_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 300
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: true
    code-summary: "Show the code"
    fig-dpi: 300
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## About this unit

-   In this unit we will cover three important [topics]{.orange}:
    -   Linear models and modelling process
    -   Computational aspects
    -   Generalized linear models (GLMs)
-   The [computational aspects]{.blue} of linear models will be novel to
    most of you...

. . .

-   ... but you should be already [very familiar]{.orange} with linear models
    and GLMs!

-   If do not remember much about them, use this first week of lectures
    to catch up (or study) from the material of previous courses.

-   A [short introduction]{.blue} to the topic is also offered in
    Azzalini & Scarpa (2011), Chapter 2 and Appendix A.3.

# Old friends: linear models

## Car data

-   We consider data for $n = 203$ models of cars in circulation in 1985
    in the USA.
-   We want to identify a relationship that allows to [predict]{.blue}
    the distance covered per unit of fuel, as a function of the vehicle
    characteristics.
-   We consider the following [continuous variables]{.orange}:
    -   The city distance per unit of fuel (km/L, `city.distance`)
    -   The engine size (L, `engine.size`)
    -   The number of cylinders (`n.cylinders`)
    -   The curb weight (kg, `curb.weight`).
-   We also considered the [categorical variable]{.orange} fuel type
    (gasoline or diesel, `fuel`).

## Car data ([diesel]{.blue} or [gas]{.orange})

```{r, message=FALSE}
library(tidyverse)
library(ggplot2)
library(GGally)
library(ggthemes)

auto <- read.table("../data/auto.txt", header = TRUE) %>% select(city.distance, engine.size, n.cylinders, curb.weight, fuel)

p0 <- ggpairs(auto,
  columns = 1:4, aes(colour = fuel),
  lower = list(continuous = wrap("points", size = 0.9)),
  upper = list(continuous = wrap("points", size = 0.9)),
  diag = "blank"
) +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("") +
  ylab("")
p0
```

## Linear regression

-   At the moment, let us focus on `city.distance` ($y$), `engine.size`
    ($x$) and `fuel` ($z$).

-   The simplest model we can come up with is a [linear
    regression]{.orange} line: $$
    y = \beta_0 + \beta_1 x + \epsilon,
    $$ where $\epsilon$ is a non-observable "error" term, having zero
    mean and variance $\sigma^2$.

. . .

-   We are looking for an estimate for the [unknown regression
    parameters]{.blue} $\beta_0$ and $\beta_1$.

-   Such an estimate could be obtained by ordinary least squares
    (OLS)...

. . .

-   ... but the next plot clearly suggests that the relationship between
    `city.distance` and `engine.size` is [not]{.orange} well
    approximated by a [linear]{.orange} function.

-   ... and also that `fuel` has an non-negligible effect on the
    response.

## Scatterplot of the data

```{r}
ggplot(data = auto, aes(x = engine.size, y = city.distance, col = fuel)) + geom_point()+
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Engine size (L)") +
  ylab("Urban distance (km/L)")
```

## Regression models

-   A more [general formulation]{.orange} for modeling the relationship
    between a vector of covariates
    $\bm{x} = (x_1,\dots,x_p)^T \in \mathbb{R}^p$ and a response
    $y \in \mathbb{R}$ is $$
    y = f(\bm{x}; \beta) + \epsilon.
    $$

. . .

-   To estimate the unknown parameters $\beta$, a possibility is to rely
    on [least squares criterion]{.blue}: we seek the [minimum]{.orange}
    of the objective function $$
    D(\beta) = \sum_{i=1}^n\{y_i - f(\bm{x}_i; \beta)\}^2,
    $$ using $n$ pairs of observations $(\bm{x}_i, y_i)$, for
    $i = 1,\dots,n$.
-   The solution to this minimization problem is denoted by
    $\hat{\beta}$.

. . .

-   The [predicted values]{.blue} $\hat{y}_i$ are then obtained as
    $\hat{y}_i = f(\bm{x}_i; \hat{\beta})$, for $i=1,\dots,n.$

## Linear models

-   There are several directions to model `city.distance` ($y$),
    `engine.size` ($x$) and `fuel` ($z$) in a more flexible way.

-   For instance, we could consider a [polynomial term]{.orange}
    combined with a [dummy variable]{.blue} $$
    f(\bm{x}; \beta) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 I(z = \texttt{gas}).
    $$

-   [Remark]{.orange}. This model is [linear in the parameters]{.blue},
    but it can capture non-linear patterns!

. . .

::: callout-note
-   The above specification is a special instance of [linear
    model]{.blue}, in which $$
    f(\bm{x}; \beta) = \beta_0  + \beta_1 x_1 + \cdots + \beta_p x_{p-1} = \bm{x}_i^T\beta,
    $$ where $\bm{x} = (1, x_1, \dots,x_{p-1})^T$ is a $p$-dimensional
    vector of [covariates]{.orange} and
    $\beta = (\beta_0,\dots,\beta_{p-1})^T$ is the corresponding vector
    of [coefficients]{.orange}.
:::

## Matrix notation

-   It is often convenient to express the quantities of linear models in
    [matrix notation]{.orange}.

-   The [response values]{.blue} are $\bm{y} = (y_1,\dots,y_n)^T$.

-   The [design matrix]{.blue} is a $n \times p$ matrix, comprising the
    covariate's values, defined by $$
    \bm{X} = (\bm{x}_1, \dots,\bm{x}_n)^T = 
    \begin{bmatrix} 
    1 & x_{1,1} & \cdots & x_{1,p-1}\\
    \vdots & \vdots & \ddots & \vdots \\
    1 & x_{n, 1} & \cdots & x_{n, p-1}
    \end{bmatrix}.
    $$

-   Thus, the linear model can be written using the compact notation: $$
    \bm{y} = \bm{X}\beta + \bm{\epsilon},
    $$ where $\bm{\epsilon} = (\epsilon_1,\dots,\epsilon_n)^T$ is a
    vector of iid error terms with zero mean and variance $\sigma^2$.

## Linear regression: estimation I

-   The optimal set of coefficients $\hat{\beta}$ is the minimizer of
    the least squared criterion $$
    D(\beta) = (\bm{y} - \bm{X}\beta)^T(\bm{y} - \bm{X}\beta) = ||\bm{y} - \bm{X}\beta||^2,
    $$ where $||\bm{y}|| = \sqrt{y_1^2 + \cdots + y_n^2}$ is the
    [Euclidean norm]{.blue}.

-   The quantity $D(\beta)$ is also known as [residual sum of squares
    (RSS)]{.orange}.

. . .

::: callout-note
-   If the design matrix has [full rank]{.blue}, that is if
    $\text{rk}(\bm{X}^T\bm{X}) = p$, then the [least square
    estimate]{.orange} has an explicit solution: $$
    \hat{\beta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T \bm{y}.
    $$
:::

## Linear regression: estimation II

-   Consequently, the predicted values are $$
    \hat{\bm{y}} = \bm{X}\hat{\beta} = \bm{P}\bm{y}, \qquad \bm{P} = \bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T.
    $$

-   $\bm{P}$ is a $n \times n$ [projection matrix]{.orange} matrix
    sometimes called [hat matrix]{.blue}.

-   It can be shown that $\text{tr}(\bm{P}) = \text{rk}(\bm{P}) = p$.
    Moreover, it holds $\bm{P} = \bm{P}^T$ and $\bm{P}^2 = \bm{P}$.

-   The quantity $D(\hat{\beta})$ is the [deviance]{.blue}, which is
    equal to $$
    D(\hat{\beta}) = ||\bm{y} - \hat{\bm{y}}||^2 = \bm{y}^T(I_n - \bm{P})\bm{y}.
    $$

-   A typical estimate for the residual variance $\sigma^2$ is then
    given by $$
    s^2 = \frac{D(\hat{\beta})}{n - p} = \frac{1}{n-p}\sum_{i=1}^n(y_i - \bm{x}_i^T\hat{\beta})^2.
    $$

## Linear regression: inference

- Let us additionally assume that the errors follow a Gaussian distribution: $\epsilon_i \overset{\text{iid}}{\sim} N(0, \sigma^2)$. 

- This implies that the distribution of the estimator $\hat{\beta}$ is
$$
\hat{\beta} \sim N_p(\beta, \sigma^2 (X^TX)^{-1}).
$$
- Hence, the estimator $\hat{\beta}$ is [unbiased]{.orange} and its [variance]{.blue} can be estimated by
$$
\widehat{\text{var}}(\hat{\beta}) = s^2 (X^TX)^{-1}.
$$
- The [standard errors]{.orange} of the components of beta correspond to the square root of the diagonal of the above covariance matrix. 

- Confidence interval and Wald's tests can be obtained through classical inferential theory. 

## Car data: a first model

- A first model to predict `city.distance` ($y$) via
    `engine.size` ($x$) and `fuel` ($z$) is $$
    f(\bm{x}; \beta) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 I(z = \texttt{gas}).
    $$
- Intuition...

## Car data: estimated coefficients

- We obtain following [summary]{.orange} for the regression coefficients $\hat{\beta}$. 

```{r}
library(broom)
library(knitr)
m1 <- lm(city.distance ~ engine.size + I(engine.size^2) + I(engine.size^3) + fuel, data = auto)
kable(tidy(m1, conf.int = TRUE), digits = 3)
```

## Car data: fitted value

```{r}
augmented_m1 <- augment(m1)
ggplot(data = augmented_m1, aes(x = engine.size, y = city.distance, col = fuel)) + geom_point() + geom_line(aes(y = .fitted)) +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Engine size (L)") +
  ylab("Urban distance (km/L)")
```

## Car data: graphical diagnostics

```{r}
ggplot(data = augmented_m1, aes(x = .fitted, y = .resid, col = fuel)) + geom_point() + geom_hline(aes(yintercept = 0), linetype = "dotted") +
  theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Fitted values") +
  ylab("Residuals")
```

## Comments

## Variable transformation

# Computational aspects

## How to obtain the least squares estimate

# Generalized linear models
