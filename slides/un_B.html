<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tommaso Rigon">

<title>Optimism, Conflicts, and Trade-offs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="un_B_files/libs/clipboard/clipboard.min.js"></script>
<script src="un_B_files/libs/quarto-html/quarto.js"></script>
<script src="un_B_files/libs/quarto-html/popper.min.js"></script>
<script src="un_B_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="un_B_files/libs/quarto-html/anchor.min.js"></script>
<link href="un_B_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="un_B_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="un_B_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="un_B_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="un_B_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#homepage" id="toc-homepage" class="nav-link active" data-scroll-target="#homepage">Homepage</a></li>
  <li><a href="#yesterdays-data" id="toc-yesterdays-data" class="nav-link" data-scroll-target="#yesterdays-data">Yesterday’s data</a></li>
  <li><a href="#polynomial-regression" id="toc-polynomial-regression" class="nav-link" data-scroll-target="#polynomial-regression">Polynomial regression</a></li>
  <li><a href="#yesterdays-data-polynomial-regression" id="toc-yesterdays-data-polynomial-regression" class="nav-link" data-scroll-target="#yesterdays-data-polynomial-regression">Yesterday’s data, polynomial regression</a></li>
  <li><a href="#yesterdays-data-goodness-of-fit" id="toc-yesterdays-data-goodness-of-fit" class="nav-link" data-scroll-target="#yesterdays-data-goodness-of-fit">Yesterday’s data, goodness of fit</a></li>
  <li><a href="#yesterdays-data-polynomial-interpolation-p-n" id="toc-yesterdays-data-polynomial-interpolation-p-n" class="nav-link" data-scroll-target="#yesterdays-data-polynomial-interpolation-p-n">Yesterday’s data, polynomial interpolation (<span class="math inline">p = n</span>)</a></li>
  <li><a href="#yesterdays-data-tomorrows-prediction" id="toc-yesterdays-data-tomorrows-prediction" class="nav-link" data-scroll-target="#yesterdays-data-tomorrows-prediction">Yesterday’s data, tomorrow’s prediction</a></li>
  <li><a href="#tomorrows-data-polynomial-regression" id="toc-tomorrows-data-polynomial-regression" class="nav-link" data-scroll-target="#tomorrows-data-polynomial-regression">Tomorrow’s data, polynomial regression</a></li>
  <li><a href="#tomorrows-data-goodness-of-fit" id="toc-tomorrows-data-goodness-of-fit" class="nav-link" data-scroll-target="#tomorrows-data-goodness-of-fit">Tomorrow’s data, goodness of fit</a></li>
  <li><a href="#comments-and-remarks" id="toc-comments-and-remarks" class="nav-link" data-scroll-target="#comments-and-remarks">Comments and remarks</a></li>
  <li><a href="#orthogonal-polynomials-and-numerical-difficulties" id="toc-orthogonal-polynomials-and-numerical-difficulties" class="nav-link" data-scroll-target="#orthogonal-polynomials-and-numerical-difficulties">☠️ - Orthogonal polynomials and numerical difficulties</a></li>
  <li><a href="#methods-for-model-selection" id="toc-methods-for-model-selection" class="nav-link" data-scroll-target="#methods-for-model-selection">Methods for model selection</a>
  <ul class="collapse">
  <li><a href="#summary-and-notation" id="toc-summary-and-notation" class="nav-link" data-scroll-target="#summary-and-notation">Summary and notation</a></li>
  <li><a href="#the-average-prediction-error" id="toc-the-average-prediction-error" class="nav-link" data-scroll-target="#the-average-prediction-error">The average prediction error</a></li>
  <li><a href="#quadratic-loss-and-the-mean-squared-error" id="toc-quadratic-loss-and-the-mean-squared-error" class="nav-link" data-scroll-target="#quadratic-loss-and-the-mean-squared-error">Quadratic loss and the mean squared error</a></li>
  <li><a href="#reducible-and-irreducible-errors" id="toc-reducible-and-irreducible-errors" class="nav-link" data-scroll-target="#reducible-and-irreducible-errors">Reducible and irreducible errors</a></li>
  <li><a href="#bias-variance-trade-off" id="toc-bias-variance-trade-off" class="nav-link" data-scroll-target="#bias-variance-trade-off">Bias-variance trade-off</a></li>
  <li><a href="#the-expected-prediction-error" id="toc-the-expected-prediction-error" class="nav-link" data-scroll-target="#the-expected-prediction-error">The expected prediction error</a></li>
  <li><a href="#if-we-knew-fx" id="toc-if-we-knew-fx" class="nav-link" data-scroll-target="#if-we-knew-fx">If we knew <span class="math inline">f(x)</span>…</a></li>
  <li><a href="#bias-variance-trade-off-1" id="toc-bias-variance-trade-off-1" class="nav-link" data-scroll-target="#bias-variance-trade-off-1">Bias-variance trade-off</a></li>
  </ul></li>
  <li><a href="#traning-test-and-cross-validation" id="toc-traning-test-and-cross-validation" class="nav-link" data-scroll-target="#traning-test-and-cross-validation">Traning, test, and cross-validation</a>
  <ul class="collapse">
  <li><a href="#but-since-we-do-not-know-fx" id="toc-but-since-we-do-not-know-fx" class="nav-link" data-scroll-target="#but-since-we-do-not-know-fx">But since we do not know <span class="math inline">f(x)</span>…</a></li>
  <li><a href="#mean-squared-error" id="toc-mean-squared-error" class="nav-link" data-scroll-target="#mean-squared-error">Mean squared error</a></li>
  <li><a href="#training-and-test" id="toc-training-and-test" class="nav-link" data-scroll-target="#training-and-test">Training and test</a></li>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation">Cross-validation</a></li>
  <li><a href="#leave-one-out-cross-validation-loo-cv" id="toc-leave-one-out-cross-validation-loo-cv" class="nav-link" data-scroll-target="#leave-one-out-cross-validation-loo-cv">Leave-one-out cross-validation (LOO-CV)</a></li>
  </ul></li>
  <li><a href="#information-criteria" id="toc-information-criteria" class="nav-link" data-scroll-target="#information-criteria">Information criteria</a></li>
  <li><a href="#optimism" id="toc-optimism" class="nav-link" data-scroll-target="#optimism">Optimism</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Optimism, Conflicts, and Trade-offs</h1>
<p class="subtitle lead">Data Mining - CdL CLAMSES</p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <span class="orange">Tommaso Rigon</span> 
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <em>Università degli Studi di Milano-Bicocca</em>
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  

</header>

<section id="homepage" class="level2">
<h2 class="anchored" data-anchor-id="homepage"><a href="../index.html">Homepage</a></h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><img src="img/razor.jpg" class="img-fluid"> <em>“Pluralitas non est ponenda sine necessitate.”</em> William of Ockham</p>
</div><div class="column" style="width:60%;">
<ul>
<li><p>In this unit we will cover the following <span class="orange">topics</span>:</p>
<ul>
<li>Bias-variance trade-off</li>
<li>Cross-validation</li>
<li>Information criteria</li>
<li>Optimism</li>
</ul></li>
<li><p>You may have seen these notions before…</p></li>
<li><p>…but it is worth discussing the <span class="orange">details</span> of these ideas once again, with the maturity you now have in a M.Sc.</p></li>
<li><p>They are the <span class="blue">foundations</span> of <span class="orange">statistical learning</span>.</p></li>
</ul>
</div>
</div>
</section>
<section id="yesterdays-data" class="level2">
<h2 class="anchored" data-anchor-id="yesterdays-data">Yesterday’s data</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="cell" data-hash="un_B_cache/html/unnamed-chunk-2_62c2fc2b99616e1009cd0c29beb21349">
<div class="cell-output-display">
<p><img src="un_B_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="750"></p>
</div>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>Let us presume that <span class="orange">yesterday</span> we observed <span class="math inline">n = 30</span> pairs of data <span class="math inline">(x_i, y_i)</span>.</p></li>
<li><p>Data were generated according to <span class="math display">
  Y_i = f(x_i) + \epsilon_i, \quad i=1,\dots,n,
  </span> with each <span class="math inline">y_i</span> being the realization of <span class="math inline">Y_i</span>.</p></li>
<li><p>The <span class="math inline">\epsilon_1,\dots,\epsilon_n</span> are iid “<span class="orange">error</span>” terms, such that <span class="math inline">\mathbb{E}(\epsilon_i)=0</span> and <span class="math inline">\text{var}(\epsilon_i)=\sigma^2 = 10^{-2}</span>.</p></li>
<li><p>Here <span class="math inline">f(x)</span> is a regression function (<span class="blue">signal</span>) that we leave unspecified.</p></li>
<li><p><span class="blue">Tomorrow</span> we will get a new <span class="math inline">x_{n+1}</span>. We wish to <span class="orange">predict</span> <span class="math inline">Y_{n+1}</span> using <span class="math inline">\mathbb{E}(Y_{n+1}) = f(x_{n+1}; \beta)</span>.</p></li>
</ul>
</div>
</div>
</section>
<section id="polynomial-regression" class="level2">
<h2 class="anchored" data-anchor-id="polynomial-regression">Polynomial regression</h2>
<div class="incremental">
<ul class="incremental">
<li><p>The function <span class="math inline">f(x)</span> is unknown, therefore it should be estimated.</p></li>
<li><p>A simple approach is using the tools of <a href="unit_A1_slides.html">Unit A.1</a>, such as <span class="orange">polynomial regression</span>: <span class="math display">
f(x) = f(x; \beta) = \beta_1 + \beta_2 x + \beta_3 x^2 + \cdots + \beta_p x^{p-1},
</span> namely <span class="math inline">f(x)</span> is <span class="orange">approximated</span> with a polynomial of degree <span class="math inline">p-1</span> (i.e.&nbsp;Taylor expansions).</p></li>
<li><p>This model is linear in the parameters, therefore we can use ordinary least squares.</p></li>
<li><p>How do we choose the <span class="blue">degree of the polynomial</span> <span class="math inline">p - 1</span>?</p></li>
<li><p>Without a clear guidance, in principle any value of <span class="math inline">p \in \{1,\dots,n\}</span> could be appropriate.</p></li>
<li><p>Let us compare the <span class="blue">mean squared error</span> (MSE) on yesterday’s data (<span class="orange">training</span>) <span class="math display">
\text{MSE}_{\text{train}} = \frac{1}{n}\sum_{i=1}^n\{y_i -f(x_i; \hat{\beta})\}^2,
</span> or alternatively <span class="math inline">R^2_\text{train}</span>, for different values of <span class="math inline">p</span>…</p></li>
</ul>
</div>
</section>
<section id="yesterdays-data-polynomial-regression" class="level2">
<h2 class="anchored" data-anchor-id="yesterdays-data-polynomial-regression">Yesterday’s data, polynomial regression</h2>
<div class="cell" data-hash="un_B_cache/html/unnamed-chunk-3_a238a6b2b0922f2d60e03aac4bf4a02b">

</div>
<div class="cell" data-layout-align="center" data-hash="un_B_cache/html/unnamed-chunk-4_aeffbb50af87d21d4eaa0ed0bdf74edb">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_B_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="1170"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="yesterdays-data-goodness-of-fit" class="level2">
<h2 class="anchored" data-anchor-id="yesterdays-data-goodness-of-fit">Yesterday’s data, goodness of fit</h2>
<div class="cell" data-hash="un_B_cache/html/unnamed-chunk-5_fc1bc19226f56c0414047b6788566731">

</div>
<div class="columns">
<div class="column" style="width:50%;">
<div class="cell" data-hash="un_B_cache/html/unnamed-chunk-6_8c50f96e3c2fabd090abbf25329b6608">
<div class="cell-output-display">
<p><img src="un_B_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="750"></p>
</div>
</div>
</div><div class="column" style="width:50%;">
<div class="cell" data-hash="un_B_cache/html/unnamed-chunk-7_da5090127fc295b02c8278f2cb0a98af">
<div class="cell-output-display">
<p><img src="un_B_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="750"></p>
</div>
</div>
</div>
</div>
</section>
<section id="yesterdays-data-polynomial-interpolation-p-n" class="level2">
<h2 class="anchored" data-anchor-id="yesterdays-data-polynomial-interpolation-p-n">Yesterday’s data, polynomial interpolation (<span class="math inline">p = n</span>)</h2>
<div class="cell" data-layout-align="center" data-hash="un_B_cache/html/unnamed-chunk-8_6cb739fd2a6f36ff9beae86d35f84e75">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_B_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="1350"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="yesterdays-data-tomorrows-prediction" class="level2">
<h2 class="anchored" data-anchor-id="yesterdays-data-tomorrows-prediction">Yesterday’s data, tomorrow’s prediction</h2>
<div class="incremental">
<ul class="incremental">
<li><p>The <span class="blue">MSE</span> decreases as the number of parameter increases; similarly, the <span class="blue"><span class="math inline">R^2</span></span> always increases as a function of <span class="math inline">p</span>. It can be <span class="orange">proved</span> that this <span class="orange">always happens</span> in linear models.</p></li>
<li><p>One might be tempted to let <span class="math inline">p</span> as large as possible to make the model more flexible…</p></li>
<li><p>Taking this reasoning to the extreme would lead to the choice <span class="math inline">p = n</span>, so that <span class="math display">
\text{MSE}_\text{train} = 0, \qquad R^2_\text{train} = 1,
</span> i.e.&nbsp;an apparently perfect fit. This procedure is called <span class="blue">interpolation</span>.</p></li>
<li><p>However, we are <span class="orange">not</span> interested in predicting <span class="orange">yesterday</span> data. Our goal is to predict <span class="blue">tomorrow</span>’s data, i.e., given <span class="math inline">x_{n + i}</span>, to predict <span class="math inline">Y_{n + i}</span>, for a new set of <span class="math inline">m = 30</span> points: <span class="math display">
(x_{n+1}, y_{n+1}), \dots, (x_{n+m}, y_{n + m}) \  \longrightarrow \ \hat{y}_{n+i} = \mathbb{E}(Y_{n+i}) = f(x_{n+i}; \hat{\beta}),
</span> for <span class="math inline">i=1,\dots,m</span>, where <span class="math inline">\hat{\beta}</span> is obtained using yesterday data.</p></li>
<li><p><span class="orange">Remark</span>. Tomorrow’s data <span class="math inline">Y_{n+1},\dots, Y_{n+m}</span> follows the same scheme of yesterday’s data.</p></li>
</ul>
</div>
</section>
<section id="tomorrows-data-polynomial-regression" class="level2">
<h2 class="anchored" data-anchor-id="tomorrows-data-polynomial-regression">Tomorrow’s data, polynomial regression</h2>
<div class="cell" data-layout-align="center" data-hash="un_B_cache/html/unnamed-chunk-9_f378597b37085579b2ae8736485d995f">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_B_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="1200"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="tomorrows-data-goodness-of-fit" class="level2">
<h2 class="anchored" data-anchor-id="tomorrows-data-goodness-of-fit">Tomorrow’s data, goodness of fit</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="cell" data-hash="un_B_cache/html/unnamed-chunk-10_0cb0409d49adedd702471ef46e7a5a23">
<div class="cell-output-display">
<p><img src="un_B_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid" width="750"></p>
</div>
</div>
</div><div class="column" style="width:50%;">
<div class="cell" data-hash="un_B_cache/html/unnamed-chunk-11_8b7f39e35f70a925529d9f0060bde4af">
<div class="cell-output-display">
<p><img src="un_B_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid" width="750"></p>
</div>
</div>
</div>
</div>
</section>
<section id="comments-and-remarks" class="level2">
<h2 class="anchored" data-anchor-id="comments-and-remarks">Comments and remarks</h2>
<div class="incremental">
<ul class="incremental">
<li><p>The mean squared error on tomorrow’s data (<span class="blue">test</span>) is defined as <span class="math display">
\text{MSE}_{\text{test}} = \frac{1}{m}\sum_{i=1}^m\{y_{n +i} -f(x_{n + i}; \hat{\beta})\}^2,
</span> and similarly the <span class="math inline">R^2_\text{test}</span>. We would like the <span class="math inline">\text{MSE}_{\text{test}}</span> to be <span class="orange">as small as possible</span>.</p></li>
<li><p>For <span class="blue">small values</span> of <span class="math inline">p</span>, an increase in the degree of the polynomial <span class="blue">improves the fit</span>. In other words, at the beginning, both the <span class="math inline">\text{MSE}_{\text{train}}</span> and the <span class="math inline">\text{MSE}_{\text{test}}</span> decrease.</p></li>
<li><p>For <span class="orange">larger values</span> of <span class="math inline">p</span>, the improvement gradually ceases and the polynomial starts to follow <span class="orange">random fluctuations</span> in yesterday’s data which are <span class="blue">not observed</span> in the <span class="blue">new sample</span>.</p></li>
<li><p>An over-adaptation to yesterday data is called <span class="orange">overfitting</span>, which occurs when the training <span class="math inline">\text{MSE}_{\text{train}}</span> is low but the test <span class="math inline">\text{MSE}_{\text{test}}</span> is high.</p></li>
</ul>
</div>
</section>
<section id="orthogonal-polynomials-and-numerical-difficulties" class="level2">
<h2 class="anchored" data-anchor-id="orthogonal-polynomials-and-numerical-difficulties">☠️ - Orthogonal polynomials and numerical difficulties</h2>
</section>
<section id="methods-for-model-selection" class="level1">
<h1>Methods for model selection</h1>
<section id="summary-and-notation" class="level2">
<h2 class="anchored" data-anchor-id="summary-and-notation">Summary and notation</h2>
<div class="incremental">
<ul class="incremental">
<li><p>As in the previous example, we consider two set of <span class="blue">random variables</span>:</p>
<ul class="incremental">
<li>The <span class="orange">training set</span> <span class="math inline">Y_1,\dots, Y_n</span>, whose realization is <span class="math inline">y_1,\dots,y_n</span>.</li>
<li>The <span class="blue">test set</span> <span class="math inline">Y_{n+1},\dots,Y_{n +m}</span>, whose realization is <span class="math inline">y_{n+1}, \dots, y_{n + m}</span>.</li>
</ul></li>
<li><p>We assume that the <span class="blue">covariates</span> <span class="math inline">\bm{x}_i = (x_{i1},\dots,x_{ip})^T</span> are <span class="blue">deterministic</span>, to simplify the notation and the discussion. This can be relaxed, see e.g.&nbsp;Chapter 7 of HTF.</p></li>
<li><p>An <span class="orange">important assumption</span> is that the random variables <span class="math inline">Y_i</span> are <span class="blue">independent</span>. In particular: <span class="math display">
  Y_i = f(\bm{x}_i) + \epsilon_i, \quad i=1,\dots,m,
</span> where <span class="math inline">\epsilon_1,\dots,\epsilon_m</span> are iid “<span class="orange">error</span>” terms with <span class="math inline">\mathbb{E}(\epsilon_i)=0</span> and <span class="math inline">\text{var}(\epsilon_i)=\sigma^2</span>.</p></li>
<li><p>The <span class="orange">training data</span> is used to estimate a function of the covariates <span class="math inline">\hat{f}(\bm{x}_i)</span> which should predict well the corresponding response <span class="math inline">Y_i</span>.</p></li>
<li><p>We hope that our predictions works well on the <span class="blue">test set</span>, which can be used to evaluate the quality of the estimated <span class="math inline">\hat{f}(\bm{x}_i)</span>.</p></li>
</ul>
</div>
</section>
<section id="the-average-prediction-error" class="level2">
<h2 class="anchored" data-anchor-id="the-average-prediction-error">The average prediction error</h2>
<div class="incremental">
<ul class="incremental">
<li><p>A measure of quality for the predictions is the so-called <span class="blue">expected prediction error</span>: <span class="math display">
\text{Err} = \mathbb{E}(\text{MSE}_\text{test}) =  \mathbb{E}\left[\frac{1}{m} \sum_{i=1}^m \mathscr{L}\{Y_{n + i}; \hat{f}(\bm{x}_{n + i})\}\right],
</span> where <span class="math inline">\mathscr{L}\{Y_i; \hat{f}(\bm{x}_i)\}</span> is a <span class="orange">loss function</span>.</p></li>
<li><p>The expectation is taken with respect to traning random variable <span class="math inline">Y_1,\dots,Y_n</span>, implicitly appearing in <span class="math inline">\hat{f}(\bm{x})</span>, and the new data points <span class="math inline">Y_{n+1},\dots,Y_{n+m}</span>.</p></li>
<li><p>The expected prediction error is measuring the <span class="blue">average</span> “discrepancy” between the new data points and the corresponding predictions.</p></li>
<li><p>Examples of loss functions for <span class="blue">regression problems</span> <span class="math inline">Y \in \mathbb{R}</span> are:</p>
<ul class="incremental">
<li>The quadratic loss <span class="math inline">\mathscr{L}\{Y_i; \hat{f}(\bm{x}_i)\} = \{Y_i - \hat{f}(\bm{x}_i)\}^2</span>, leading to the MSE.</li>
<li>The absolute loss <span class="math inline">\mathscr{L}\{Y_i; \hat{f}(\bm{x}_i)\} = |Y_i - \hat{f}(\bm{x}_i)|</span>, leading to the MAE.</li>
</ul></li>
</ul>
</div>
</section>
<section id="quadratic-loss-and-the-mean-squared-error" class="level2">
<h2 class="anchored" data-anchor-id="quadratic-loss-and-the-mean-squared-error">Quadratic loss and the mean squared error</h2>
<div class="callout-note callout callout-style-simple no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Error decomposition (reducible and irreducible)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Under a quadratic loss, <span class="orange">each element</span> of the <span class="blue">expected prediction error</span> admits the following decomposition <span class="math display">
\begin{aligned}
\mathbb{E}\left[\{Y_i - \hat{f}(\bm{x}_i)\}^2\right] &amp;= \mathbb{E}\left[\{f(\bm{x}_i) + \epsilon_i - \hat{f}(\bm{x}_i)\}^2\right] \\
&amp; = \mathbb{E}\left[\{f(\bm{x}_i) - \hat{f}(\bm{x}_i)\}^2\right] + \mathbb{E}(\epsilon_i^2) + 2 \: \mathbb{E}\left[\epsilon_i \: \{f(\bm{x}_i) - \hat{f}(\bm{x}_i)\}\right]\\
&amp; = \underbrace{\mathbb{E}\left[\{\hat{f}(\bm{x}_i) - f(\bm{x}_i)\}^2\right]}_{\text{reducible}} + \underbrace{\sigma^2}_{\text{irreducible}},
\end{aligned}
</span> recalling that <span class="math inline">\mathbb{E}(\epsilon_i^2) = \text{var}(\epsilon_i) = \sigma^2</span> and for any <span class="math inline">i = n+1,\dots,n+m</span> (<span class="orange">test set</span>).</p>
</div>
</div>
</section>
<section id="reducible-and-irreducible-errors" class="level2">
<h2 class="anchored" data-anchor-id="reducible-and-irreducible-errors">Reducible and irreducible errors</h2>
<div class="incremental">
<ul class="incremental">
<li><p>We would like to make the <span class="orange">mean squared error</span> as <span class="orange">small</span> as possible, e.g.&nbsp;by choosing an “optimal” degree of the polynomial <span class="math inline">p-1</span> that minimizes it.</p></li>
<li><p>Let us recall the previous decomposition <span class="math display">
\mathbb{E}\left[\{Y_i - \hat{f}(\bm{x}_i)\}^2\right] =  \underbrace{\mathbb{E}\left[\{\hat{f}(\bm{x}_i) - f(\bm{x}_i)\}^2\right]}_{\text{reducible}} + \underbrace{\sigma^2}_{\text{irreducible}}, \quad i=n+1,\dots,n + m.
</span></p></li>
<li><p>The <span class="blue">best case scenario</span> is when the estimated function coincides with the mean of <span class="math inline">Y_i</span>, i.e. <span class="math display">
\hat{f}(\bm{x}_i) =  f(\bm{x}_i) = \mathbb{E}(Y_i),
</span> but even in this (overly optimistic) situation, we would still commit mistakes, due to the presence of <span class="math inline">\epsilon_i</span> (unless <span class="math inline">\sigma^2 = 0</span>). Hence, the variance <span class="math inline">\sigma^2</span> is called the <span class="orange">irreducible error</span>.</p></li>
<li><p>Since we do not know <span class="math inline">f(\bm{x}_i)</span>, we seek for an estimate <span class="math inline">\hat{f}(\bm{x}_i) \approx f(\bm{x}_i)</span>, in the attempt of minimizing the <span class="blue">reducible error</span>.</p></li>
</ul>
</div>
</section>
<section id="bias-variance-trade-off" class="level2">
<h2 class="anchored" data-anchor-id="bias-variance-trade-off">Bias-variance trade-off</h2>
<ul>
<li><p>In many books, including A&amp;S, the starting point of the analysis is the <span class="blue">reducible error</span>, because it is the only one we can control, e.g.&nbsp;by selecting the “optimal” <span class="math inline">p</span>.</p></li>
<li><p>The reducible error measures the <span class="orange">discrepancy</span> between the unknown function <span class="math inline">f(x)</span> and its estimate <span class="math inline">\hat{f}(\bm{x})</span>.</p></li>
<li><p>What follows is the <span class="blue">most important result</span> of this unit.</p></li>
</ul>
<div class="incremental">
<div class="callout-note callout callout-style-simple no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Bias-variance decomposition
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any covariate value <span class="math inline">\bm{x}</span>, it holds the following well-known decomposition: <span class="math display">
\mathbb{E}\left[\{\hat{f}(\bm{x}) - f(\bm{x})\}^2\right] = \underbrace{\mathbb{E}\left[\{\hat{f}(\bm{x}) - f(\bm{x})\}\right]^2}_{\text{Bias}^2} + \underbrace{\text{var}\{\hat{f}(\bm{x})\}}_{\text{variance}}.
</span></p>
</div>
</div>
</div>
</section>
<section id="the-expected-prediction-error" class="level2">
<h2 class="anchored" data-anchor-id="the-expected-prediction-error">The expected prediction error</h2>
<div class="incremental">
<ul class="incremental">
<li><p>Summarizing, the <span class="orange">expected prediction error</span> can be decomposed as <span class="math display">
\begin{aligned}
\text{Err} = \sigma^2 + \frac{1}{m}\sum_{i=1}^m\mathbb{E}\left[\{\hat{f}(\bm{x}_{n+i}) - f(\bm{x}_{n+i})\}\right]^2 + \frac{1}{m}\sum_{i=1}^m\text{var}\{\hat{f}(\bm{x}_{n+i})\}.
\end{aligned}
</span></p></li>
<li><p>In <span class="blue">ordinary least squares</span> the above quantity can be computed in closed form.</p></li>
<li><p>Indeed, each element of the <span class="orange">bias</span> term equals <span class="math display">
\mathbb{E}\left[\{\hat{f}(\bm{x}_{n+i}) - f(\bm{x}_{n+i})\}\right] = \bm{x}_{n+i}^T(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{f} - f(\bm{x}_{n+i}).
</span> where <span class="math inline">\bm{f} = (f(\bm{x}_1),\dots,f(\bm{x}_n))^T</span>. Note that if <span class="math inline">f(\bm{x}) = \bm{x}^T\beta</span>, then the bias is zero.</p></li>
<li><p>Moreover, each element of the <span class="orange">variance</span> term equals <span class="math display">
\text{var}\{\hat{f}(\bm{x}_{n+i})\} = \sigma^2 \bm{x}_{n+i}^T (\bm{X}^T\bm{X})^{-1}\bm{x}_{n+i}.
</span></p></li>
<li><p>If we knew <span class="math inline">f(\bm{x})</span> and <span class="math inline">\sigma^2</span>, then the expected prediction error could be calculated…</p></li>
</ul>
</div>
</section>
<section id="if-we-knew-fx" class="level2">
<h2 class="anchored" data-anchor-id="if-we-knew-fx">If we knew <span class="math inline">f(x)</span>…</h2>
<div class="cell" data-hash="un_B_cache/html/unnamed-chunk-12_b342a73e223f72659824dc4752b06075">

</div>
<div class="cell" data-hash="un_B_cache/html/unnamed-chunk-13_c84d4b9b1b85ad58d1ddca9db41fbc40">
<div class="cell-output-display">
<p><img src="un_B_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid" width="1200"></p>
</div>
</div>
</section>
<section id="bias-variance-trade-off-1" class="level2">
<h2 class="anchored" data-anchor-id="bias-variance-trade-off-1">Bias-variance trade-off</h2>
<div class="incremental">
<ul class="incremental">
<li><p>When <span class="math inline">p</span> grows, the mean squared error first decreases and then it increases. In the example, the <span class="orange">theoretical optimum</span> is <span class="math inline">p = 6</span> (5th degree polynomial).</p></li>
<li><p>The <span class="orange">bias</span> measures the ability of <span class="math inline">\hat{f}(\bm{x})</span> to reconstruct the true <span class="math inline">f(\bm{x})</span>. The bias is due to <span class="blue">lack of knowledge</span> of the data-generating mechanism. It equals zero when <span class="math inline">\mathbb{E}\{\hat{f}(\bm{x})\} = f(\bm{x})</span>.</p></li>
<li><p>The <span class="orange">bias</span> term can be reduced by increasing the flexibility of the model (e.g., by considering a high value for <span class="math inline">p</span>).</p></li>
<li><p>The <span class="blue">variance</span> measures the variability of the estimator <span class="math inline">\hat{f}(\bm{x})</span> and its tendency to follow random fluctuations of the data.</p></li>
<li><p>The <span class="blue">variance</span> increases together with the model complexity.</p></li>
<li><p>It is not possible to minimize both the bias and the variance, there is a <span class="orange">trade-off</span>.</p></li>
<li><p>We say that an estimator is <span class="orange">overfitting</span> the data if an increase in variance comes without important gains in terms of bias.</p></li>
</ul>
<!-- - **Summary**. Low model complexity: [high bias]{.orange}, [low variance]{.blue}. High model complexity: [low bias]{.blue}, [high variance]{.orange}. -->
</div>
</section>
</section>
<section id="traning-test-and-cross-validation" class="level1">
<h1>Traning, test, and cross-validation</h1>
<section id="but-since-we-do-not-know-fx" class="level2">
<h2 class="anchored" data-anchor-id="but-since-we-do-not-know-fx">But since we do not know <span class="math inline">f(x)</span>…</h2>
<ul>
<li></li>
</ul>
</section>
<section id="mean-squared-error" class="level2">
<h2 class="anchored" data-anchor-id="mean-squared-error">Mean squared error</h2>
<div class="cell" data-hash="un_B_cache/html/unnamed-chunk-14_3ec5c88c5bfbd87ddc8c96f429fcea07">

</div>
<div class="cell" data-layout-align="center" data-hash="un_B_cache/html/unnamed-chunk-15_6d29ec6493a0258f9b720181b755b30e">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_B_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="1200"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="training-and-test" class="level2">
<h2 class="anchored" data-anchor-id="training-and-test">Training and test</h2>
</section>
<section id="cross-validation" class="level2">
<h2 class="anchored" data-anchor-id="cross-validation">Cross-validation</h2>
</section>
<section id="leave-one-out-cross-validation-loo-cv" class="level2">
<h2 class="anchored" data-anchor-id="leave-one-out-cross-validation-loo-cv">Leave-one-out cross-validation (LOO-CV)</h2>
<div class="cell" data-hash="un_B_cache/html/unnamed-chunk-16_7c73fd7c66949ad6430f2fc0e648849d">

</div>
<div class="cell" data-layout-align="center" data-hash="un_B_cache/html/unnamed-chunk-17_7734bdefb1bc77786dac39620b405d7e">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="un_B_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="1200"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="information-criteria" class="level1">
<h1>Information criteria</h1>
</section>
<section id="optimism" class="level1">
<h1>Optimism</h1>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



<script src="un_B_files/libs/quarto-html/zenscroll-min.js"></script>
</body></html>