---
title: "Frequentialy Asked Questions"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Universit√† degli Studi di Milano-Bicocca_"
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    df-print: tibble
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_F_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 200
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 200
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_G.qmd", output = "../code/un_G.R", documentation = 0)
styler:::style_file("../code/un_G.R")
```

::: columns
::: {.column width="50%"}
![](img/russell.jpg){width="60%" fig-align="center"}

::: {style="font-size: 80%;"}
"*The whole problem with the world is that fools and fanatics are always so certain of themselves, and wiser people so full of doubts.*" 

[Bertrand Russel]{.grey}
:::
:::

::: {.column width="50%"}
- This is not a normal unit and it has a more dynamic nature. 

- I will collect here [interesting]{.blue} questions that have been made by students over the years.

- The questions are organized by topic, but there will be a certain degree of overlap.

- Hopefully, this unit will form the basis of a [statistical culture]{.orange}. 

- Indeed, it is based on the knowledge of several statisticians. 
:::
:::

<!-- ## What is the difference between an algorithm and a model? -->

<!-- ::: columns -->
<!-- ::: {.column width="50%"} -->
<!-- - [Model]{.blue}: a simplified *mathematical* representation of reality. -->

<!-- - [Algorithm]{.blue}: a step-by-step procedure that is useful to obtain the unknown parts of a model. -->
<!-- ::: -->

<!-- ::: {.column width="50%"} -->
<!-- [Example]{.orange}: linear models, neural networks, support vector machine, trees. -->

<!-- [Example]{.orange}: gradient descent, Newton-Raphson, pathwise coordinate descent, LAR. -->
<!-- ::: -->
<!-- ::: -->

## Why do we care about simple models?

- Simple models are useful to [explain]{.orange} reality, not just to make predictions.

- Simple models $\neq$ linear models. A tree with a few splits is indeed "simple".

- Simple models tend to be more [robust]{.orange}, i.e. less affected by contamination and/or changes in the data generative process.

- Besides, the [understanding]{.blue} gained by fitting simple models is extremely valuable for fitting the more complex ones, because it provides guidance about the next steps. 

- The modelling process is [iterative]{.blue} and [not a collection of independent]{.orange} and [separate approaches]{.orange}. You fit a model, check the results, and adjust your strategy. 

## What if simple models do not work well?

...for example, what to do when simple models do not predict well?

- In that case, of course, it is time to move on and try something else.

- However, you [should not]{.orange} fit a sophisticated model for no reason, without having tried the simple ones. 

- In other terms, you should try to fit a [deep neural network]{.blue} only if [everything else has failed]{.orange}.

- And even when it turns out that a complex model improves the predictive performance, always ask yourself: ["is it worth it?"]{.orange} The answer is often [context-dependent]{.blue}. 

## What to do when different estimators lead to different conclusions?

- Well, this happens all the times in applied statistics. It even happens in Statistics I!

- For instance, you might get different values (or even opposite signs) when looking at the estimated regression coefficients $\hat{\beta}_\text{ols}$, $\hat{\beta}_\text{pcr}$, $\hat{\beta}_\text{ridge}$.

- Do not assume OLS coefficients are "better" just because they are [unbiased]{.blue}. There is value in biased estimates, if they lead to a (sufficiently high) reduction of the variance.

- In any event, do not forget these are [estimates]{.orange}! By construction, there will be some variability among these values, therefore the discrepancies might be due to random [fluctuations]{.orange}.

- To choose among the estimates, we need some criterion, such as the [predictive performance]{.blue}, to identify the "best" estimator.

- This is [not]{.orange} the only criterion! There might be other considerations to keepn in mind, including the [interpretability]{.blue}, the [robustness]{.orange} of the method, and the [computational complexity]{.grey}.

- More generally, whenever different models display some differences, try to see this as an [opportunity]{.blue} to [learn something from the data]{.orange}. What could be the reason for such a discrepancy?

## How to interpret a wrong model?

- First of all, by wrong we mean "[biased]{.orange}". Therefore, being "wrong" could actually be an advantage [if]{.orange} the [focus]{.blue} is on [pure predictions]{.blue}.  

- Even when there is interest in predictions, we can often say something about the [associations]{.orange} between variables.

- The association structure is related to the model itself. For example, linear models capture the [linear part]{.orange} of the unknown and possibly more complex relationship $f(x)$. 

- At the very least, we can often say something about the main factors affecting the predictions.

- Obviously, we must be [very careful]{.orange} before making any [causal statement]{.orange}. Association $\neq$ causation. 

- Also, do not forget that the estimated effects are "*ceteris paribus*"! In other words, the association