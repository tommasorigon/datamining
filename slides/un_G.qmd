---
title: "Frequently Asked Questions"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    df-print: tibble
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_F_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 200
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 200
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_G.qmd", output = "../code/un_G.R", documentation = 0)
styler:::style_file("../code/un_G.R")
```

::: columns
::: {.column width="50%"}
![](img/russell.jpg){width="60%" fig-align="center"}

::: {style="font-size: 80%;"}
"*The whole problem with the world is that fools and fanatics are always so certain of themselves, and wiser people so full of doubts.*" 

[Bertrand Russell]{.grey}
:::
:::

::: {.column width="50%"}
- This is not a normal unit and has a more dynamic nature. 

- I will collect here [interesting]{.blue} questions made by students over the years.

- The questions are organized by topic, but some overlap will occur.

- Hopefully, this unit will form the basis of a [statistical culture]{.orange}. 

- Indeed, it is based on the knowledge of several statisticians. 
:::
:::

# The modeling process

::: {.callout-note collapse=true}

## What to do when different estimators lead to dissimilar conclusions?

- Well, it happens all the time in applied statistics. It even occurs in Statistics I!

- For example, you might get slightly different values (or even opposite signs) when looking at the estimated regression coefficients $\hat{\beta}_\text{ols}$, $\hat{\beta}_\text{pcr}$, $\hat{\beta}_\text{ridge}$.

- In linear models, do not assume the OLS estimator is "better" just because it is [unbiased]{.blue}. Biased estimates are helpful if they lead to a (sufficiently high) [reduction of the variance]{.orange}.

- In any event, remember these are, indeed, [estimates]{.orange}! By definition, there will be some variability; therefore, the [discrepancies]{.blue} might be due to random [fluctuations]{.orange}.

- To choose among different estimates, we need a criterion (or a combination of criteria), such as the [predictive performance]{.blue}. However, the out-of-sample error is [not]{.orange} the only relevant aspect! 

- There might be other considerations to keep in mind, including the [interpretability]{.blue}, the [robustness]{.orange} of the method, and the [computational complexity]{.grey}.

- Whenever different estimates display some differences, try to see it as an [opportunity]{.blue} to [learn something from the data]{.orange}. What could be the reason for such a discrepancy? What tells us about the data we are analyzing?
:::

::: {.callout-note collapse=true}

## How to interpret a wrong model?

- First of all, by wrong, we mean "[biased]{.orange}." Therefore, being "wrong" could be an advantage [if]{.orange} the [focus]{.blue} is on [pure predictions]{.blue}.  

- We can often say something about the [association]{.orange} between the response variable and the covariates, i.e., we can still try to [interpret]{.blue} the results. 

- Finding association is appropriate even in the presence of biased estimates, do not be discouraged by this aspect. However, you must be aware that the fitted model is, at best, an [approximation of reality]{.orange} (especially the biased ones), therefore you need to be [prudent]{.blue} in your conclusions.

- The specific [type of association]{.blue} reflects the [assumptions]{.blue} implied by the chosen model. For example, linear models capture the [linear part]{.orange} of the unknown and possibly more complex relationship $f(x)$, but they cannot capture interactions (unless manually included). 

- At the very least, we can often say something about the main factors affecting the predictions. 

- Obviously, we must be [very careful]{.orange} before making any [causal statement]{.orange}. Remember that association $\neq$ causation even when the model is well-specified.

- Do not forget that the estimated effects should be interpreted as "*ceteris paribus*"! This means the association between a variable and the response is often expressed conditionally on the other variables' effect. 
:::

<!-- ## What is the difference between an algorithm and a model? -->

<!-- ::: columns -->
<!-- ::: {.column width="50%"} -->
<!-- - [Model]{.blue}: a simplified *mathematical* representation of reality. -->

<!-- - [Algorithm]{.blue}: a step-by-step procedure that is useful to obtain the unknown parts of a model. -->
<!-- ::: -->

<!-- ::: {.column width="50%"} -->
<!-- [Example]{.orange}: linear models, neural networks, support vector machine, trees. -->

<!-- [Example]{.orange}: gradient descent, Newton-Raphson, pathwise coordinate descent, LAR. -->
<!-- ::: -->
<!-- ::: -->

::: {.callout-note collapse=true}
## Why do we care about simple models?

- Simple models are helpful to [explain]{.orange} reality, not just to make predictions.

- Simple models $\neq$ linear models. A tree with a few splits is indeed "simple." Some might say that parametric models relying on well-chosen assumptions are simple, but it depends. 

- Simple models tend to be more [robust]{.orange}, i.e., less affected by contamination and changes in the data generative process.

- Besides, the [understanding]{.blue} gained by fitting simple models is precious for estimating the more complex ones because it guides the next steps. 

- The modelling process is [iterative]{.blue} and [not a collection of independent]{.orange} and [separate approaches]{.orange}. You fit a model, check the results, and adjust your strategy. 

- Another practical concern is that senior statisticians know hundreds of different techniques. When all these ingredients are combined, they generate thousands, if not millions, of competing estimates. Hence, it becomes impossible to "use them all and pick the best performer." You need guidance to find the final answer. 
:::

::: {.callout-note collapse=true}

## What if simple models do not work well?

...for example, what to do when simple models do not predict well?

- In that case, of course, it is time to move on and try something more sophisticated. This often occurs in practice; only a few case-study can be solved with simple tools.

- However, you [should not]{.orange} fit a sophisticated model without trying the simple ones first. In other terms, I would fit a [deep neural network]{.blue} only after [everything else has failed]{.orange}, as a last resort.

- The statement "*data are complicated, therefore I need a complex model*" is probably the result of [hype/over-excitement]{.orange} for the new shiny machine learning tool. 

- At the cost of being repetitive, especially when data are complex, you should first check whether a simple solution exists. Sometimes, this is the case because "simple" models tend to have low variance. 

- A skilled statistician is not somebody who runs thousands of sophisticated models seeking for the one with the lowest error (i.e., the [button pressing culture](un_intro.html#press-the-button)). A skilled statistician is more like an [artisan]{.blue}, who uses her/his tools [when needed]{.orange} and with [intelligence]{.orange}, obtaining accurate predictions because she/he has a good understanding of the data, not as the result of brutal trial and error. 

- Finally, even when it turns out that a complex model improves the predictive performance, always ask yourself: ["Is it worth it?"]{.orange} The answer is often [context-dependent]{.blue}. 

- Once again, there might be other considerations to keep in mind: [interpretability]{.blue}, [robustness]{.orange}, [computational complexity]{.grey}. 
:::

::: {.callout-note collapse=true}
## What is a statistical model?

This question is too complicated to be answered using bullet points on a website. You may find some answers in the very instructive but technical paper by Peter McCullagh, published on the *Annals of Statistics*.

- McCullagh, P. (2002). [What is a statistical model?](https://projecteuclid.org/journals/annals-of-statistics/volume-30/issue-5/What-is-a-statistical-model/10.1214/aos/1035844977.full). Annals of Statistics **30** (5), 1225–67. 
:::

# Handling categorical variables

::: {.callout-note collapse=true}

## I have been told that linear models cannot use categorical variables

- Let me present you [dummy variables](https://en.wikipedia.org/wiki/Dummy_variable_(statistics))!

- This question was not made during an M.Sc. level course such as Data Mining at CLAMSES (otherwise, it would have been a bit depressing), but it happened in [other contexts]{.orange}.
:::

::: {.callout-note collapse=true}

## In the test set, there are a few values that were not observed in the training, what should I do?

- This often occurs when a categorical variable, say `city`, takes multiple values, e.g., `Milan`, `Naples`, `Rome`, etc. However, a rare value, such as `Isernia`, may be present in the training and not in the test. 

- The same issue may arise during cross-validation or when splitting the data into training, validation, and test sets. You might be tempted to [change the seed]{.orange} until you find a good split that does not display the issue. This practice is [wrong]{.orange}; see below.

- It might take forever to find a "good" seed. But then, if you keep sampling new splits until a specific condition is satisfied, you are implicitly [inducing dependence]{.orange} across observations, breaking the primary rationale of cross-validation or training/test set.

- It would be even worse to "manually" allocate units into each split because that would also break the primary assumption of cross-validation, i.e., the data should be [independently sampled]{.blue}.

- Fair enough, so what should we do? In general, it [depends]{.blue}. As a general strategy: be [creative]{.orange}, use your [judgment]{.blue}.

- Obviously, an option is to [omit the variable]{.orange} at the explicit cost of losing information. However, for example, if the variable is [binary]{.blue}, this is typically your only option. In this case, the discarded information is limited because the variable would be nearly constant. 

- Alternatively, you could [merge]{.blue} the [rare values]{.blue} with other categories. However, the aggregation criterion is crucial. You could merge rare categories into a residual class called `Other`. Alternatively, you could use your judgment to make [smarter choices]{.blue} (e.g., merging `Isernia` with `Campobasso`, both in Molise). 

- Other [details]{.orange}: is it preferable to perform the merging before estimating the model (using all the observations for fitting it) or after the estimation procedure (based on a reduced dataset) when making the predictions? This is another instance of the [bias-variance trade-off]{.orange}. Hence, there are no general answers. 

- The story does not end here. For instance, taking a [Bayesian approach]{.blue} might be another principled possibility: prior information could be exploited to help the estimation, even without data points in the training.
:::