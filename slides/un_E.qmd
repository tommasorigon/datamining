---
title: "The curse of dimensionality"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Universit√† degli Studi di Milano-Bicocca_"
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    df-print: tibble
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_E_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 200
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
    highlight-style: github
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 200
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_E.qmd", output = "../code/un_E.R")
styler:::style_file("../code/un_E.R")
```

::: columns
::: {.column width="30%"}

![](img/curse2.png){fig-align="center" width="80%"}

::: {style="font-size: 50%;"}
*"In view of all that we have said in the foregoing sections, the many obstacles we appear to have surmounted, what casts the pall over our victory celebration? It is the [curse of dimensionality]{.orange}, a malediction that has plagued the scientist from the earliest days."*
:::

[Richard Bellman]{.grey}
:::

::: {.column width="70%"}
- In [Unit C](un_C.html) we explored [linear]{.blue} predictive models for [high-dimensional]{.orange} data (i.e. $p$ is large).

- In [Unit D](un_D.html) we explored [nonparametric]{.blue} predictive models for [univariate]{.orange} data, placing almost no assumptions on $f(x)$.

- Thus, the expectations are that this unit should cover models with the following features:
    - [High-dimensional]{.orange}, with large $p$;
    - [Nonparametric]{.blue}, placing no assumptions on $f(\bm{x})$.
    
- The title of this unit, however, is not "*fully flexible high-dimensional models*". 

- Instead, it sounds like [bad news]{.orange} are coming. Let us see why, unfortunately, this will be indeed the case. 
:::
:::

## Multidimensional local regression

-  At least [conceptually]{.orange}, kernel methods could be applied with [two]{.orange} or [more covariates]{.orange}. 
    
. . .

-   To estimate $f$ on a specific point $\bm{x} = (x_1,\dots,x_p)$, a [natural
    extension]{.blue} of the Nadaraya-Watson takes the form 
    $$
    \hat{f}(x) = \frac{1}{\sum_{i'=1}^n w_{i'}(\bm{x})}\sum_{i=1}^n w_i(\bm{x}) y_i = \sum_{i=1}^n s_i(\bm{x}) y_i,
    $$ where the [weights]{.orange} $w_i(\bm{x})$ are defined as
    $$
    w_i(\bm{x}) = \prod_{j=1}^p \frac{1}{h_j} w\left(\frac{x_{ij} - x_j}{h_j}\right).
    $$

. . .
    
- This estimator is well-defined and it considers "[local]{.orange}" points in $p$ dimensions. 

- If the theoretical definition of multidimensional nonparametric tools is not a problem, why are they [not used]{.orange} in practice?

## The curse of dimensionality I

## The curse of dimensionality II

## The curse of dimensionality III

## The curse of dimensionality ([HTF, 2011]{.grey})

![](img/curse.png){fig-align="center" width="70%"}

## So what?
