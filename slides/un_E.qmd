---
title: "The curse of dimensionality"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_UniversitÃ  degli Studi di Milano-Bicocca_"
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    df-print: tibble
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_E_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 200
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
    highlight-style: github
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 200
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_E.qmd", output = "../code/un_E.R")
styler:::style_file("../code/un_E.R")
```

::: columns
::: {.column width="30%"}

![](img/curse3.jpg){fig-align="center" width="80%"}

::: {style="font-size: 50%;"}
*"In view of all that we have said in the foregoing sections, the many obstacles we appear to have surmounted, what casts the pall over our victory celebration? It is the [curse of dimensionality]{.orange}, a malediction that has plagued the scientist from the earliest days."*
:::

[Richard Bellman]{.grey}
:::

::: {.column width="70%"}
- In [Unit C](un_C.html) we explored [linear]{.blue} predictive models for [high-dimensional]{.orange} data (i.e. $p$ is large).

- In [Unit D](un_D.html) we explored [nonparametric]{.blue} predictive models for [univariate]{.orange} data, placing almost no assumptions on $f(x)$.

- Thus, the expectations are that this unit should cover models with the following features:
    - [High-dimensional]{.orange}, with large $p$;
    - [Nonparametric]{.blue}, placing no assumptions on $f(\bm{x})$.
    
- The title of this unit, however, is not "*fully flexible high-dimensional models*." 

- Instead, it sounds like [bad news]{.orange} is coming. Let us see why, unfortunately, this will be indeed the case. 
:::
:::

## Multidimensional local regression

-  At least [conceptually]{.orange}, kernel methods could be applied with [two]{.orange} or [more covariates]{.orange}. 
    
. . .

-   To estimate $f$ on a specific point $\bm{x} = (x_1,\dots,x_p)^T$, a [natural
    extension]{.blue} of the Nadaraya-Watson takes the form 
    $$
    \hat{f}(x) = \frac{1}{\sum_{i'=1}^n w_{i'}(\bm{x})}\sum_{i=1}^n w_i(\bm{x}) y_i = \sum_{i=1}^n s_i(\bm{x}) y_i,
    $$ where the [weights]{.orange} $w_i(\bm{x})$ are defined as
    $$
    w_i(\bm{x}) = \prod_{j=1}^p \frac{1}{h_j} w\left(\frac{x_{ij} - x_j}{h_j}\right).
    $$

. . .
    
- This estimator is well-defined and it considers "[local]{.blue}" points in $p$ dimensions. 

- If the theoretical definition of multidimensional nonparametric tools is not a problem, why are they [not used]{.orange} in practice?

## The curse of dimensionality I

- When the function $f(x)$ is entirely unspecified and a [local nonparametric]{.orange} method is used, a [dense]{.blue} dataset is needed to get a reasonably accurate estimate $\hat{f}(x)$.

. . .

- However, when $p$ grows, the data points becomes [sparse]{.orange}, even when $n$ is "big" in absolute terms. 

- In other words, a neighborhood of a generic point $x$ contains a small fraction of observations.

- Thus, a [neighborhood]{.blue} with a fixed percentage of data points is [no longer local]{.orange}. 

. . .

- To put it another way, to get a local neighborhood with $10$ data points along each axis, $10^p$ data points are needed in the corresponding $p$-dimensional neighborhood. 

. . .

- As a consequence, [much larger]{.orange} datasets are needed even for moderate $p$, because the sample size $n$ needs to grows [exponentially]{.blue} with $p$. 

## The curse of dimensionality II

::: incremental

- The following illustration may help clarify this notion of [sparsity]{.orange}. Let us consider data points that are uniformaly distributed on $(0, 1)^p$, that is $\bm{x}_i \overset{\text{iid}}{\sim} \text{U}^p(0, 1)$.

- Then, the [median distance]{.orange} from the origin $(0,\dots,0)^T$ to the [closest point]{.blue} is:
$$
\text{dist}(p, n) = \left\{1 - \left(\frac{1}{2}\right)^{1/n}\right\}^{1/p}.
$$

- In the [univariate]{.blue} case, such a median distance for $n = 100$ is quite small:
$$
\text{dist}(1, 100) = 0.007.
$$

- Conversely, when the [dimension]{.orange} $p$ [increases]{.orange}, the [median distance]{.blue} becomes:
$$
\text{dist}(2, 100) = 0.083, \quad \text{dist}(10, 100) = 0.608, \quad \text{dist}(50, 100) = 0.905.
$$
Note that we get $\text{dist}(10, 1000) = 0.483$ even with a much larger sample size. 

- Most points are [close to the boundary]{.blue}, making predictions very hard.
:::

## The curse of dimensionality III

::: incremental

- The following argument gives another [intuition]{.orange} of the curse of dimensionality. Let us consider again [uniform]{.blue} covariates $\bm{x}_i \overset{\text{iid}}{\sim} \text{U}^p(0, 1)$. 

- Let us consider a subcube which contains a fraction $r \in (0, 1)$ of the total number of observations $n$. In the univariate case ($p = 1$), the side of this cube is $r$. 

- In the more general case, it can be shown that on [average]{.orange}, the [side]{.blue} of the cube is
$$
\text{side}(r, p) = r^{1/p},
$$
which is again exponentially decreasing in $p$.

- Hence, when $p = 1$, the expected amount of points in the [local sub-interval]{.orange} $(0, 1/10)$ is again $1/10$. 

- Instead, when $p = 10$ the amount of point is the [local subcube]{.blue} $(0, 1)^{10}$ is
$$
n \left(\frac{1}{10}\right)^{10} = \frac{n}{1.000.000.000}.
$$

:::

## The curse of dimensionality ([HTF, 2011]{.orange})

![](img/curse.png){fig-align="center" width="70%"}

## Implications of the curse of dimensionality

::: incremental
- In the local [kernel smoothing]{.blue} approach, we can precisely quantify the impact of the [curse of dimensionality]{.orange} on the [mean squared error]{.blue}.

- Under some regularity conditions, the Nadaraya-Watson and the local linear regression estimator has [asymptotic mean squared error]{.blue}
$$
\mathbb{E}\left[\{f(x) - \hat{f}(x)\}^2\right] \sim n^{-4/5},
$$
which is [slower]{.orange} than the [parametric rate]{.orange} $n^{-1}$, but still reasonably fast for predictions.

- Conversely, it can be shown that in [high-dimension]{.orange} the [asymptotic rate]{.blue} becomes
$$
\mathbb{E}\left[\{f(\bm{x}) - \hat{f}(\bm{x})\}^2\right] \sim n^{-4/(4 + p)}.
$$

- Thus, the sample size for a $p$-dimensional problem to have the same accuracy as a sample size $n$ in one dimension is $m \propto n^{c p}$, with $c = (4 + p)/(5p) > 0$.

- To maintain a given degree of accuracy of a [local nonparametric]{.blue} estimator, the sample size must increase [exponentially]{.orange} with the dimension $p$. 
:::

## Escaping the curse

::: incremental
- Is there a "[solution]{.blue}" to the [curse of dimensionality]{.orange}? Well, yes... and no.

- If $f(x)$ is assumed to be [arbitrarily complex]{.orange} and our estimator $f(x)$ is [nonparametric]{.blue}, we are destined to face the curse.

- However, in [linear models]{.blue} you never encountered the curse of dimensionality. Indeed:
$$
\frac{1}{n}\sum_{i=1}^n\mathbb{E}\left\{(\bm{x}_i^T\beta - \bm{x}_i^T\hat{\beta})^2\right\} =  \sigma^2\frac{p}{n},
$$
which is increasing [linearly]{.blue} in $p$, but [not exponentially]{.orange}.

- Linear models make [assumptions]{.orange} and impose a [structure]{.blue}. If the assumptions are correct, the estimates exploit [global features]{.orange} and are less affected by the local sparsity.

- Nature is not necessarily a linear model, so we explored the nonparametric case.

- Nonetheless, making (correct) [assumptions]{.orange} and therefore imposing (appropriate) restrictions is [beneficial]{.blue}, to the extent that it is [unavoidable]{.orange} in high dimensions. 
:::

## Escaping the curse

::: incremental
- The [multidimensional]{.blue} methods you will study (GAM, trees, random forest, boosting, neural networks, etc.) deal with the curse of dimensionality by making (implicit) assumptions.

- These [assumptions]{.orange} differentiate because of:
    - The particular [nature]{.grey} of the knowledge they impose (e.g., no interactions, piecewise constant functions, etc.);
    - The [strength]{.orange} of this assumption;
    - The [sensibility]{.blue} of the methods to a potential violation of the assumptions.

- Thus, several alternative ideas and methods are needed; no single "best" algorithm exists. 

- This is why having a well-trained statistician on the team is important because they can identify the method that best suits the specific applied example.

- ...or at least, they will be aware of the [limitations]{.orange} of the methods. 
:::

## References

-   [Main references]{.orange}
    - **Section 4.3** of Azzalini, A. and Scarpa, B. (2011), [*Data
        Analysis and Data
        Mining*](http://azzalini.stat.unipd.it/Book-DM/), Oxford
        University Press.
    - **Section 2.5** of Hastie, T., Tibshirani, R., and Friedman, J.
        (2009), [*The Elements of Statistical
        Learning*](https://hastie.su.domains/ElemStatLearn/), Second
        Edition, Springer.
    - **Sections 4.5 and 5.12** of Wasserman, L. (2006), [*All of Nonparametric statistics*](https://link.springer.com/book/10.1007/0-387-30623-4), Springer.
