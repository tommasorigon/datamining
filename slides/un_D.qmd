---
title: "Nonparametric regression"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    df-print: tibble
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_D_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 200
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
    highlight-style: github
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 200
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_D.qmd", output = "../code/un_D.R", documentation = 0)
styler:::style_file("../code/un_D.R")
```

::: columns
::: {.column width="30%"}
![](img/nonparametric.png)

::: {style="font-size: 70%;"}
*"Nonparametric regression might, like linear regression, become an
object treasured both for its artistic merit as well as usefulness."*
:::

[Leo Breiman]{.grey}
:::

::: {.column width="70%"}
- This unit will cover the following [topics]{.orange}:

    -   Kernel methods and local regression;
    -   Regression splines;
    -   Smoothing splines.

-   Let us consider again the [relationship]{.blue} between a response
    variable $Y_i$ and a set of covariates $\bm{x}_i$: $$
      Y_i = f(\bm{x}_i) + \epsilon_i, \qquad
      $$ where $\epsilon_i$ are [iid]{.orange} with
    $\mathbb{E}(\epsilon_i) = 0$ and
    $\text{var}(\epsilon_i) = \sigma^2$.

-   We do not believe $f(\bm{x})$ is a polynomial nor it belongs to some
    parametric family of functions.

-   Can we fit a [nonparametric]{.blue} relationship that does
    [not]{.orange} make strong [assumptions]{.orange} on $f(\bm{x})$?
    Let us review some old datasets...
:::
:::

# Motivating applications

## The `cholesterol` data

::: columns
::: {.column width="40%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
#| warning: false

library(tidyverse)
library(ggplot2)
library(ggthemes)
rm(list = ls())
# The dataset can be downloaded here: https://tommasorigon.github.io/datamining/data/cholesterol.txt
dataset <- read.table("../data/cholesterol.txt", header = TRUE)
ggplot(data = dataset, aes(x = compliance, y = cholesterol.decrease)) +
  geom_point() +
  theme_minimal() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Compliance") +
  ylab("Cholesterol Decrease")
```
:::

::: {.column width="60%"}
-   In this first example, a drug called "cholestyramine" is administered to $n = 164$ men.

- We observe the pair $(x_i, y_i)$ for each man.

-   The response $y_i$ is the [decrease in cholesterol level]{.orange}
    over the experiment.

-   The covariate $x_i$ is a measure of [compliance]{.blue}.

-   We assume, as before, that the data are generated according to $$
        Y_i = f(x_i) + \epsilon_i, \quad i=1,\dots,n.
        $$

-   In [Unit B](un_B.html#another-example-cholesterol-data) we fit a
    [polynomial]{.orange} with degree $3$ on this data, although there
    was some [uncertainty]{.blue}.
:::
:::

## The `auto` dataset

::: columns

::: {.column width="60%"}
-   In [Unit A](un_A.html) we considered the `auto` dataset.

-   We wanted to model the relationship between `city.distance` ($y$)
    and `engine.size` ($x$).

-   The chosen model involved a [non-linear]{.blue} function $$
    Y_i = f(x_i) + \epsilon_i, \qquad i=1,\dots,n,
    $$ where $f(x)$ was "manually" selected.

-   There are [no]{.orange} reasons to believe that
    $f(x) = \alpha x^\beta$ or that $f(x)$ belongs to any other
    [parametric]{.orange} family.

-   We would like the data to "speak for themselves."
:::

::: {.column width="40%"}
```{r}
#| fig-width: 5
#| fig-height: 4

rm(list = ls())
# The dataset can also be downloaded here: https://tommasorigon.github.io/datamining/data/auto.txt
auto <- read.table("../data/auto.txt", header = TRUE) %>% select(city.distance, engine.size, n.cylinders, curb.weight, fuel)

ggplot(data = auto, aes(x = engine.size, y = city.distance)) +
  geom_point() +
  theme_minimal() +
  xlab("Engine size (L)") +
  ylab("Urban distance (km/L)")
```
:::

:::

## The `mcycle` dataset

::: columns
::: {.column width="40%"}
```{r}
#| fig-width: 5
#| fig-height: 4

rm(list = ls())
dataset <- MASS::mcycle

x <- dataset$times
y <- dataset$accel

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_point() +
  theme_minimal() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```
:::

::: {.column width="60%"}
-   Data consist of variables $y$ [accelerometer]{.blue} (`accel`)
    readings, taken through time $x$ (`times`).

-   The $n = 133$ observations were measured during a simulated
    [motor-cycle crash]{.blue} experiment, for testing the
    [efficacy]{.orange} of [crash helmets]{.orange}.

-   Some characteristics of the data:

    -   The time points are [not regularly spaced]{.orange} and
        sometimes there are [multiple observations]{.blue};
    -   The observations are subject to [error]{.orange};
    -   The errors $\epsilon_i$ are probably heteroscedastic, but let
        us ignore this now.

-   It is of interest to discern the [general shape]{.blue} of the
    underlying acceleration curve.
:::
:::

## Old friends: polynomials

-   In the `mcycle` dataset, it is [not]{.orange} obvious which
    [parametric]{.orange} function we should consider, therefore this
    route is not an option.

. . .

-   In [theory]{.orange} [polynomials]{.blue} can
    [approximate]{.blue} a large class of functions, as a consequence of
    Taylor's expansion theorem.

-   In the statistical [practice]{.orange}, however, polynomial
    regression is not very well suited for modeling complex
    relationships.

. . .

-   When performing flexible regression, we expect the prediction
    at $x_i$ to depend on observations close to $x_i$. However,
    polynomials are [not local]{.orange}.

-   Instead, in polynomial regression points that are far away from
    $x_i$ have a big impact on $\hat{f}(x_i)$. This produces [spurious
    oscillations]{.blue} at the boundaries and [unstable]{.orange}
    estimates.

-   This is known as [Runge's
    phenomenon](https://en.wikipedia.org/wiki/Runge%27s_phenomenon) in
    numerical analysis.

## Old friends: polynomials (`mcycle` data)

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center

# Degrees of the polynomials
degree_list <- c(9, 11, 13, 15, 17, 19)

# I am using 30.000 obs to improve the quality of the graph
times_seq <- seq(from = min(dataset$times), to = max(dataset$times), length = 30000)

# Actual fitting procedure
data_pred <- NULL
for (degree in degree_list) {
  # Fitting a polynomial of degree p - 1
  fit <- lm(accel ~ poly(times, degree = degree, raw = FALSE), data = dataset)
  # Fitted values
  y_hat <- predict(fit, newdata = data.frame(times = times_seq))
  data_pred <- rbind(data_pred, data.frame(x = times_seq, y_hat = y_hat, degree = paste("Number of parameters p:", degree + 1)))
}

# Graphical adjustment to get the plots in the right order
data_pred$degree <- factor(data_pred$degree)

# Final plot
ggplot(data = data_pred) +
  geom_line(aes(x = x, y = y_hat, col = degree)) +
  geom_point(data = dataset, aes(x = times, y = accel), size = 0.4) +
  theme_light() +
  theme(legend.position = "none") +
  facet_wrap(. ~ degree, ncol = 3) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("x") +
  ylab("y") # Manual identification of an "interesting" region
```

# Local regression

## The regression function

::: incremental
-   The only assumption we are making in this Unit is the following
    [additive]{.orange} structure $$
      Y_i = f(x_i) + \epsilon_i, \qquad i=1,\dots,n,
    $$ where $\epsilon_i$ are [iid]{.orange} with
    $\mathbb{E}(\epsilon_i) = 0$ and
    $\text{var}(\epsilon_i) = \sigma^2$. This structure can be relaxed even further. 

-   Let $\tilde{Y}_i$ be a new data point. In [Unit B](un_B.html#regression-under-quadratic-loss-ii) we showed
    that under the [quadratic loss]{.orange}$$
    \mathbb{E}\left[\{\tilde{Y}_i - \hat{f}(x_i)\}^2\right],
    $$ the best prediction $\hat{f}(x_i)$, i.e. the one minimizing the
    loss, coincides with $$
    \hat{f}(x_i) = \mathbb{E}(\tilde{Y}_i) = f(x_i),
    $$ which is the [conditional expectation]{.blue} of $Y_i$ given the
    value $x_i$, called [regression function]{.blue}.

-   The regression function $f(x_i) = \mathbb{E}(\tilde{Y}_i)$ is the
    [optimal prediction]{.blue} even in presence of
    [heteroschedastic]{.orange} data or when the above
    [additive]{.orange} decomposition does [not hold]{.orange}.
:::

## Local estimates of the prediction

-   We do not know $f(x)$, but the previous formulas suggest that we
    could consider an [arithmetic average]{.blue} of the data points.

-   Hence, a [prediction]{.orange} for a generic value $x$ could be
    obtained as follows: $$
    \hat{f}(x) = \frac{1}{n_x}\sum_{i : x_i = x} y_i, \qquad n_x = \sum_{i=1}^n I(x_i = x).
    $$

. . .

-   This idea, unfortunately, [does not work]{.orange} in most practical
    cases.

-   Indeed, in a typical dataset it is very unlikely that there exist
    multiple observations [exactly equal]{.orange} to $x$ among the
    points $(x_i, y_i)$.

-   Even if there were values such that $x_i = x$, the [sample
    size]{.blue} $n_x$ would be so [small]{.orange} (e.g. $n_x = 1$)
    that the variance of $\hat{f}(x)$ would be extremely high, making
    this estimator useless.

. . .

-   However, this "local average" idea seems [intuitively
    appealing]{.orange}. Can we "fix" it?

## K-nearest neighbours

-   Instead of considering the values exactly equal to $x$, we could
    identify the pairs $(x_i, y_i)$ that are [close]{.blue} to (i.e. in
    a [neighbour]{.orange} of) $x$.

-   A natural measure of proximity between $x$ and the data points $x_i$
    is the [Euclidean distance]{.orange} $|x_i - x|$, but in principle
    any other metric could be used.

. . .

-   We consider an average of the [$k$ values]{.orange} $y_i$ whose
    $x_i$ are [nearest]{.orange} to $x$, that is: $$
    \hat{f}(x) = \frac{1}{|\mathcal{N}_x|}\sum_{i \in \mathcal{N}_x} y_i,
    $$ where $\mathcal{N}_x$ is indeed the set of $k$ points nearest to
    $x$ in Euclidean distance.

-   This method is called [$k$-nearest neighbours]{.blue} (KNN).

## K-nearest neighbours ($k = 6$)

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
library(kknn)
fit_knn <- fitted(kknn(accel ~ times, train = dataset, test = data.frame(times = times_seq), kernel = "rectangular", k = 6))

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_ribbon(aes(xmin = 51.7, xmax = 57.7), fill = "#fc7d0b", alpha = 0.6) +
  geom_ribbon(aes(xmin = 19, xmax = 21), fill = "#fc7d0b", alpha = 0.6) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = times_seq, y = fit_knn), aes(x = x, y = y), col = "#1170aa") +
  geom_vline(xintercept = 20, lty = "dashed", linewidth = 0.4) +
  geom_vline(xintercept = 54.7, lty = "dashed", linewidth = 0.4) +
  theme_minimal() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Comments and limitations about the KNN method

-   The number of neighbours $k$ influences how "[local]{.orange}" is
    the estimate.

-   When $k$ is low, the KNN estimator $\hat{f}(x)$ has [high
    variance]{.orange}. The extreme case $k = 1$ corresponds to an
    "average" of a single data point.

-   When $k$ is high, the KNN estimator $\hat{f}(x)$ is not local and it
    has [high bias]{.blue}. The extreme case $k = n$ produces a
    constant, i.e., the average of all the observations.

. . .

-   Thus, there is a [bias-variance trade-off]{.orange} in the choice of
    $k$, which should be selected, e.g., via [cross-validation]{.blue}.

. . .

-   The $k$-nearest neighbors produce a [sensible result]{.blue}, but
    the method [can be improved]{.orange}.

-   The [blue curve]{.blue} is bumpy, because $\hat{f}(x)$ is
    [discontinuous]{.orange} in $x$.

-   Indeed, as we move $x$ from left to right, the $k$-nearest
    neighborhood remains constant until a new point $x_i$ to the right
    of $x$ is included, and one to the left is excluded.

-   This discontinuity is [ugly]{.orange} and [unnecessary]{.orange}. We
    are looking instead for a [smooth]{.blue} prediction.

## Nadaraya-Watson estimator

-   The [Nadaraya-Watson]{.blue} estimator addresses the aforementioned
    issues of the KNN method. It is a [weighted average]{.orange} $$
    \hat{f}(x) = \frac{1}{\sum_{i'=1}^n w_{i'}(x)}\sum_{i=1}^n w_i(x) y_i = \sum_{i=1}^n s_i(x) y_i,
    $$ where $s_i(x) = w_i(x) / \sum_{i'=1}^n w_{i'}(x)$ are the
    [normalized weights]{.blue}.

. . .

-   The values $w_i(x) \ge 0$ are chosen so that the points $x_i$
    [close]{.orange} to $x$ are [weighted more]{.orange}.

. . .

-   A convenient way of selecting these weights is through [kernel
    functions]{.blue}: $$
    w_i(x) = \frac{1}{h}w\left(\frac{x_i - x}{h}\right), \qquad i=1,\dots,n,
    $$ where $w(\cdot)$ is a [density]{.orange} function,
    [symmetric]{.orange} around the origin, called kernel in this
    context.

-   The value $h > 0$ is a [scale factor]{.blue}, sometimes called
    [bandwidth]{.orange} or [smoothing parameter]{.orange}.

## Nadaraya-Watson estimator: comments

-   The fitted function $\hat{f}(x)$ is [continuous]{.blue} and is
    obtained by computing several weighted averages, one for each value
    of $x$.

-   A popular kernel is the [Gaussian kernel]{.orange}, that is: $$
    w_i(x) = \frac{1}{h} \phi\left(\frac{x_i - x}{h}\right), \qquad i=1,\dots,n,
    $$ therefore $h^2$ represents the [variance]{.blue}. We will discuss
    alternative choices later on.

. . .

-   The most important factor, however, is not the functional form of
    $w(\cdot)$, but rather the [smoothing parameter]{.orange} $h$, which
    is a complexity parameter.

-   Indeed, $h$ defines the "[smoothing window]{.blue}" on the $x$-axis,
    i.e. the relevant data points that are considered for $\hat{f}(x)$.

-   As with any complexity parameter, $h$ should be chosen via
    cross-validation or related ideas.

## Nadaraya-Watson (Gaussian kernel)

::: panel-tabset
## Smoothing $h = 1$

```{r}
#| fig-width: 7.8
#| fig-height: 4
#| fig-align: center
#| message: false
library(KernSmooth)

h_param <- 1
band <- 4 * qnorm(0.75) * h_param # Bandwidth as parametrized in ksmooth
fit_nw <- ksmooth(x, y, kernel = "normal", bandwidth = band, x.points = times_seq)

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_ribbon(aes(xmin = qnorm(0.05, 20, sd = h_param), xmax = qnorm(0.95, 20, sd = h_param)), fill = "#fc7d0b", alpha = 0.6) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = fit_nw$x, y = fit_nw$y), aes(x = x, y = y), col = "#1170aa") +
  geom_vline(xintercept = 20, lty = "dashed", linewidth = 0.4) +
  theme_minimal() +
  geom_function(fun = function(x) 150 * dnorm(x, 20, h_param) - 134, linetype = "dotted", n = 500, xlim = c(qnorm(0.001, 20, sd = h_param), xmax = qnorm(0.999, 20, sd = h_param))) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Smoothing $h = 0.3$

```{r}
#| fig-width: 7.8
#| fig-height: 4
#| fig-align: center

h_param <- 0.3
band <- 4 * qnorm(0.75) * h_param # Bandwidth as parametrized in ksmooth
fit_nw <- ksmooth(x, y, kernel = "normal", bandwidth = band, x.points = times_seq)

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_ribbon(aes(xmin = qnorm(0.05, 20, sd = h_param), xmax = qnorm(0.95, 20, sd = h_param)), fill = "#fc7d0b", alpha = 0.6) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = fit_nw$x, y = fit_nw$y), aes(x = x, y = y), col = "#1170aa") +
  geom_vline(xintercept = 20, lty = "dashed", linewidth = 0.4) +
  theme_minimal() +
  geom_function(fun = function(x) 50 * dnorm(x, 20, h_param) - 134, linetype = "dotted", n = 500, xlim = c(qnorm(0.001, 20, sd = h_param), xmax = qnorm(0.999, 20, sd = h_param))) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Smoothing $h = 2$

```{r}
#| fig-width: 7.8
#| fig-height: 4
#| fig-align: center
h_param <- 2
band <- 4 * qnorm(0.75) * h_param # Bandwidth as parametrized in ksmooth
fit_nw <- ksmooth(x, y, kernel = "normal", bandwidth = band, x.points = times_seq)

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_ribbon(aes(xmin = qnorm(0.05, 20, sd = h_param), xmax = qnorm(0.95, 20, sd = h_param)), fill = "#fc7d0b", alpha = 0.6) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = fit_nw$x, y = fit_nw$y), aes(x = x, y = y), col = "#1170aa") +
  geom_vline(xintercept = 20, lty = "dashed", linewidth = 0.4) +
  theme_minimal() +
  geom_function(fun = function(x) 180 * dnorm(x, 20, h_param) - 134, linetype = "dotted", n = 500, xlim = c(qnorm(0.001, 20, sd = h_param), xmax = qnorm(0.999, 20, sd = h_param))) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Smoothing $h = 4$

```{r}
#| fig-width: 7.8
#| fig-height: 4
#| fig-align: center
h_param <- 4
band <- 4 * qnorm(0.75) * h_param # Bandwidth as parametrized in ksmooth
fit_nw <- ksmooth(x, y, kernel = "normal", bandwidth = band, x.points = times_seq)

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_ribbon(aes(xmin = qnorm(0.05, 20, sd = h_param), xmax = qnorm(0.95, 20, sd = h_param)), fill = "#fc7d0b", alpha = 0.6) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = fit_nw$x, y = fit_nw$y), aes(x = x, y = y), col = "#1170aa") +
  geom_vline(xintercept = 20, lty = "dashed", linewidth = 0.4) +
  theme_minimal() +
  geom_function(fun = function(x) 600 * dnorm(x, 20, h_param) - 134, linetype = "dotted", n = 500, xlim = c(qnorm(0.001, 20, sd = h_param), xmax = qnorm(0.999, 20, sd = h_param))) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```
:::

## Local linear regression I

::: incremental
-   [Local linear regression]{.blue} is a refinement of the
    Nadaraya-Watson estimator that has typically [lower bias]{.orange},
    especially at the [boundaries]{.orange}, without noticeable
    increases in variance.

-   If $f(x)$ is differentiable, then it can be [approximated]{.orange}
    with a linear function tangent in $x_0$: $$
    f(x) = \underbrace{f(x_0)}_{\beta_1} + \underbrace{f'(x_0)}_{\beta_2}(x - x_0) + \text{rest}.
    $$

-   Hence, instead of computing a [local average]{.orange}
    ($\beta_2 = 0$), we consider a [local linear model]{.blue}. In other
    words, for every $x$ we seek the coefficients solving: $$
    \hat{\beta}(x) = \left(\hat{\beta}_1(x), \hat{\beta}_2(x)\right) = \arg\min_{(\beta_1, \beta_2)} \sum_{i=1}^n \textcolor{darkblue}{w_i(x)}\left\{y_i - \beta_1 - \textcolor{red}{\beta_2(x_i - x)}\right\}^2.
    $$

-   Once the parameter $\hat{\beta}_1(x)$ and $\hat{\beta}_2(x)$ are obtained, the
    local [linear regression estimator]{.blue} is $$
    \hat{f}(x) = \hat{\beta}_1(x) + \hat{\beta}_2(x) (x - x) = \hat{\beta}_1(x).
    $$
:::

## Local linear regression II

-   The local linear regression, as we have seen in [Unit A](un_A.html),
    has an [explicit solution]{.orange}: $$
    \hat{\beta}(x) = (\bm{X}_x^T\bm{W}_x\bm{X}_x)^{-1}\bm{X}_x^T\bm{W}_x\bm{y},
    $$ where the rows of $\bm{X}_x$ are $\bm{x}_{i,x} = (1, x_i - x)$ and
    $\bm{W}_x = \text{diag}\{w_1(x),\dots,w_n(x)\}$.

-   In practice, we do [not]{.orange} need to solve this [linear
    algebra]{.orange} problem. An even more explicit and
    [non-iterative]{.orange} solution can be found (see Exercises).

. . .

::: callout-note
#### Theorem (Local linear smoothing)

The local linear regression smoother, evaluated in $x$, admits an
[explicit expression]{.blue}: $$
\hat{f}(x) = \frac{1}{n}\sum_{i=1}^n  \frac{w_i(x) \{a_2(x) - (x_i - x) a_1(x)\}}{a_2(x)a_0(x) - a_1(x)^2 } y_i = \sum_{i=1}^n s_i(x) y_i,
$$ where $a_j(x) = n^{-1}\sum_{i=1}^n w_i(x) (x_i - x)^j$, for
$j=0,1,2$.
:::

## Local linear regression ($h = 1.46$, Gaussian kernel)

```{r}
loclin1 <- function(x, y, bandwidth, x0){
  w <- dnorm(x, mean = x0, sd = bandwidth)
  a0 <- mean(w)
  a1 <- mean(w * (x - x0))
  a2 <- mean(w * (x - x0)^2)
  mean( ((a2 - a1 * (x - x0)) * w * y) / (a2 * a0 - a1^2))
}

loclin <- Vectorize(loclin1, vectorize.args = "x0")

S_diag <- function(x, y, bandwidth){
  n <- length(y)
  S_diag <- numeric(n)
  for(i in 1:n){
    x0 <- x[i]
    w <- dnorm(x, mean = x0, sd = bandwidth)
    a0 <- mean(w)
    a1 <- mean(w * (x - x0))
    a2 <- mean(w * (x - x0)^2)
    S_diag[i] <- ((a2 - a1 * (x[i] - x0)) * w[i]) / (a2 * a0 - a1^2) / n
  }
  S_diag
}
```

```{r}
# Code execution and storage of the interesting quantities
bandwidth_list <- exp(seq(from = -1, to = 2, length = 100))
data_goodness <- data.frame(bandwidth = bandwidth_list)
for (i in 1:length(bandwidth_list)) {
  # Fitting a polynomial of degree p -1
  lev <- S_diag(x, y, bandwidth_list[i])
  fit <- loclin(x, y, bandwidth_list[i], x)
  res_loo <- (y - fit) / (1 - lev)
  data_goodness$df[i] <- sum(lev)
  data_goodness$LOO_CV[i] <- mean(res_loo^2)
  data_goodness$LOO_CV_SE[i] <- sd(res_loo) / sqrt(nrow(dataset))
  data_goodness$GCV[i] <- mean(((y - fit) / (1 - sum(lev) / nrow(dataset)))^2)
}
```


```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center
#| 
id_opt <- which.min(data_goodness$LOO_CV)
h_opt <- data_goodness$bandwidth[id_opt]
df_opt <- data_goodness$df[id_opt]

x_seq <- seq(from = min(x), to = max(x), length = 2000)
fit_locpoly <- loclin(x, y, bandwidth = h_opt, x0 = x_seq)

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = x_seq, y = fit_locpoly), aes(x = x, y = y), col = "#1170aa") +
  theme_minimal() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Linear smoothers I

-   The Nadaraya-Watson estimator and local linear regression are
    special instances of [linear smoothers]{.blue}, which are estimators
    having the following form: $$
    \hat{f}(x) = \sum_{i=1}^ns_i(x) y_i.
    $$

. . .

-   We will study other members of this class, such as regression and
    smoothing splines.

-   Polynomial regression, ridge regression, [Gaussian processes]{.blue}
    and [moving averages]{.orange} are also linear smoothers.

. . .

-   The [mean]{.blue} (and hence the bias), and the [variance]{.orange}
    of a linear smoother can be easily obtained: $$
    \mathbb{E}\{\hat{f}(x)\} = \sum_{i=1}^n s_i(x)f(x_i), \qquad \text{var}\{\hat{f}(x)\} = \sigma^2\sum_{i=1}^ns_i(x)^2.
    $$

## Linear smoothers II

-   In linear smoothers, we can express the predicted values
    $\hat{\bm{y}}$ using [matrix notation]{.orange} $$
    \hat{\bm{y}} = \sum_{i=1}^n\bm{s}_i \bm{y} = \bm{S}\bm{y}, \qquad \bm{s}_i = (s_1(x_i), \dots, s_n(x_i))^T,
    $$ where $\bm{S} = (\bm{s}_1,\dots,\bm{s}_n)^T$ is the so-called
    $n \times n$ [smoothing]{.blue} matrix.

. . .

-   Each row of the smoothing matrix $\bm{s}_i$ is called [equivalent
    kernel]{.orange} for estimating $\hat{f}(x_i)$; in the Nadaraya
    Watson estimator $\bm{s}_i$ is indeed a normalized kernel.

. . .

-   The weights of all the smoothers we will use are such that
    $\sum_{i=1}^ns_i(x) = 1$ for all $x$.

-   Hence, the smoother [preserves constant curves]{.blue}, namely if
    all $y_i = c$, then $\hat{f}(x) = c$.

## On the choice of the kernel

-   As mentioned before, the choice of the kernel is [not
    crucial]{.orange}. Some alternatives are:

| Kernel       | $w(x)$                                                    | Support      |
|---------------------|-----------------------------|-----------------------|
| Gaussian     | $\frac{1}{\sqrt{2 \pi}}\exp{\left(-\frac{x^2}{2}\right)}$ | $\mathbb{R}$ |
| Rectangular  | $\frac{1}{2}$                                             | $(-1, 1)$    |
| Epanechnikov | $\frac{3}{4}(1 - x^2)$                                    | $(-1, 1)$    |
| Bi-quadratic | $\frac{15}{16}(1 - x^2)^2$                                | $(-1, 1)$    |
| Tri-cubic    | $\frac{70}{81}(1 - |x|^3)^3$                              | $(-1, 1)$    |

. . .

-   Some [asymptotic]{.blue} considerations lead to the choice of the
    "[optimal]{.orange}" Epanechnikov kernel.

. . .

-   [Bounded]{.blue} kernels have [computational]{.blue} advantages,
    because one needs to compute averages of a limited number of data
    points.

-   On the other hand, bounded kernels may lead to [discontinuous
    derivatives]{.orange} of $\hat{f}(x)$ that could be unappealing in
    certain contexts.

## Bias-variance tradeoff

::: callout-warning
#### Theorem (Fan and Gijbels, 1996, Theorem 3.1)

Let $(X_i, Y_i)$ be iid random vectors with $g(x)$ denoting the
[marginal density]{.orange} of $X_i$. The conditional law is such that
$Y_i = f(X_i) + \epsilon_i$, with $\epsilon_i$ iid and
$\mathbb{E}(\epsilon_i) = 0$, $\text{var}(\epsilon_i) = \sigma^2$.

Moreover, suppose $g(x) > 0$ and that $g(\cdot)$ and $f''(\cdot)$ are
continuous in a neighborhood of $x$. Then, as $h \rightarrow 0$ and
$n h \rightarrow \infty$ we have that for the [local linear
regression]{.blue} $\hat{f}(x)$ the [bias]{.orange} is $$
\mathbb{E}\{\hat{f}(x) - f(x) \} \approx  \frac{\textcolor{red}{h^2}}{2}\sigma^2_w f''(x),
$$ where $\sigma^2_w = \int z^2 w(z)\mathrm{d}z$. In addition, the
[variance]{.blue} is $$
\text{var}\{\hat{f}(x)\} \approx \frac{\sigma^2}{\textcolor{darkblue}{n h}} \frac{\alpha_w}{g(x)},
$$ where $\alpha_w = \int w^2(z)\mathrm{d}z$.
:::

## Bias-variance tradeoff II

-   The previous theorem shows that [bias]{.orange} is of [order
    $h^2$]{.orange} and the [variance]{.blue} is of [order
    $(1 / nh)$]{.blue}.

-   Once again, there is a trade-off because we would like
    $h \rightarrow 0$ but, at the same time, we need to keep the
    variance under control.

. . .

-   We can select $h$ so that the [asymptotic mean squared error]{.blue}
    is [minimal]{.orange}. This leads to the following optimal choice
    for the bandwidth: $$
    h_\text{opt}(x) = \left(\frac{1}{n} \frac{\sigma^2 \alpha_w}{\sigma^4_w f''(x)^2 g(x)}\right)^{1/5}.
    $$

. . .

-   Unfortunately, $h_\text{opt}(x)$ is of [little practical
    utility]{.orange}, as it involves the [unknown]{.orange} terms
    $f''(x)$, $g(x)$ and $\sigma^2$. However, it highlights two
    important facts:

-   The bandwidth $h$ should decrease at the rate $n^{-1/5}$, i.e.
    [quite slowly]{.blue}.

-   If we plug-in $h_\text{opt}(x)$ into the bias/variance formulas, we
    get that the mean squared error tends to $0$ at the rate $n^{-4/5}$,
    which is much [slower]{.orange} than the [parametric]{.orange} case
    $n^{-1}$.

## ☠️ - Bias reduction of local linear regression

- Compared to the Nadaraya-Watson estimator,
    local linear regression corrects the [first-order]{.orange} term of the [bias]{.orange},
    without affecting the variance sensibly.

-   Indeed, it can be shown that the [asymptotic variance]{.blue} of
    Nadaraya-Watson and local linear regression is the [same]{.blue},
    but the [asymptotic bias]{.orange} is [different]{.orange}.

. . .

-   To get an intuition of this, consider the following Taylor expansion
    for $\mathbb{E}\{\hat{f}(x)\}$ around $x$, and for the local
    linear regression case: $$
    \begin{aligned}
    \mathbb{E}\{\hat{f}(x)\} &= \sum_{i=1}^n s_i(x)f(x_i) \\
    & = f(x)\underbrace{\sum_{i=1}^ns_i(x)}_{=1} + f'(x) \underbrace{\sum_{i=1}^n(x_i - x)s_i(x)}_{=0} + \frac{f''(x)}{2}\sum_{i=1}^n(x_i - x)^2s_i(x) + \text{rest}.
    \end{aligned}
    $$

-   It can be shown with some algebra that the first-order term
    simplifies ($=0$) in the local linear regression case, but it doesn't
    for the Nadaraya-Watson, therefore [reducing the bias]{.orange}.

## Choice of the bandwidth I

-   In practice, we need to choose the bandwidth by other means. A first
    solution is based on [information criteria]{.blue} such as the $C_p$
    or the AIC/BIC.

-   However, as before, their usage requires a suitable notion of
    [effective degrees of freedom]{.orange}.

. . .

::: callout-note
#### Effective degrees of freedom for linear smoothers

Let $\hat{f}(x) = \sum_{i=1}^ns_i(x)y_i$ be a linear smoother. Then the
effective degrees of freedom are $$
\text{df}_\text{sm} = \frac{1}{\sigma^2}\sum_{i=1}^n\text{cov}(Y_i, \hat{f}(x_i)) =\frac{1}{\sigma^2} \text{tr}\{\text{cov}(\bm{Y}, \bm{S}\bm{Y})\} = \frac{\sigma^2}{\sigma^2}\text{tr} (\bm{S}) = \text{tr} (\bm{S}).
$$
:::

. . .

-   Some authors proposed to use $\text{tr}(\bm{S}\bm{S}^T)$ or $\text{tr}(2\bm{S}-\bm{S}\bm{S}^T)$, but the
    connection with the [optimism]{.orange} and the definition of
    effective degrees of freedom is less clear.

## Choice of the bandwidth II

- Cross-validation is another option for selecting the bandwidth $h$. For [most]{.orange} [linear smoothers]{.blue} there is a brilliant [computational shortcut]{.orange} for the leave-one-out case.

. . .

- Any reasonable linear smoother is constant preserving, that is $\sum_{j=1}^ns_j(x) = 1$ for all $x$. Moreover, for most linear smoothers the following property holds: 
$$
\hat{y}_{-i} = \frac{1}{1 - s_i(x_i)}\sum_{j \neq i} s_j(x_i) y_j.
$$
- In other words, the [leave-one-out predictions]{.orange} can be obtained by [excluding]{.orange} the $i$th observation and [re-normalizing]{.blue} the weights.

. . .

- A linear smoother is called [projective]{.orange} if it has the above property. 

- [All]{.orange} the [linear smoothers]{.orange} presented in this unit (Nadaraya-Watson, local linear regression, regression an smoothing splines) are projective (see Exercises).

## Choice of the bandwidth III

::: callout-note
#### Theorem (LOO-CV for linear smoothers)

Let $\hat{y}_{-i} = \hat{f}_{-i}(x_i)$ be the leave-one-out predictions
of a [projective linear smoother]{.blue} and let $\hat{\bm{y}} = \bm{S}\bm{y}$ be
the predictions of the full model. Then: $$
y_i - \hat{y}_{-i} = \frac{y_i - \hat{y}_i}{1 - [\bm{S}]_{ii}}, \qquad i=1,\dots,n.
$$ Therefore, the leave-one-out mean squared error is
$$\widehat{\mathrm{Err}} = \frac{1}{n}\sum_{i=1}^n \left(\frac{y_i - \hat{y}_i}{1 - [\bm{S}]_{ii}}\right)^2.$$
:::

## Choice of the bandwidth IV


```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center

# Organization of the results for graphical purposes
data_bv <- data.frame(df = data_goodness$df, GCV = data_goodness$GCV, LOO_CV = data_goodness$LOO_CV, SE = data_goodness$LOO_CV_SE)
data_bv <- reshape2::melt(data_bv, id = c("df", "SE"))
data_bv$SE[data_bv$variable == "GCV"] <- NA
levels(data_bv$variable) <- c("GCV", "LOO-CV")
colnames(data_bv) <- c("df", "SE", "Error term", "value")

ggplot(data = data_bv, aes(x = df, y = value, col = `Error term`)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = df_opt, linetype = "dotted", col ="#fc7d0b") +
  theme_light() +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Effective degrees of freedom (df)") +
  ylab("Mean Squared Error (MSE)") 
```


## Loess

-   Sometimes it is convenient to choose $h$ [adaptively]{.orange}, i.e.
    specifying a variable bandwidth $h(x)$ that depends on the local
    density of the data.

-   Indeed, recall that the asymptotic variance depends on the sampling
    design of the $x_i$s $$
    \text{var}\{\hat{f}(x)\} \approx \frac{\sigma^2}{n h} \frac{\alpha_w}{\textcolor{darkblue}{g(x)}}.
    $$

. . .

-   The `loess` (Cleveland, 1979) considers a [fixed percentage]{.blue}
    of data points (assuming a bounded kernel is used), which
    automatically induces a [variable bandwidth]{.orange}, as in KNN.

. . .

-   Moreover, the `loess` algorithm combines the [variable
    bandwidth]{.blue} with some [robust estimation]{.orange} ideas, so
    that outliers less influence the resulting estimate.

. . .

-   `loess` is a short-hand for "[l]{.orange}ocally [w]{.orange}eighted
    [e]{.orange}stimated [s]{.orange}catterplot [s]{.orange}moothing".

## Local likelihoods

-   The concept of local regression and varying coefficients is
    extremely [broad]{.orange}.

-   In principle, any [parametric]{.orange} model can be made
    [local]{.blue} as long as it accommodates [weights]{.blue}.

. . .

-   Let us consider a [logistic regression]{.blue} with a single predictor. For
    every value $x$ we seek $$
    \hat{\beta}(x)= \arg\max_{(\beta_1, \beta_2)}\sum_{i=1}^nw_i(x)\left[y_i (\beta_1 + \beta_2 x_i) - \log\{1 + \exp(\beta_1 + \beta_2 x_i)\}\right],
    $$ whose solution can be found using iteratively re-weighted least
    squares.

. . .

-   The [computations]{.orange} and the [theory]{.blue} are not as straightforward and limpid as in the regression case,
    but they do hold in an [approximate]{.orange} sense.

- Once again, the conceptual scheme is: (i) perform a [quadratic approximation]{.orange} of the log-likelihood; (ii) proceed as in the [regression]{.blue} case. 


## The bivariate case

-   Local linear regression can be applied when [two]{.orange} or [more
    covariates]{.orange}, say $p$, are used. Let us begin with two
    covariates so that $$
    y_i = f(x_{i1}, x_{i2}) + \epsilon_i.
    $$

-   To estimate $f$ on a specific point $\bm{x} = (x_1,x_2)^T$, a [natural
    extension]{.blue} of local linear regression takes the form $$
      \hat{\beta}(\bm{x}) = \arg\min_{(\beta_1, \beta_2, \beta_3)} \sum_{i=1}^n w_i(\bm{x})\left\{y_i - \beta_1 - \beta_2(x_{i1} - x_1) - \beta_3(x_{i2} - x_2) \right\}^2.
    $$

. . .

-   A common way of choosing the [weights]{.orange} $w_i(\bm{x})$ is to set
    $$
    w_i(\bm{x}) = \frac{1}{h_1 h_2} w\left(\frac{x_{i1} - x_1}{h_1}\right)w\left(\frac{x_{i2} - x_2}{h_2}\right).
    $$

-   Clearly, this now involves the choice of [two]{.orange} different
    [smoothing parameters]{.blue}.

## The bivariate case ($h_1 = 0.5, h_2 = 150$)

```{r}
#| message: false
#| fig-width: 9
#| fig-height: 7
#| fig-align: center

library(sm)
auto <- read.table("../data/auto.txt", header = TRUE) %>% select(city.distance, engine.size, n.cylinders, curb.weight, fuel)

h_sm <- c(0.5, 150)
sm.options(ngrid = 50)

fit_sm <- sm.regression(cbind(auto$engine.size, auto$curb.weight), auto$city.distance,
  h = h_sm, display = "none", ngrid = 500,
  hull = TRUE,
  options = list(xlab = "Engine size (L)", ylab = "Curb weight (kg)", zlab = "City distance (km/L)")
)

contour(fit_sm$eval.points[, 1], fit_sm$eval.points[, 2], fit_sm$estimate,
  xlab = "Engine size (L)", ylab = "Curb weight (kg)", col = "#1170aa")
points(auto$engine.size, auto$curb.weight, cex = 0.5, pch = 16, col = "#fc7d0b")
```

## Pros and cons of kernel nonparametric regression

::: callout-tip
#### Pros

-   Local linear regression is a [nonparametric]{.blue} estimator for unknown functions $f(x)$ which makes very [few assumptions]{.orange} on its form.

- The procedure is simple and [computationally efficient]{.blue}. 

- The smoothing parameter $h$ can be easily handled, since $\hat{f}(x)$ a linear smoother.
:::

. . .

::: callout-warning
#### Cons

- There is a price to pay for not making assumptions: estimation is [less efficient]{.orange} in terms of mean squared error compared to parametric models (when they are correctly specified!).

- This is a drawback of all nonparametric estimators, not just local linear regression.
:::

# Regression splines

## Basis expansions

-   The idea of polynomial regression can be [generalized]{.blue} and
    [improved]{.orange}. The main idea is to augment or replace the
    input $x$ with additional variables ([basis expansion]{.blue}).

. . .

-   Let $h_1(x), \dots, h_p(x)$ be [pre-specified]{.orange} functions
    $h_j(x) : \mathbb{R} \rightarrow \mathbb{R}$ that transform the
    original predictor $x$ in some [non-linear]{.blue} fashion. Then, we
    let $$ 
    f(x;\beta) = \sum_{j=1}^p h_j(x) \beta_j,
    $$ where $\beta = (\beta_1, \dots, \beta_p)^T$ is a vector of
    [unknown coefficients]{.orange}.

-   [Polynomials]{.blue} are a specific instance of basis expansion, in
    which $$
    h_1(x) = 1, \quad h_2(x) = x, \quad h_3(x) = x^2, \quad \dots \quad h_p(x) = x^{p-1}.$$

. . .

-   The main advantage of this approach is its [linearity]{.orange} in
    the [parameters]{.orange}, because it means that [ordinary least
    squares]{.blue} can be used for the estimation of $\beta$.

## Piecewise regression I

```{r}
#| fig-width: 6
#| fig-height: 3
#| fig-align: center

library(splines2)

knots <- c(15, 25)
fit_bs <- lm(accel ~ bsp(times, knots = knots, degree = 0, intercept = TRUE) - 1, data = dataset)
y_hat_bs <- predict(fit_bs, newdata = data.frame(times = times_seq))

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = times_seq, y = y_hat_bs), aes(x = x, y = y), col = "#1170aa") +
  theme_minimal() +
  geom_vline(xintercept = 15, linetype = "dotted") +
  geom_vline(xintercept = 25, linetype = "dotted") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

-   A [piecewise constant]{.blue} regression model is another instance
    of basis expansions, in which we consider step functions, say
    $p = 3$ $$
    h_1(x) = I(x < \xi_1), \quad h_2(x) = I(\xi_1 \le x < \xi_2), \quad h_3(x) =  I(x \ge \xi_2),
    $$ where $\xi = (\xi_1,\xi_2)$ are pre-specified cutpoints, called
    [knots]{.orange}. Here $\xi = (15, 25)$.

## Piecewise regression II

-   The previous choice of knots is [not working]{.orange} very well.
    The model is not flexible enough.

. . .

-   To improve the fit, we could consider [piecewise polynomial]{.blue}
    functions rather than constant. For example, a piecewise [quadratic]{.orange}
    function with $p = 30$ is $$
    \begin{aligned}
    h_1(x) &= I(x < \xi_1), && h_2(x) = x\:I(x < \xi_1), &&& h_3(x) = x^2\:I(x < \xi_1),  \\
    h_4(x) &= I(\xi_1 \le x  <\xi_2), &&h_5(x) = x\:I(\xi_1 \le x < \xi_2), &&& h_6(x) = x^2\:I(\xi_1 \le x < \xi_2), \\
    \vdots & &&\vdots &&&\vdots\\
    h_{28}(x) &= I(x \ge \xi_9),  &&h_{29}(x) = x\:I(x \ge \xi_9), &&& h_{30}(x) = x^2\:I(x \ge \xi_9).\\
    \end{aligned}
    $$

. . .

-   The piecewise quadratic
    $f(x; \beta) = \sum_{j=1}^{30} h_j(x) \beta_j$ is [not continuous]{.orange} e.g. at the knot $\xi_1$: $$
    \beta_1 + \beta_2 \xi_1 + \beta_3\xi_1^2 \neq \beta_4 + \beta_5 \xi_1 + \beta_6\xi_1^2.
    $$ To achieve [smoothness]{.blue}, it would be appealing to add
    some [continuity constraints]{.orange}.


## Piecewise polynomial functions

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center

library(splines)
knots <- quantile(dataset$times, ppoints(n = 9))

dataset$time_cut <- cut(dataset$times, breaks = c(2.3, knots, 57.6))

fit_piece <- lm(accel ~ time_cut + times * time_cut + I(times^2) * time_cut, data = dataset)
y_hat_piece <- predict(fit_piece, newdata = data.frame(times = times_seq, time_cut = cut(times_seq, breaks = c(2.3, knots, 57.6))))

fit_bs <- lm(accel ~ bsp(times, knots = knots, degree = 2, intercept = TRUE) - 1, data = dataset)
y_hat_bs <- predict(fit_bs, newdata = data.frame(times = times_seq))

data_plot <- data.frame(
  times = times_seq, pred = c(y_hat_piece, y_hat_bs),
  Method = rep(c("Piecewise quadratic", "Piecewise quadratic (continuity constraints)"),
    each = length(times_seq)
  )
)
ggplot(data = data_plot, aes(x = times, y = pred, col = Method)) +
  geom_line() +
  geom_point(data = dataset, aes(x = times, y = accel), size = 0.7, col = "black") +
  theme_minimal() +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Splines I

- Splines are [piecewise polynomial functions]{.blue} with [smoothness]{.orange} and
[continuity]{.orange} constraints.

- Originally developed for [ship-building]{.blue} to [draw a smooth curve]{.orange} through a
set of points. 

- The solution was to place metal weights (called
[knots]{.orange}) at the control points, and bend a thin metal or wooden beam
(called a [spline]{.blue}) through the weights.


::: columns
::: {.column width="50%"}
![](img/ship.jpg){fig-align="center" width="70%"}

:::

::: {.column width="50%"}
![](img/spline_draw.png){fig-align="center" width="65%"}
:::
:::

## Splines II

::: callout-note
#### Definition (Spline of degree $d$)
Let $\xi_1 < \cdots < \xi_k$ be a set of ordered points called [knots]{.orange} belonging to the interval $(a, b)$.  

A [spline]{.blue} $f(x; \beta) : (a, b) \rightarrow \mathbb{R}$ of degree $d$ is a piecewise polynomial function of degree $d$ that has continuous derivatives up to order $d - 1$.
:::

. . .

- [Cubic splines]{.blue} ($d = 3$) are the most common spline used in practice.

::: callout-tip
#### Definition (Cubic spline, $d = 3$)

Letting $\xi_1 < \dots < \xi_k$ denote a set of ordered knots, a [cubic spline]{.blue} $f(x; \beta)$ is a piecewise cubic polynomial that has [continuous first]{.orange} and [second derivatives]{.orange}.
:::

## Splines III

- Figure 5.2 of [HTF (2011)]{.grey}, in which are shown piecewise [cubic polynomials]{.orange} with [increasing regularity]{.blue}. The bottom-right plot (green line) depicts a [cubic spline]{.orange}. 

![](img/splines_HTF.png){fig-align="center" width="120%"}

## Splines IV

- To [recap]{.orange}: a spline of degree $d$ is a piecewise polynomial $f(x; \beta)$ of order $d$ such that
$$
f(\xi_j^+) = f(\xi_j^-), \dots, f^{(d-1)}(\xi_j^+) = f^{(d-1)}(\xi_j^-), \qquad j=1,\dots,k,
$$
where $\xi_j^+$ and $\xi_j^-$ denote the left and the right limits. 

. . .

- The degree $d$ controls the amount of smoothness:
    - $d = 0$ is a [piecewise constant]{.blue} function;
    - $d = 1$ is a [polygonal line]{.orange} (continuous, but with discontinuous first derivative).

- Higher values of $d$ increase the smoothness, but the spline behaves more and more like a global polynomial. In practice, one rarely goes beyond $d = 3$. 

. . .

- The current definition of spline is quite [abstract]{.blue} and [non-operative]{.orange}. How do we fit a regression model whose $f(x; \beta)$ is a spline? 

## Truncated power basis

::: callout-warning
#### Theorem (Truncated power basis) 
Let $\xi_1 < \cdots < \xi_k$ be a set of ordered points called [knots]{.orange} belonging to $(a, b)$. Let $$
h_j(x) = x^{j-1}, \qquad j=1,\dots,d + 1,
$$
and $$
h_j(x) = (x - \xi_j)_+^d, \qquad j = 2 + d, \dots, k + d + 1.
$$
Then, the functions $\{h_1,\dots,h_{k+d + 1}\}$ form a basis for the
set of splines of degree $d$ at these knots, called the [truncated power
basis]{.blue}. 

Thus, any [$d$th degree spline]{.orange} $f(x, \beta)$ with these knots can be written as a basis expansion $$
f(x; \beta) = \sum_{j=1}^{k+d+1}h_j(x) \beta_j.
$$
:::

## Regression splines

- The truncated power basis is a [constructive]{.orange} way of defining splines. Moreover, it clarifies that splines are [linear in the parameters]{.blue}.

. . .

- Let $\bm{B}$ be a $n \times p$ design matrix whose elements are obtained from the basis functions:
$$
[\bm{B}]_{ij} = h_j(x_i), \qquad j=1,\dots,p; \quad i=1,\dots,n.
$$

. . .

- Let $f(x;\beta) = \sum_{j=1}^p h_j(x)\beta_j$. Then, the [ordinary least squares]{.blue} for $\beta$ are obtained as usual:
$$
\hat{\beta} = (\bm{B}^T\bm{B})^{-1}\bm{B}^T\bm{y} \implies \hat{f}(x) = \sum_{j=1}^p h_j(x)\hat{\beta}_j = \sum_{i=1}^n s_i(x) y_i.
$$

- Hence, [regression splines]{.orange} are another instance of [linear smoother]{.blue} (actually, of [linear model]{.blue}). The smoothing matrix in this case is $\bm{S} = \bm{B} (\bm{B}^T\bm{B})^{-1}\bm{B}^T$, so that $\text{tr}(\bm{S}) = p$. 

. . .

- Regression splines are generating "new" covariates. Hence, their extension to GLMs, particularly to [logistic regression]{.orange}, is straightforward. 

## On the choice of the knots

- The knots' placement and their number $k$ are [complexity parameters]{.orange}, which should be chosen via cross-validation or other tools. 

. . .

- In principle, the [position of the knots]{.blue} could be manually selected to get the best fit. However, this results in an [incredible optimization problem]{.orange}.

. . .

- In practice, the knots are typically selected in two ways:
    1. Knots are [equally spaced]{.orange} on a grid of values ranging from $\min(x)$ to $\max(x)$;
    2. Knots are placed on [quantiles]{.blue} (`bs` default), to get [variable bandwidth]{.blue}.

. . .

- The [degree]{.blue} $d$ influences the [number of knots]{.orange} we can place for a fixed number of degrees of freedom $p$. For example:
  - In linear splines ($d = 1$), with $p = 12$ we can place $k = p - d - 1 = 10$ knots;
  - In quadratic splines ($d = 2$), with $p = 12$ we can place $k = 9$ knots;
  - In cubic splines ($d = 3$), with $p = 12$ we can place $k = 8$ knots.



## Regression splines ($p = 12$)

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center

fit_bs_3 <- lm(accel ~ bs(times, df = 12, degree = 3, intercept = TRUE) - 1, data = dataset)
y_hat_bs_3 <- predict(fit_bs_3, newdata = data.frame(times = times_seq))

fit_bs_2 <- lm(accel ~ bs(times, df = 12, degree = 2, intercept = TRUE) - 1, data = dataset)
y_hat_bs_2 <- predict(fit_bs_2, newdata = data.frame(times = times_seq))

fit_bs_1 <- lm(accel ~ bs(times, df = 12, degree = 1, intercept = TRUE) - 1, data = dataset)
y_hat_bs_1 <- predict(fit_bs_1, newdata = data.frame(times = times_seq))

data_plot <- data.frame(
  times = times_seq, pred = c(y_hat_bs_3, y_hat_bs_2, y_hat_bs_1),
  Method = rep(c("3. Cubic spline (d = 3)", "2. Quadratic spline (d = 2)", "1. Linear splines (d = 1)"),
    each = length(times_seq)
  )
)
ggplot(data = data_plot, aes(x = times, y = pred, col = Method)) +
  geom_line() +
  geom_point(data = dataset, aes(x = times, y = accel), size = 0.7, col = "black") +
  theme_minimal() +
  theme(legend.position = "top") +
  # geom_vline(xintercept = knots, linetype = "dotted") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## On the choice of $p$ (cubic splines)

```{r}
# Code execution and storage of the interesting quantities
p_list <- 4:40
data_goodness <- data.frame(p = p_list)
for (i in 1:length(p_list)) {
  # Fitting a polynomial of degree p -1
  fit <- lm(accel ~ bs(times, degree = 3, df = p_list[i], intercept = TRUE) - 1, data = dataset)
  # Computation of the leverages h_i efficiently (using QR)
  lev <- influence(fit)$hat
  res_loo <- (y - fitted(fit)) / (1 - lev)
  data_goodness$LOO_CV[i] <- mean(res_loo^2)
  data_goodness$LOO_CV_SE[i] <- sd(res_loo) / sqrt(nrow(dataset))
  data_goodness$GCV[i] <- mean(((y - fitted(fit)) / (1 - p_list[i] / nrow(dataset)))^2)
}

# Organization of the results for graphical purposes
data_bv <- data.frame(p = p_list, GCV = data_goodness$GCV, LOO_CV = data_goodness$LOO_CV, SE = data_goodness$LOO_CV_SE)
data_bv <- reshape2::melt(data_bv, id = c("p", "SE"))
data_bv$SE[data_bv$variable == "GCV"] <- NA
levels(data_bv$variable) <- c("GCV", "LOO-CV")
colnames(data_bv) <- c("p", "SE", "Error term", "value")
```

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center
ggplot(data = data_bv, aes(x = p, y = value, col = `Error term`)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = 12, linetype = "dotted", col ="#fc7d0b") +
  theme_light() +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Model complexity (p)") +
  ylab("Mean Squared Error (MSE)") 
```

## Natural cubic splines I

- Polynomials fit [beyond]{.blue} the [boundary knots]{.blue} $\xi_1$ and $\xi_k$ tends to be [erratic]{.orange}. Prediction and extrapolations can be dangerous. 

. . .

:::callout-note
#### Definition (Natural cubic spline)
A [natural cubic spline]{.blue} $f(x; \beta)$ is a cubic spline which is [linear]{.orange} beyond the boundary knots $\xi_1$ and $\xi_k$, which means $f''(\xi_1) = f''(\xi_k) = 0$.
:::

- Natural cubic splines enforce $4$ [additional constraints]{.orange}; these degrees of freedom can be used more efficiently to place more internal knots. 

. . .

:::callout-warning
#### Proposition
A set of $n \ge 2$ [distinct]{.blue} points ($x_i, y_i$) can be [interpolated]{.blue} using a natural cubic spline with the data points $x_1 < \dots < x_n$ as knots. The interpolating natural cubic spline is [unique]{.orange}.
:::


## Natural cubic splines II

- In practice, the [truncated power basis]{.blue} can be easily [modified]{.orange}, to get the following basis
$$
\begin{aligned}
&N_1(x) = 1, \quad N_2(x) = x, \\
&N_{j + 2}(x) = \frac{(x - \xi_j)^3_+ - (x - \xi_k)^3_+}{\xi_k - \xi_j} - \frac{(x - \xi_{k-1})^3_+ - (x - \xi_k)^3_+}{\xi_k - \xi_{k-1}}, \quad j = 1,\dots,k - 2.
\end{aligned}
$$

. . .

- This formula is a [scaled version]{.orange} of the truncated power basis for any $x \le \xi_{k-1}$, namely
$$
N_{j+2}(x) = \frac{(x - \xi_j)^3_+}{\xi_k - \xi_j}, \qquad x \le \xi_{k-1}, \quad j = 1,\dots,k - 2.
$$
The formula becomes more complicated when $x > \xi_{k-1}$ and the constraint is enforced. 

. . .

- Hence, in [natural cubic splines]{.blue} $k = p$ and the function can be express as follows
$$
f(x; \beta) = \sum_{j=1}^k N_j(x)\beta_j.
$$


## Natural cubic splines ($k = 12$)

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center

knots <- quantile(dataset$times, ppoints(n = 12))

fit_bs_3 <- lm(accel ~ bs(times, knots = knots, degree = 3, intercept = TRUE) - 1, data = dataset)
y_hat_bs_3 <- predict(fit_bs_3, newdata = data.frame(times = times_seq))

# Here it's tricky, because by default ns does a different thing (it uses the boundary knots as regular knots)
fit_ns_3 <- lm(accel ~ ns(times, knots = knots[-c(1, 12)], Boundary.knots = c(knots[1], knots[12]), intercept = TRUE) - 1, data = dataset)
y_hat_ns_3 <- predict(fit_ns_3, newdata = data.frame(times = times_seq))

data_plot <- data.frame(
  times = times_seq, pred = c(y_hat_bs_3, y_hat_ns_3),
  Method = rep(c("Cubic regression splines", "Natural cubic splines"),
    each = length(times_seq)
  )
)
ggplot(data = data_plot, aes(x = times, y = pred, col = Method)) +
  geom_line() +
  geom_point(data = dataset, aes(x = times, y = accel), size = 0.7, col = "black") +
  theme_minimal() +
  theme(legend.position = "top") +
  geom_vline(xintercept = knots[c(1, 12)], linetype = "dotted") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Computations: B-splines I

::: incremental
- Despite their conceptual simplicity, the truncated power basis and its "natural" modification are [not]{.blue} used in [practice]{.blue}, due to [ill-conditioning]{.orange}. 

- Indeed, the condition number of $\bm{B}^T\bm{B}$ using a truncated power basis is very large, leading to [numerical inaccuracies]{.orange}.

- For this reason, more [computationally convenient]{.blue} bases are preferred. This means we will consider an [equivalent]{.blue} set of functions $\mathcal{B}_1(x),\dots,\mathcal{B}_p(x)$ such that
$$
\mathcal{B}_j(x) = \sum_{\ell=1}^p\gamma_{\ell j} h_\ell(x), \qquad j=1,\dots,p,
$$
for some set of weights $\gamma_{\ell j}$ that makes this transformation [one-to-one]{.orange}. 

- Since we are performing a linear transformation, if ordinary least squares are used, this [does not change]{.orange} the fit. 

- A particularly convenient basis are [B-splines]{.blue}, which are [local]{.blue} and [numerically stable]{.orange}. They admit a direct construction; i.e., we do not need to compute the coefficients $\gamma_{\ell j}$.
:::

## Computations: B-splines II

```{r}
tpower <- function(x, t, p) {
  (x - t)^p * (x > t)
}

tbase <- function(x, knots, degree = 3) {
  B <- cbind(outer(x, 0:degree, "^"), outer(x, knots, function(x, y) pmax(x - y, 0))^degree)
  B
}

knots <- c(10, 30, 50)
data_plot <- data.frame(times_seq, 
                        rbind(reshape2::melt(bs(times_seq, knots = knots, degree = 3, intercept = TRUE)), reshape2::melt(tbase(times_seq, knots = knots, degree = 3))),
                        Basis = rep(c("B-splines", "Truncated power basis"), 
                                    each = length(times_seq) * (length(knots) + 4)))


ggplot(data = data_plot, aes(x = times_seq, y = value, col = as.factor(Var2))) +
  geom_line() +
  theme_minimal() +
  theme(legend.position = "none") +
  facet_grid(Basis~., scales = "free_y") +
  geom_vline(xintercept = knots, linetype = "dotted") +
  scale_color_tableau(type = "ordered-diverging", palette = "Orange-Blue Diverging") +
  xlab("x") +
  ylab(expression(h[j](x)))
```

## ☠️ - Details of B-splines I

- Define $\xi = (\xi_1,\dots,\xi_k)$ and consider an [augmented]{.orange} sequence of ordered [knots]{.orange} $$
\tau = (\tau_1,\dots,\tau_{k + 2d + 2}) = (\underbrace{\xi_{-d},\dots,\xi_0}_{\text{auxiliary knots}}, \: \xi,\: \underbrace{\xi_{k+1},\dots,\xi_{k + d + 1}}_{\text{auxiliary knots}}).
$$
The most common choice is $\xi_{-d}= \dots = \xi_0 = a$ and $\xi_{k+1} = \dots = \xi_{k + d+ 1} = b$.

. . .

- [Step 1]{.blue}. For $j = 1,\dots, k + 2d + 1$, obtain the B-spline of degree $m = 0$ as follows: 
$$
\mathcal{B}_{j,0}(x) = I_{[\tau_j, \tau_{j+1})}(x),
$$
where by convention we say that $\mathcal{B}_{j,0}(x) = 0$ if the knots are equal $\tau_j = \tau_{j+1}$. 

. . .

- [Step 2 (recursion)]{.orange}. The B-spline of [degree $m \le d$]{.blue} are obtained [recursively]{.orange}, so that  $$
\mathcal{B}_{j, m}(x) = \frac{x - \tau_j}{\tau_{j + m} - \tau_j}\mathcal{B}_{j, m-1}(x)  + \frac{\tau_{j + m + 1} - x}{\tau_{j + m + 1} - \tau_{j +1}}\mathcal{B}_{j + 1, m-1}(x),
$$
for $j=1,\dots,k + 2d + 1 - m$ and for $m=1,\dots,d$. 

## ☠️ - Details of B-splines II


::: incremental

- The [intercept]{.blue} term is [implicitly included]{.orange} in a B-spline basis, in fact 
$$
\sum_{j=1}^{k + d + 1}\mathcal{B}_{j, d}(x) = 1, \qquad x \in (a, b).
$$


- B-splines $\tau$ have [local support]{.orange}, which means that for $j=1,\dots,k + 2d + 1 - m$ we have
$$
\begin{aligned}
&\mathcal{B}_{j, d}(x) = 0, \qquad x \not\in (\tau_j, \tau_{j+d+1}), \\
&\mathcal{B}_{j, d}(x) > 0, \qquad x \in (\tau_j, \tau_{j+d +1}).
\end{aligned}
$$
This implies that the support of cubic B-splines is at most $4$ knots. 

- The presence of [structural zeros]{.orange} implies that, when computing ordinary least squares, extremely efficient Cholesky factorization for [banded matrices]{.blue} can be exploited. 

- The B-spline basis can be modified to produce a [natural cubic spline]{.blue}, by numerically enforcing the linearity constraint. This is implemented in the `ns` **R** function. 
:::

## Pros and cons of regression splines

::: callout-tip
#### Pros

-   Regression splines is a [semi-parametric]{.orange} estimator for unknown functions $f(x)$.

- They are essentially a [linear model]{.blue} with smart covariates that account for non-linearity. Hence, the procedure is simple and [computationally efficient]{.blue} (thanks to B-splines). 

- The smoothing parameter $k$ is discrete and can be easily handled, being directly associated with the number of degrees of freedom.

- They are trivial to extend to generalized linear models.
:::

. . .

::: callout-warning
#### Cons

- Knot placement based on quantiles or other automatic choices could be inefficient.

- Manual placement of the knots is out of question because it is an almost impossible optimization problem.  
:::

# Smoothing splines

## Smoothing splines I

- Let us consider the following penalized least squares criterion
$$
\mathscr{L}(f; \lambda) = \sum_{i=1}^n\{y_i - f(x_i)\}^2 + \lambda \underbrace{\int_a^b\{f''(t)\}^2\mathrm{d}t}_{\text{roughness penalty}},
$$
where $(a, b)$ is an interval containing the data points and $\lambda > 0$ is a [smoothing parameter]{.orange}.

. . .

- We consider as our estimator the [minimizer]{.blue} of the above loss, that is
$$
\hat{f}(x) = \arg\min_{f \in \mathcal{F}} \mathscr{L}(f; \lambda),
$$
where $\mathcal{F}$ is a sufficiently regular [functional space]{.blue} (Sobolev space). 

- The [roughness penalty]{.orange} quantifies the "wiggliness" of the curve. There are two extremes:
  -  When $\lambda = 0$ there is no penalization: any solution [interpolates]{.blue} the points $(x_i, y_i)$;
  -  When $\lambda = \infty$ then necessarily $f''(x) = 0$, i.e. the solution is a [linear model]{.orange}.


## Smoothing splines II

:::callout-note
#### Theorem (Green and Silverman, 1994)
Let $n_0 \le n$ be the [distinct]{.blue} points among $x_1,\dots,x_n$, with $x_i \in (a,b)$. Suppose $n_0 \ge 3$. 

Then, for any $\lambda > 0$ the minimizer of $\mathscr{L}(f; \lambda)$ is [unique]{.orange} and is a [natural cubic spline]{.blue} with $n_0$ knots at the distinct points. 
:::

. . .

- The Green and Silverman theorem is remarkably [elegant]{.orange} and [powerful]{.blue}. 

- Since the solution is a [natural cubic spline]{.blue}, we can write it as follows:
$$
f(x; \beta) = \sum_{j=1}^{n_0} N_j(x)\beta_j,
$$
whose coefficients $\beta$ still [needs]{.orange} to be [determined]{.orange}.

. . .

- In smoothing splines, we do [not]{.orange} need to [choose the knots]{.orange}: every distinct observation is a knot. The model is not overparametrized because the [complexity]{.blue} is controlled by $\lambda$. 

## Smoothing splines III

- Since $f(x;\beta)$ is a [natural cubic spline]{.orange}, the penalized least squares criterion becomes
$$
\mathscr{L}(\beta; \lambda) = \sum_{i=1}^n\{y_i - f(x_i; \beta)\}^2 + \lambda \beta^T\bm{\Omega}\beta,
\qquad [\bm{\Omega}]_{jk} = \int_a^bN''_j(t)N''_k(t)\mathrm{d}t,
$$
whose [minimization]{.blue} over $\beta$ is much easier, because it becomes [finite-dimensional]{.orange}.

. . .

- The minimization of $\mathscr{L}(\beta; \lambda)$ is reminiscent of [ridge regression]{.blue}, and in fact the solution is
$$
\hat{\beta} = (\bm{N}^T\bm{N} + \lambda \bm{\Omega})^{-1}\bm{N}^T\bm{y}, \qquad [\bm{N}]_{ij} = N_j(x_i).
$$
which leads to a [linear smoother]{.orange}, with $\bm{S} = \bm{N}(\bm{N}^T\bm{N} + \lambda \bm{\Omega})^{-1}\bm{N}^T$.

. . .

- The above formula is not used in practice directly. The `smooth.spline` **R** implementation relies on B-splines to make computations [fast]{.orange} and [stable]{.blue}. 

- Alternatively, the so-called Reinsch (1967) algorithm has computational complexity $\sim n$. 

## Smoothing splines ($\text{df}_\text{sm} = 12.26$)

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center
#| 

x_seq <- seq(from = min(x), to = max(x), length = 2000)
fit_smooth <- smooth.spline(x, y, all.knots = TRUE)
y_hat_smooth <- predict(fit_smooth, x = x_seq)

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x =y_hat_smooth$x, y = y_hat_smooth$y), aes(x = x, y = y), col = "#1170aa") +
  theme_minimal() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```


## The equivalent kernel

- As already mentioned, smoothing splines are [linear smoothers]{.orange}, which means that
$$
\hat{f}(x) = \sum_{i=1}^ns_i(x) y_i.
$$
- Provided $x$ is not too near the edge of the interval $(a, b)$, and $\lambda$ is not too big or too small, we obtain the following [approximation]{.orange} for the [equivalent kernel]{.blue} $$
s_i(x) \approx \frac{1}{g(x)}\frac{1}{h(x)}w\left(\frac{x - x_i}{h(x)}\right).
$$

. . .

- The [kernel function]{.orange} $w(t)$, which is not a density, and the [local bandwidth]{.blue} equal to
$$
w(t) = \frac{1}{2}\exp\left(-\frac{|t|}{\sqrt{2}}\right)\sin\left(\frac{|t|}{\sqrt{2}} + \frac{\pi}{4}\right), \qquad h(x) = \lambda^{1/4} \{\textcolor{red}{n} \textcolor{darkblue}{g(x)}\}^{-1/4}.
$$
- Smoothing splines [automatically]{.orange} incorporate a [local bandwidth]{.blue} decreasing with $n$.  

## Multi-dimensional splines

- There are several ways of extending regression and smoothing splines to the multivariate case. An example are [tensor splines]{.blue}, based on the cross-multiplication of basis functions. 

. . .

- Another instance are the [thin-plate splines]{.blue}, in which the 2d [roughness]{.orange} penalty becomes
$$
\int_{\mathbb{R}^2}\left\{\left(\frac{\partial^2 f(x_1,x_2)}{\partial x_1^2}\right)^2 + 2\left(\frac{\partial^2 f(x_1,x_2)}{\partial x_1 \partial x_2}\right)^2 + \left(\frac{\partial^2 f(x_1,x_2)}{\partial x_2^2}\right)^2\right\}\mathrm{d}x_1\mathrm{d}x_2.
$$
The minimization of the above loss has a simple [finite-dimensional]{.orange} solution. 

. . .

- We do not discuss any further multi-dimensional splines because, when the [dimension is large]{.orange}, they are affected by the so-called [curse of dimensionality]{.blue}; see [Unit E](un_E.html).

. . .

- Nonetheless, the 2d case is extremely useful in [spatial statistics]{.orange}, in which $x_1$ and $x_2$ represent longitude and latitude. 

- There will be a strong connection between the [smoothers]{.orange} we have seen in this unit and the so-called [kriging equations]{.blue}.

## Further properties of smoothing splines

- Extension of smoothing splines to [generalized linear models]{.blue} is possible by adding the "roughness" penalty to the log-likelihood function. 

. . .

- Smoothing splines have a Bayesian interpretation, being an instance of [Gaussian process]{.blue}. 

- From a theoretical perspective, there exists (Chapter 5.8 of [HTF, 2011]{.grey}) an elegant theory based on [reproducing kernel Hilbert spaces]{.orange}, that [unifies]{.blue}:
    - Gaussian processes;
    - Smoothing splines;
    - Support vector machine.

:::callout-tip
#### Pro-tip (a joke?)

*"If you want to derive an estimator that performs well in practice, define a Bayesian model, derive the posterior mean, call this a frequentist estimator, and hide all evidence you ever considered a Bayesian approach."* [Credits to Eric B. Laber](https://github.com/ericlaber/ericlaber.github.io/blob/main/docs/topic_one_linear_regression_regularization.pdf)
:::


## Pros and cons of smoothing splines

::: callout-tip
#### Pros

- Smoothing splines is a [nonparametric]{.orange} estimator for unknown functions $f(x)$.

- They are a [linear smoother]{.blue} with variable bandwidth.

- Compared to regression splines, they do not require the choice of the knots.

- Simple and efficient algorithms for computing 1d smoothing splines exist, such as `smooth.spline` available in **R**. 
:::

. . .

::: callout-warning
#### Cons

- Efficient implementations require a profound knowledge of linear algebra, B-spline basis, etc. 

- Hence, the "manual" incorporation (i.e., the coding) of smoothing splines into bigger, non-standard models is not straightforward.
:::


# References

## References

-   [Main references]{.blue}
    - **Chapter 4** of Azzalini, A. and Scarpa, B. (2011), [*Data
        Analysis and Data
        Mining*](http://azzalini.stat.unipd.it/Book-DM/), Oxford
        University Press.
    - **Chapters 5 and 6** of Hastie, T., Tibshirani, R. and Friedman, J.
        (2009), [*The Elements of Statistical
        Learning*](https://hastie.su.domains/ElemStatLearn/), Second
        Edition, Springer.
    - **Chapter 5** of Wasserman, L. (2006), [*All of Nonparametric statistics*](https://link.springer.com/book/10.1007/0-387-30623-4), Springer.

- [Kernel methods]{.orange}
    - **Chapter 5** of Wand, M. P., and Jones, M.C. (1995). *Kernel Smoothing*. Chapman and Hall.
    - **Chapters 2, 3, and 4** of Fan, J., and Gijbels, I. (1996). *Local Polynomial Modelling and Its Applications*. Chapman and Hall.

- [Smoothing splines]{.orange}
    - Green, P. J., and Silverman, B. W. (1994). *Nonparametric Regression and Generalized Linear Models: A Roughness Penalty Approach*. Springer.