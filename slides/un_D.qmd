---
title: "Nonparametric regression"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Universit√† degli Studi di Milano-Bicocca_"
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    df-print: tibble
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_D_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 200
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
    highlight-style: github
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 200
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---


## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_D.qmd", output = "../code/un_D.R")
styler:::style_file("../code/un_D.R")
```

::: columns
::: {.column width="30%"}
![](img/nonparametric.jpg) 

::: {style="font-size: 70%;"}
*"Nonparametric regression might, like linear regression, become an object treasured both for its artistic merit as well as usefulness."* 
:::

[Leo Breiman]{.grey}
:::

::: {.column width="70%"}
-   In this unit we will cover the following [topics]{.orange}:
  
    - Kernel methods and local regression;
    - Regression splines;
    - Smoothing splines.
    
- Let us consider again the [relationship]{.blue} between a response variable $Y_i$ and a set of covariates $\bm{x}_i$: $$
    Y_i = f(\bm{x}_i) + \epsilon_i, \qquad
    $$ where $\epsilon_i$ are [iid]{.orange} with
    $\mathbb{E}(\epsilon_i) = 0$ and
    $\text{var}(\epsilon_i) = \sigma^2$.
    
- We do not believe $f(\bm{x})$ is a polynomial or any other parametric function.

- Can we fit a [nonparametric]{.blue} relationship that does [not]{.orange} make strong [assumptions]{.orange} on $f(\bm{x})$?

:::
:::

# Motivating applications

## The `cholesterol` data

::: columns
::: {.column width="40%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
#| warning: false

library(tidyverse)
library(ggplot2)
library(ggthemes)
rm(list = ls())
# The dataset can be downloaded here: https://tommasorigon.github.io/datamining/data/cholesterol.txt
dataset <- read.table("../data/cholesterol.txt", header = TRUE)
ggplot(data = dataset, aes(x = compliance, y = cholesterol.decrease)) +
  geom_point() +
  theme_minimal() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Compliance") +
  ylab("Cholesterol Decrease")
```
:::

::: {.column width="60%"}
-   A drug called "cholestyramine" is administered to $n = 164$ men.

-   For each man, we observe the pair $(x_i, y_i)$.

-   The response $y_i$ is the [decrease in cholesterol level]{.orange}
    over the experiment.

-   The covariate $x_i$ is a measure of [compliance]{.blue}.

-   We assume, as before, that the data are generated according to $$
        Y_i = f(x_i) + \epsilon_i, \quad i=1,\dots,n.
        $$

-   The original data can be [found
    here](https://hastie.su.domains/CASI_files/DATA/cholesterol.txt).
:::
:::


## The `auto` dataset

::: columns
::: {.column width="40%"}
```{r}
#| fig-width: 5
#| fig-height: 4

rm(list = ls())
# The dataset can be also downloaded here: https://tommasorigon.github.io/datamining/data/auto.txt
auto <- read.table("../data/auto.txt", header = TRUE) %>% select(city.distance, engine.size, n.cylinders, curb.weight, fuel)

ggplot(data = auto, aes(x = engine.size, y = city.distance)) +
  geom_point() +
  theme_minimal() +
  xlab("Engine size (L)") +
  ylab("Urban distance (km/L)")
```
:::

::: {.column width="60%"}
-   Let us consider the variables `city.distance` ($y$) and `engine.size`
    ($x$).

-   A [simple linear regression]{.blue} $$
    Y_i = \beta_1 + \beta_2 x_i + \epsilon_i, \qquad i=1,\dots,n,
    $$ could be easily fit by least squares...

-   ... but the plot clearly suggests that the relationship between
    `city.distance` and `engine.size` is [not]{.orange} well
    approximated by a [linear]{.orange} function.

-   ... and also that `fuel` has an non-negligible effect on the
    response.
:::
:::

## The `mcycle` dataset

::: columns
::: {.column width="40%"}
```{r}
#| fig-width: 5
#| fig-height: 4

rm(list = ls())
dataset <- MASS::mcycle

x <- dataset$times
y <- dataset$accel

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_point() +
  theme_minimal() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```
:::

::: {.column width="60%"}
- Data consist of variables $y$ [accelerometer]{.blue} (`accel`) readings, taken through time $x$ (`times`). 

- The $n = 133$ observations were measured during a simulated [motor-cycle crash]{.blue} experiment, for testing the [efficacy]{.orange} of [crash helmets]{.orange}. 

- Some characteristics of the data:
    - The time points are [not regularly spaced]{.orange} and sometimes there are [multiple observations]{.blue}; 
    - The observations are subject to [error]{.orange};
    - The errors $\epsilon_i$ are probably heteroschedastic, but let us ignore this for the moment.

- It is of interest  to discern the [general shape]{.blue} of the underlying acceleration curve.
:::
:::


## Old friends: polynomials

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center

# Degrees of the polynomials
degree_list <- c(9, 11, 13, 15, 17, 19)

# I am using 30.000 obs to improve the quality of the graph
times_seq <- seq(from = min(dataset$times), to = max(dataset$times), length = 30000) 

# Actual fitting procedure
data_pred <- NULL
for (degree in degree_list) {
  # Fitting a polynomial of degree p - 1
  fit <- lm(accel ~ poly(times, degree = degree, raw = FALSE), data = dataset)
  # Fitted values
  y_hat <- predict(fit, newdata = data.frame(times = times_seq))
  data_pred <- rbind(data_pred, data.frame(x = times_seq, y_hat = y_hat, degree = paste("Number of parameters p:", degree + 1)))
}

# Graphical adjustment to get the plots in the right order
data_pred$degree <- factor(data_pred$degree)
#data_pred$degree <- factor(data_pred$degree, levels = levels(data_pred$degree)[c(3, 5, 6, 1, 2, 4)])

# Final plot
ggplot(data = data_pred) +
  geom_line(aes(x = x, y = y_hat, col = degree)) +
  geom_point(data = dataset, aes(x = times, y = accel), size = 0.4) +
  theme_light() +
  theme(legend.position = "none") +
  facet_wrap(. ~ degree, ncol = 3) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("x") +
  ylab("y") # Manual identification of an "interesting" region
```

## Old friends: polynomials

- asd

# Local regression

## K-nearest neighbours ($k = 6$)

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
library(kknn)
fit_knn <- fitted(kknn(accel ~ times, train = dataset, test = dataset, kernel = "rectangular", k = 6))

dataset$fit_knn <- fit_knn
ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_ribbon(aes(xmin = 51.7, xmax = 57.7), fill = "#fc7d0b", alpha = 0.6) +
  geom_ribbon(aes(xmin = 19, xmax = 21), fill = "#fc7d0b", alpha = 0.6) +
  geom_point(size = 0.7) +
  geom_line(aes(x = times, y = fit_knn), col = "#1170aa") +
  geom_vline(xintercept = 20, lty = "dashed", linewidth = 0.4) +
  geom_vline(xintercept = 54.7, lty = "dashed", linewidth = 0.4) +
  theme_minimal() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```


## Nadaraya-Watson estimator

## Local linear regression I

## Local linear regression II

## Local linear regression III

## Local linear regression IV

## Choice of the kernel

## Choice of the bandwidth I

## Choice of the bandwidth II

## Choice of the bandwidth III

## Loess I

## Loess II

## Local likelihoods I

## Local likelihoods II

## The bivariate case I

## The bivariate case II

# Regression splines

# Smoothing splines

# Penalized splines
