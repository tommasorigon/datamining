---
title: "Nonparametric regression"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    df-print: tibble
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_D_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 200
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
    highlight-style: github
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 200
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_D.qmd", output = "../code/un_D.R")
styler:::style_file("../code/un_D.R")
```

::: columns
::: {.column width="30%"}
![](img/nonparametric.png)

::: {style="font-size: 70%;"}
*"Nonparametric regression might, like linear regression, become an
object treasured both for its artistic merit as well as usefulness."*
:::

[Leo Breiman]{.grey}
:::

::: {.column width="70%"}
-   In this unit we will cover the following [topics]{.orange}:

    -   Kernel methods and local regression;
    -   Regression splines;
    -   Smoothing splines.

-   Let us consider again the [relationship]{.blue} between a response
    variable $Y_i$ and a set of covariates $\bm{x}_i$: $$
      Y_i = f(\bm{x}_i) + \epsilon_i, \qquad
      $$ where $\epsilon_i$ are [iid]{.orange} with
    $\mathbb{E}(\epsilon_i) = 0$ and
    $\text{var}(\epsilon_i) = \sigma^2$.

-   We do not believe $f(\bm{x})$ is a polynomial nor it belongs to some
    parametric family of functions.

-   Can we fit a [nonparametric]{.blue} relationship that does
    [not]{.orange} make strong [assumptions]{.orange} on $f(\bm{x})$?
    Let us review some old dataset...
:::
:::

# Motivating applications

## The `cholesterol` data

::: columns
::: {.column width="40%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
#| warning: false

library(tidyverse)
library(ggplot2)
library(ggthemes)
rm(list = ls())
# The dataset can be downloaded here: https://tommasorigon.github.io/datamining/data/cholesterol.txt
dataset <- read.table("../data/cholesterol.txt", header = TRUE)
ggplot(data = dataset, aes(x = compliance, y = cholesterol.decrease)) +
  geom_point() +
  theme_minimal() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Compliance") +
  ylab("Cholesterol Decrease")
```
:::

::: {.column width="60%"}
-   A drug called "cholestyramine" is administered to $n = 164$ men.

-   For each man, we observe the pair $(x_i, y_i)$.

-   The response $y_i$ is the [decrease in cholesterol level]{.orange}
    over the experiment.

-   The covariate $x_i$ is a measure of [compliance]{.blue}.

-   We assume, as before, that the data are generated according to $$
        Y_i = f(x_i) + \epsilon_i, \quad i=1,\dots,n.
        $$

-   In [Unit B](un_B.html#another-example-cholesterol-data) we fit a
    [polynomial]{.orange} with degree $3$ on this data, although there
    was some [uncertainty]{.blue}.
:::
:::

## The `auto` dataset

::: columns
::: {.column width="40%"}
```{r}
#| fig-width: 5
#| fig-height: 4

rm(list = ls())
# The dataset can be also downloaded here: https://tommasorigon.github.io/datamining/data/auto.txt
auto <- read.table("../data/auto.txt", header = TRUE) %>% select(city.distance, engine.size, n.cylinders, curb.weight, fuel)

ggplot(data = auto, aes(x = engine.size, y = city.distance)) +
  geom_point() +
  theme_minimal() +
  xlab("Engine size (L)") +
  ylab("Urban distance (km/L)")
```
:::

::: {.column width="60%"}
-   In [Unit A](un_A.html) we considered the `auto` dataset.

-   We wanted to model the relationship between `city.distance` ($y$)
    and `engine.size` ($x$).

-   The chosen model involved a [non-linear]{.blue} function $$
    Y_i = f(x_i) + \epsilon_i, \qquad i=1,\dots,n,
    $$ where $f(x)$ was "manually" selected.

-   There are [no]{.orange} reasons to believe that
    $f(x) = \alpha x^\beta$ or that $f(x)$ belongs to any other
    [parametric]{.orange} family.

-   We would like the data to "speak for themselves".
:::
:::

## The `mcycle` dataset

::: columns
::: {.column width="40%"}
```{r}
#| fig-width: 5
#| fig-height: 4

rm(list = ls())
dataset <- MASS::mcycle

x <- dataset$times
y <- dataset$accel

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_point() +
  theme_minimal() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```
:::

::: {.column width="60%"}
-   Data consist of variables $y$ [accelerometer]{.blue} (`accel`)
    readings, taken through time $x$ (`times`).

-   The $n = 133$ observations were measured during a simulated
    [motor-cycle crash]{.blue} experiment, for testing the
    [efficacy]{.orange} of [crash helmets]{.orange}.

-   Some characteristics of the data:

    -   The time points are [not regularly spaced]{.orange} and
        sometimes there are [multiple observations]{.blue};
    -   The observations are subject to [error]{.orange};
    -   The errors $\epsilon_i$ are probably heteroschedastic, but let
        us ignore this for the moment.

-   It is of interest to discern the [general shape]{.blue} of the
    underlying acceleration curve.
:::
:::

## Old friends: polynomials

-   In the `mcycle` dataset it is [not]{.orange} obvious which
    [parametric]{.orange} function we should consider, therefore this
    route is not an option.

. . .

-   The [theory]{.orange} says that [polynomials]{.blue} can
    [approximate]{.blue} a large class of functions, as a consequence of
    Taylor's expansion theorem.

-   In the statistical [practice]{.orange}, however, polynomial
    regression is not very well suited for modelling complex
    relationships.

. . .

-   When performing flexible regression we would expect the prediction
    at $x_i$ to depend on observations close to $x_i$. However,
    polynomials are [not local]{.orange}.

-   Instead, in polynomial regression points that are far away from
    $x_i$ have a big impact on $\hat{f}(x_i)$. This produces [spurious
    oscillations]{.blue} at the boundaries and [unstable]{.orange}
    estimates.

-   This is known as [Runge's
    phenomenon](https://en.wikipedia.org/wiki/Runge%27s_phenomenon) in
    numerical analysis.

## Old friends: polynomials (`mcycle` data)

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center

# Degrees of the polynomials
degree_list <- c(9, 11, 13, 15, 17, 19)

# I am using 30.000 obs to improve the quality of the graph
times_seq <- seq(from = min(dataset$times), to = max(dataset$times), length = 30000)

# Actual fitting procedure
data_pred <- NULL
for (degree in degree_list) {
  # Fitting a polynomial of degree p - 1
  fit <- lm(accel ~ poly(times, degree = degree, raw = FALSE), data = dataset)
  # Fitted values
  y_hat <- predict(fit, newdata = data.frame(times = times_seq))
  data_pred <- rbind(data_pred, data.frame(x = times_seq, y_hat = y_hat, degree = paste("Number of parameters p:", degree + 1)))
}

# Graphical adjustment to get the plots in the right order
data_pred$degree <- factor(data_pred$degree)

# Final plot
ggplot(data = data_pred) +
  geom_line(aes(x = x, y = y_hat, col = degree)) +
  geom_point(data = dataset, aes(x = times, y = accel), size = 0.4) +
  theme_light() +
  theme(legend.position = "none") +
  facet_wrap(. ~ degree, ncol = 3) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("x") +
  ylab("y") # Manual identification of an "interesting" region
```

# Local regression

## The regression function

::: incremental
-   The only assumption we are making is the following
    [additive]{.orange} structure $$
      Y_i = f(x_i) + \epsilon_i, \qquad i=1,\dots,n,
    $$ where $\epsilon_i$ are [iid]{.orange} with
    $\mathbb{E}(\epsilon_i) = 0$ and
    $\text{var}(\epsilon_i) = \sigma^2$.

-   In [Unit B](un_B.html#regression-under-quadratic-loss-ii) we showed
    that under the following [quadratic loss]{.orange}$$
    \mathbb{E}\left[\{\tilde{Y}_i - \hat{f}(x_i)\}^2\right],
    $$ the best prediction $\hat{f}(x_i)$, i.e. the one minimizing the
    loss, coincides with $$
    \hat{f}(x_i) = \mathbb{E}(\tilde{Y}_i) = f(x_i),
    $$ which is the [conditional expectation]{.blue} of $Y_i$ given the
    value $x_i$, called [regression function]{.blue}.

-   The regression function $f(x_i) = \mathbb{E}(\tilde{Y}_i)$ is the
    [optimal prediction]{.blue} even in presence of
    [heteroschedastic]{.orange} data or when the above
    [additive]{.orange} decomposition does [not hold]{.orange}.
:::

## Local estimates of the prediction

-   We do not know $f(x)$, but the previous formulas suggest that we
    could consider an [arithmetic average]{.blue} of the data points.

-   Hence, a [prediction]{.orange} for a generic value $x$ could be
    obtained as follows: $$
    \hat{f}(x) = \frac{1}{n_x}\sum_{i : x_i = x} y_i, \qquad n_x = \sum_{i=1}^n I(x_i = x).
    $$

. . .

-   This idea, unfortunately, [does not work]{.orange} in most practical
    cases.

-   Indeed, in a typical dataset it is very unlikely that there exist
    multiple observations [exactly equal]{.orange} to $x$ among the
    points $(x_i, y_i)$.

-   Even if there were values such that $x_i = x$, the [sample
    size]{.blue} $n_x$ would be so [small]{.orange} (e.g. $n_x = 1$)
    that the variance of $\hat{f}(x)$ would be extremely high, making
    this estimator useless.

. . .

-   However, this "local average" idea seems [intuitively
    appealing]{.orange}. Can we "fix" it?

## K-nearest neighbours

-   Instead of considering the values exactly equal to $x$, we could
    identify the pairs $(x_i, y_i)$ that are [close]{.blue} to (i.e. in
    a [neighbour]{.orange} of) $x$.

-   A natural measure of proximity between $x$ and the data points $x_i$
    is the [Euclidean distance]{.orange} $|x_i - x|$, but in principle
    any other metric could be used.

. . .

-   We consider an average of the [$k$ values]{.orange} $y_i$ whose
    $x_i$ are [nearest]{.orange} to $x$, that is: $$
    \hat{f}(x) = \frac{1}{|\mathcal{N}_x|}\sum_{i \in \mathcal{N}_x} y_i,
    $$ where $\mathcal{N}_x$ is indeed the set of $k$ points nearest to
    $x$ in Euclidean distance.

-   This method is called [$k$-nearest neighbours]{.blue} (KNN).

## K-nearest neighbours ($k = 6$)

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
library(kknn)
fit_knn <- fitted(kknn(accel ~ times, train = dataset, test = data.frame(times = times_seq), kernel = "rectangular", k = 6))

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_ribbon(aes(xmin = 51.7, xmax = 57.7), fill = "#fc7d0b", alpha = 0.6) +
  geom_ribbon(aes(xmin = 19, xmax = 21), fill = "#fc7d0b", alpha = 0.6) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = times_seq, y = fit_knn), aes(x = x, y = y), col = "#1170aa") +
  geom_vline(xintercept = 20, lty = "dashed", linewidth = 0.4) +
  geom_vline(xintercept = 54.7, lty = "dashed", linewidth = 0.4) +
  theme_minimal() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Comments and limitations about the KNN method

-   The number of neighbours $k$ influences how "[local]{.orange}" is
    the estimate.

-   When $k$ is low, the KNN estimator $\hat{f}(x)$ has [high
    variance]{.orange}. The extreme case $k = 1$ corresponds to an
    "average" of a single data point.

-   When $k$ is high, the KNN estimator $\hat{f}(x)$ is not local and it
    has [high bias]{.blue}. The extreme case $k = n$ produces a
    constant, i.e. the average of all the observations.

. . .

-   Thus, there is a [bias-variance trade-off]{.orange} in the choice of
    $k$, which should be selected e.g. via [cross-validation]{.blue}.

. . .

-   The $k$-nearest neighbours produces a [sensible result]{.blue}, but
    the method [can be improved]{.orange}.

-   The [blue curve]{.blue} is bumpy, because $\hat{f}(x)$ is
    [discontinuous]{.orange} in $x$.

-   Indeed, as we move $x$ from left to right, the $k$-nearest
    neighborhood remains constant, until a new point $x_i$ to the right
    of $x$ is included and one to the left is excluded.

-   This discontinuity is [ugly]{.orange} and [unnecessary]{.orange}. We
    are looking instead for a [smooth]{.blue} prediction.

## Nadaraya-Watson estimator

-   The [Nadaraya-Watson]{.blue} estimator addresses the aforementioned
    issues of the KNN method. It is a [weighted average]{.orange} $$
    \hat{f}(x) = \frac{1}{\sum_{i'=1}^n w_{i'}(x)}\sum_{i=1}^n w_i(x) y_i = \sum_{i=1}^n s_i(x) y_i,
    $$ where $s_i(x) = w_i(x) / \sum_{i'=1}^n w_{i'}(x)$ are the
    [normalized weights]{.blue}.

. . .

-   The values $w_i(x) \ge 0$ are chosen so that the points $x_i$
    [close]{.orange} to $x$ are [weighted more]{.orange}.

. . .

-   A convenient way of selecting these weights is through [kernel
    functions]{.blue}: $$
    w_i(x) = \frac{1}{h}w\left(\frac{x_i - x}{h}\right), \qquad i=1,\dots,n,
    $$ where $w(\cdot)$ is a [density]{.orange} function,
    [symmetric]{.orange} around the origin, called kernel in this
    context.

-   The value $h > 0$ is a [scale factor]{.blue}, sometimes called
    [bandwidth]{.orange} or [smoothing parameter]{.orange}.

## Nadaraya-Watson estimator: comments

-   The fitted function $\hat{f}(x)$ is [continuous]{.blue} and is
    obtained by computing several weighted averages, one for each value
    of $x$.

-   A popular kernel is the [Gaussian kernel]{.orange}, that is: $$
    w_i(x) = \frac{1}{h} \phi\left(\frac{x_i - x}{h}\right), \qquad i=1,\dots,n,
    $$ tehrefore $h^2$ represents the [variance]{.blue}. We will discuss
    alternative choices later on.

. . .

-   The most important factor, however, is not the functional form of
    $w(\cdot)$, but rather the [smoothing parameter]{.orange} $h$, which
    is a complexity parameter.

-   Indeed, $h$ defines the "[smoothing window]{.blue}" on the $x$-axis,
    i.e. the relevant data points that are considered for $\hat{f}(x)$.

-   As any complexity parameter, $h$ should be chosen via
    cross-validation or related ideas.

## Nadaraya-Watson (Gaussian kernel)

::: panel-tabset
## Smoothing $h = 1$

```{r}
#| fig-width: 7.8
#| fig-height: 4
#| fig-align: center
#| message: false
library(KernSmooth)

h_param <- 1
band <- 4 * qnorm(0.75) * h_param # Bandwidth as parametrized in ksmooth
fit_nw <- ksmooth(x, y, kernel = "normal", bandwidth = band, x.points = times_seq)

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_ribbon(aes(xmin = qnorm(0.05, 20, sd = h_param), xmax = qnorm(0.95, 20, sd = h_param)), fill = "#fc7d0b", alpha = 0.6) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = fit_nw$x, y = fit_nw$y), aes(x = x, y = y), col = "#1170aa") +
  geom_vline(xintercept = 20, lty = "dashed", linewidth = 0.4) +
  theme_minimal() +
  geom_function(fun = function(x) 150 * dnorm(x, 20, h_param) - 134, linetype = "dotted", n = 500, xlim = c(qnorm(0.001, 20, sd = h_param), xmax = qnorm(0.999, 20, sd = h_param))) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Smoothing $h = 0.3$

```{r}
#| fig-width: 7.8
#| fig-height: 4
#| fig-align: center

h_param <- 0.3
band <- 4 * qnorm(0.75) * h_param # Bandwidth as parametrized in ksmooth
fit_nw <- ksmooth(x, y, kernel = "normal", bandwidth = band, x.points = times_seq)

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_ribbon(aes(xmin = qnorm(0.05, 20, sd = h_param), xmax = qnorm(0.95, 20, sd = h_param)), fill = "#fc7d0b", alpha = 0.6) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = fit_nw$x, y = fit_nw$y), aes(x = x, y = y), col = "#1170aa") +
  geom_vline(xintercept = 20, lty = "dashed", linewidth = 0.4) +
  theme_minimal() +
  geom_function(fun = function(x) 50 * dnorm(x, 20, h_param) - 134, linetype = "dotted", n = 500, xlim = c(qnorm(0.001, 20, sd = h_param), xmax = qnorm(0.999, 20, sd = h_param))) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Smoothing $h = 2$

```{r}
#| fig-width: 7.8
#| fig-height: 4
#| fig-align: center
h_param <- 2
band <- 4 * qnorm(0.75) * h_param # Bandwidth as parametrized in ksmooth
fit_nw <- ksmooth(x, y, kernel = "normal", bandwidth = band, x.points = times_seq)

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_ribbon(aes(xmin = qnorm(0.05, 20, sd = h_param), xmax = qnorm(0.95, 20, sd = h_param)), fill = "#fc7d0b", alpha = 0.6) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = fit_nw$x, y = fit_nw$y), aes(x = x, y = y), col = "#1170aa") +
  geom_vline(xintercept = 20, lty = "dashed", linewidth = 0.4) +
  theme_minimal() +
  geom_function(fun = function(x) 180 * dnorm(x, 20, h_param) - 134, linetype = "dotted", n = 500, xlim = c(qnorm(0.001, 20, sd = h_param), xmax = qnorm(0.999, 20, sd = h_param))) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Smoothing $h = 4$

```{r}
#| fig-width: 7.8
#| fig-height: 4
#| fig-align: center
h_param <- 4
band <- 4 * qnorm(0.75) * h_param # Bandwidth as parametrized in ksmooth
fit_nw <- ksmooth(x, y, kernel = "normal", bandwidth = band, x.points = times_seq)

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_ribbon(aes(xmin = qnorm(0.05, 20, sd = h_param), xmax = qnorm(0.95, 20, sd = h_param)), fill = "#fc7d0b", alpha = 0.6) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = fit_nw$x, y = fit_nw$y), aes(x = x, y = y), col = "#1170aa") +
  geom_vline(xintercept = 20, lty = "dashed", linewidth = 0.4) +
  theme_minimal() +
  geom_function(fun = function(x) 600 * dnorm(x, 20, h_param) - 134, linetype = "dotted", n = 500, xlim = c(qnorm(0.001, 20, sd = h_param), xmax = qnorm(0.999, 20, sd = h_param))) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```
:::

## Local linear regression I

::: incremental
-   [Local linear regression]{.blue} is a refinement of the
    Nadaraya-Watson estimator that has typically [lower bias]{.orange},
    especially at the [boundaries]{.orange}, without noticeable
    increases in variance.

-   If $f(x)$ is differentiable, then it can be [approximated]{.orange}
    with a linear function tangent in $x_0$: $$
    f(x) = \underbrace{f(x_0)}_{\beta_1} + \underbrace{f'(x_0)}_{\beta_2}(x - x_0) + \text{rest}.
    $$

-   Hence, instead of computing a [local average]{.orange}
    ($\beta_2 = 0$), we consider a [local linear model]{.blue}. In other
    words, for every $x$ we seek the coefficients solving: $$
    (\hat{\beta}_1, \hat{\beta}_2) = \arg\min_{(\beta_1, \beta_2)} \sum_{i=1}^n \textcolor{darkblue}{w_i(x)}\left\{y_i - \beta_1 - \textcolor{red}{\beta_2(x_i - x)}\right\}^2.
    $$

-   Once the parameter $\hat{\beta}_1$ and $\beta_2$ are obtained, the
    local [linear regression estimator]{.blue} is $$
    \hat{f}(x) = \hat{\beta}_1 + \hat{\beta}_2 (x - x) = \hat{\beta}_1.
    $$
:::

## Local linear regression II

-   The local linear regression, as we have seen in [Unit A](un_A.html),
    has an [explicit solution]{.orange}: $$
    \hat{\beta} = (\bm{X}^T\bm{W}_x\bm{X})^{-1}\bm{X}^T\bm{W}_x\bm{y},
    $$ where the rows of $\bm{X}$ are $\bm{x}_i = (1, x_i - x)$ and
    $\bm{W}_x = \text{diag}\{w_1(x),\dots,w_n(x)\}$.

-   In practice, we do [not]{.orange} need to solve this [linear
    algebra]{.orange} problem. An even more explicit and
    [non-iterative]{.orange} solution can be found (see Exercises).

. . .

::: callout-note
#### Theorem (Local linear smoothing)

The local linear regression smoother, evaluated in $x$, admits an
[explicit expression]{.blue}: $$
\hat{f}(x) = \frac{1}{n}\sum_{i=1}^n  \frac{w_i(x) \{a_2(x) - (x_i - x) a_1(x)\}}{a_2(x)a_0(x) - a_1(x)^2 } y_i = \sum_{i=1}^n s_i(x) y_i,
$$ where $a_j(x) = n^{-1}\sum_{i=1}^n w_i(x) (x_i - x)^j$, for
$j=0,1,2$.
:::

## Local linear regression ($h = 1.45$, Gaussian kernel)

```{r}
library(KernSmooth)

# Optimal parameter is selected using a plug-in method that is not described in the slides.
# Congratulations for spotting this, I expect ~3% of the students will actually read this code.
# Anyway, if you want to understand how this method works, you can read the paper described in the documentation (? dpill)
h_param <- dpill(x, y)

fit_locpoly <- locpoly(x, y, bandwidth = h_param, gridsize = 2000)

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = fit_locpoly$x, y = fit_locpoly$y), aes(x = x, y = y), col = "#1170aa") +
  theme_minimal() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Linear smoothers I

-   The Nadaraya-Watson estimator and local linear regression are
    special instances of [linear smoothers]{.blue}, which are estimators
    having the following form: $$
    \hat{f}(x) = \sum_{i=1}^ns_i(x) y_i.
    $$

. . .

-   We will study other members of this class, such as regression and
    smoothing splines.

-   Polynomial regression, ridge regression, [Gaussian processes]{.blue}
    and [moving averages]{.orange} are also linear smoothers.

. . .

-   The [mean]{.blue} (and hence the bias), and the [variance]{.orange}
    of a linear smoother can be easily obtained: $$
    \mathbb{E}\{\hat{f}(x)\} = \sum_{i=1}^n s_i(x)f(x_i), \qquad \text{var}\{\hat{f}(x)\} = \sigma^2\sum_{i=1}^ns_i(x)^2.
    $$

## Linear smoothers II

-   In linear smoothers, we can express the predicted values
    $\hat{\bm{y}}$ using [matrix notation]{.orange} $$
    \hat{\bm{y}} = \sum_{i=1}^n\bm{s}_i \bm{y} = \bm{S}\bm{y}, \qquad \bm{s}_i = (s_1(x_i), \dots, s_n(x_i))^T,
    $$ where $\bm{S} = (\bm{s}_1,\dots,\bm{s}_n)^T$ is the so-called
    $n \times n$ [smoothing]{.blue} matrix.

. . .

-   Each row of the smoothing matrix $\bm{s}_i$ is called [equivalent
    kernel]{.orange} for estimating $\hat{f}(x_i)$; in the Nadaraya
    Watson estimator $\bm{s}_i$ is indeed a normalized kernel.

. . .

-   The weights of all the smoothers we will use have are such that
    $\sum_{i=1}^ns_i(x) = 1$ for all $x$.

-   Hence, the smoother [preserves constant curves]{.blue}, namely if
    all $y_i = c$, then $\hat{f}(x) = c$.

## On the choice of the kernel

-   As mentioned before, the choice of the kernel is [not
    crucial]{.orange}. Some alternatives are:

| Kernel       | $w(x)$                                                    | Support      |
|---------------------|-----------------------------|-----------------------|
| Gaussian     | $\frac{1}{\sqrt{2 \pi}}\exp{\left(-\frac{x^2}{2}\right)}$ | $\mathbb{R}$ |
| Rectangular  | $\frac{1}{2}$                                             | $(-1, 1)$    |
| Epanechnikov | $\frac{3}{4}(1 - x^2)$                                    | $(-1, 1)$    |
| Bi-quadratic | $\frac{15}{16}(1 - x^2)^2$                                | $(-1, 1)$    |
| Tri-cubic    | $\frac{70}{81}(1 - |x|^3)^3$                              | $(-1, 1)$    |

. . .

-   Some [asymptotic]{.blue} considerations lead to the choice of the
    "[optimal]{.orange}" Epanechnikov kernel.

. . .

-   [Bounded]{.blue} kernels have [computational]{.blue} advantages,
    because one needs to compute averages of a limited number of data
    points.

-   On the other hand, bounded kernels may lead to [discontinuous
    derivatives]{.orange} of $\hat{f}(x)$ that could be unappealing in
    certain contexts.

## Bias-variance tradeoff

::: callout-warning
#### Theorem (Fan and Gijbels, 1996, Theorem 3.1)

Let $(X_i, Y_i)$ be iid random vectors with $g(x)$ denoting the
[marginal density]{.orange} of $X_i$. The conditional law is such that
$Y_i = f(X_i) + \epsilon_i$, with $\epsilon_i$ iid and
$\mathbb{E}(\epsilon_i) = 0$, $\text{var}(\epsilon_i) = \sigma^2$.

Moreover, suppose $g(x) > 0$ and that $g(\cdot)$ and $f''(\cdot)$ are
continuous in a neighborhood of $x$. Then, as $h \rightarrow 0$ and
$n h \rightarrow \infty$ we have that for the [local linear
regression]{.blue} $\hat{f}(x)$ the [bias]{.orange} is $$
\mathbb{E}\{\hat{f}(x) - f(x) \} \approx  \frac{\textcolor{red}{h^2}}{2}\sigma^2_w f''(x),
$$ where $\sigma^2_w = \int z^2 w(z)\mathrm{d}z$. In addition, the
[variance]{.blue} is $$
\text{var}\{\hat{f}(x)\} \approx \frac{\sigma^2}{\textcolor{darkblue}{n h}} \frac{\alpha_w}{g(x)},
$$ where $\alpha_w = \int w^2(z)\mathrm{d}z$.
:::

## Bias-variance tradeoff II

-   The previous theorem shows that [bias]{.orange} is of [order
    $h^2$]{.orange} and the [variance]{.blue} is of [order
    $(1 / nh)$]{.blue}.

-   Once again, there is a trade-off because we would like
    $h \rightarrow 0$ but, at the same time, we need to keep the
    variance under control.

. . .

-   We can select $h$ so that the [asymptotic mean squared error]{.blue}
    is [minimal]{.orange}. This leads to the following optimal choice
    for the bandwidth: $$
    h_\text{opt}(x) = \left(\frac{1}{n} \frac{\sigma^2 \alpha_w}{\sigma^4_w f''(x)^2 g(x)}\right)^{1/5}.
    $$

. . .

-   Unfortunately, $h_\text{opt}(x)$ is of [little practical
    utility]{.orange}, as it involves the [unknown]{.orange} terms
    $f''(x)$, $g(x)$ and $\sigma^2$. However, it highlights two
    important facts:

-   The bandwidth $h$ should decrease at the rate $n^{-1/5}$, i.e.
    [quite slowly]{.blue}.

-   If we plug-in $h_\text{opt}(x)$ into the bias/variance formulas, we
    get that the mean squared error tends to $0$ at the rate $n^{-4/5}$,
    which is much [slower]{.orange} than the [parametric]{.orange} case
    $n^{-1}$.

## ☠️ - Bias reduction of local linear regression

-   Local linear regression, compared to the Nadaraya-Watson estimator,
    correct the [first-order]{.orange} term of the [bias]{.orange},
    without affecting the variance in a sensible way.

-   Indeed, it can be shown that the [asymptotic variance]{.blue} of
    Nadaraya-Watson and local linear regression is the [same]{.blue},
    but the [asymptotic bias]{.orange} is [different]{.orange}.

. . .

-   To get an intuition of this, consider the following Taylor expansion
    for $\mathbb{E}\{\hat{f}(x)\}$ around $x_0$, and for the local
    linear regression case: $$
    \begin{aligned}
    \mathbb{E}\{\hat{f}(x)\} &= \sum_{i=1}^n s_i(x)f(x_i) \\
    & = f(x_0)\underbrace{\sum_{i=1}^ns_i(x_0)}_{=1} + f'(x_0) \underbrace{\sum_{i=1}^n(x_i - x)s_i(x_0)}_{=0} + \frac{f''(x_0)}{2}(x_i - x_0)^2s_i(x_0) + \text{rest}.
    \end{aligned}
    $$

-   It can be shown with some algebra that the first order term
    simplifies ($=0$) in the local linear regression case but it doesn't
    for the Nadaraya-Watson, therefore [reducing the bias]{.orange}.

## Choice of the bandwidth I

-   In practice, we need to choose the bandwidth by other means. A first
    solution is based on [information criteria]{.blue} such as the $C_p$
    or the AIC/BIC.

-   However, as before, their usage requires a suitable notion of
    [effective degrees of freedom]{.orange}.

. . .

::: callout-note
#### Effective degrees of freedom for linear smoothers

Let $\hat{f}(x) = \sum_{i=1}^ns_i(x)y_i$ be a linear smoother. Then the
effective degrees of freedom are $$
\text{df}_\text{sm} = \frac{1}{\sigma^2}\sum_{i=1}^n\text{cov}(Y_i, \hat{f}(x_i)) =\frac{1}{\sigma^2} \text{tr}\{\text{cov}(\bm{Y}, \bm{S}\bm{Y})\} = \frac{\sigma^2}{\sigma^2}\text{tr} (\bm{S}) = \text{tr} (\bm{S}).
$$
:::

. . .

-   Some authors proposed to use $\text{tr}(\bm{S}\bm{S}^T)$, but the
    connection with the [optimism]{.orange} and the definition of
    effective degrees of freedom is less clear.

## Choice of the bandwidth II

-   Cross-validation is another option for selecting the bandwidth $h$.
    In [linear smoothers]{.blue} there is a brilliant [computational
    shortcut]{.orange} for the leave-one-out case.

. . .

::: callout-note
#### LOO-CV (Linear smoothers)

Let $\hat{y}_{-i} = \hat{f}_{-i}(x_i)$ be the leave-one-out predictions
of a [linear smoother]{.blue} and let $\hat{\bm{y}} = \bm{S}\bm{y}$ be
the predictions of the full model. Then: $$
y_i - \hat{y}_{-i} = \frac{y_i - \hat{y}_i}{1 - [\bm{S}]_{ii}}, \qquad i=1,\dots,n.
$$ Therefore, the leave-one-out mean squared error is
$$\widehat{\mathrm{Err}} = \frac{1}{n}\sum_{i=1}^n \left(\frac{y_i - \hat{y}_i}{1 - [\bm{S}]_{ii}}\right)^2.$$
:::

## Loess

-   Sometimes it is convenient to choose $h$ [adaptively]{.orange}, i.e.
    specifying a variable bandwidth $h(x)$ that depends on the local
    density of the data.

-   Indeed, recall that the asymptotic variance depends on the sampling
    design of the $x_i$s $$
    \text{var}\{\hat{f}(x)\} \approx \frac{\sigma^2}{n h} \frac{\alpha_w}{\textcolor{darkblue}{g(x)}}.
    $$

. . .

-   The `loess` (Cleveland, 1979) consider a [fixed percentage]{.blue}
    of data points (assuming a bounded kernel is used), which
    automatically induces a [variable bandwidth]{.orange}, as in KNN.

. . .

-   Moreover, the `loess` algorithm combines the [variable
    bandwidth]{.blue} with some [robust estimation]{.orange} ideas, so
    that the resulting estimate is less influenced by outliers.

. . .

-   `loess` is a short-hand for "[l]{.orange}ocally [w]{.orange}eighted
    [e]{.orange}stimated [s]{.orange}catterplot [s]{.orange}moothing".

## Local likelihoods

-   The concept of local regression and varying coefficients is
    extremely [broad]{.orange}.

-   In principle any [parametric]{.orange} model can be made
    [local]{.blue} as long as it accommodates [weights]{.blue}.

. . .

-   In a [generalized linear model]{.blue} with a single predictor, for
    every value $x$ we seek $$
    (\hat{\beta}_1,\hat{\beta}_2)= \arg\max_{(\beta_1, \beta_2)}\sum_{i=1}^nw_i(x) \ell(y_i; \beta_1 + (x_i - x_0)\beta_2) ,
    $$ whose solution can be found using iteratively re-weighted least
    squares.

. . .

-   The [computations]{.orange} and the [theory]{.blue} are not
    necessarily as straightforward and limpid as in the regression case,
    but most general considerations carry over.

-   For example, the choice of the bandwidth $h$ remains a critical
    parameter, which is usually selected via cross-validation, using a
    suitable loss function.

## The bivariate case

-   Local linear regression can be applied when [two]{.orange} or [more
    covariates]{.orange}, say $p$, are used. Let us begin with two
    covariates, so that $$
    y_i = f(x_{i1}, x_{i2}) + \epsilon_i.
    $$

-   To estimate $f$ on a specific point $x = (x_1,x_2)$, a [natural
    extension]{.blue} of local linear regression takes the form $$
      (\hat{\beta}_1, \hat{\beta}_2, \hat{\beta}_3) = \arg\min_{(\beta_1, \beta_2, \beta_3)} \sum_{i=1}^n w_i(x)\left\{y_i - \beta_1 - \beta_2(x_{i1} - x_1) - \beta_3(x_{i2} - x_2) \right\}^2.
    $$

. . .

-   A common way of choosing the [weights]{.orange} $w_i(x)$ is to set
    $$
    w_i(x) = \frac{1}{h_1 h_2} w\left(\frac{x_{i1} - x_1}{h_1}\right)w\left(\frac{x_{i2} - x_2}{h_2}\right).
    $$

-   Clearly, this now involves the choice of [two]{.orange} different
    [smoothing parameters]{.blue}.

## The bivariate case ($h_1 = 0.5, h_2 = 150$)

```{r}
#| message: false
#| fig-width: 10
#| fig-height: 8
#| fig-align: center

library(sm)
auto <- read.table("../data/auto.txt", header = TRUE) %>% select(city.distance, engine.size, n.cylinders, curb.weight, fuel)

h_sm <- c(0.5, 150)
sm.options(ngrid = 50)

fit_sm <- sm.regression(cbind(auto$engine.size, auto$curb.weight), auto$city.distance,
  h = h_sm, display = "none",
  options = list(xlab = "Engine size (L)", ylab = "Curb weight (kg)", zlab = "City distance (km/L)")
)

surf.colors <- function(x, col = terrain.colors(20)) {
  # First we drop the 'borders' and average the facet corners
  # we need (nx - 1)(ny - 1) facet colours!
  x.avg <- (x[-1, -1] + x[-1, -(ncol(x) - 1)] +
    x[-(nrow(x) - 1), -1] + x[-(nrow(x) - 1), -(ncol(x) - 1)]) / 4

  # Now we construct the actual colours matrix
  colors <- col[cut(x.avg, breaks = length(col), include.lowest = T)]

  return(colors)
}

persp(fit_sm$eval.points[, 1], fit_sm$eval.points[, 2], fit_sm$estimate,
  xlab = "Engine size (L)", ylab = "Curb weight (kg)", zlab = "City distance (km/L)", cex = 0.4,
  theta = 145, phi = 20, ticktype = "detailed", col = surf.colors(fit_sm$estimate, col = terrain.colors(80)), expand = 0.8
)
```

# Regression splines

## Basis expansions

-   The idea of polynomial regression can be [generalized]{.blue} and
    [improved]{.orange}. The core idea is to augment or replace the
    input $x$ with additional variables ([basis expansion]{.blue}).

. . .

-   Let $h_1(x), \dots, h_K(x)$ be [pre-specified]{.orange} functions
    $h_j(x) : \mathbb{R} \rightarrow \mathbb{R}$ that transform the
    original predictor $x$ in some [non-linear]{.blue} fashion. Then, we
    let $$ 
    f(x,\beta) = \sum_{j=1}^K h_j(x) \beta_j,
    $$ where $\beta = (\beta_1, \dots, \beta_K)^T$ is a vector of
    [unknown coefficients]{.orange}.

-   [Polynomials]{.blue} are a specific instance of basis expansion, in
    which $$
    h_1(x) = 1, \quad h_2(x) = x, \quad h_3(x) = x^2, \quad \dots \quad h_K(x) = x^{K-1}.$$

. . .

-   The main advantage of this approach is its [linearity]{.orange} in
    the [parameters]{.orange}, because it means that [ordinary least
    squares]{.blue} can be used for the estimation of $\beta$.

## Piecewise regression I

```{r}
#| fig-width: 6
#| fig-height: 3
#| fig-align: center

library(splines2)

knots <- c(15, 25)
fit_bs <- lm(accel ~ bsp(times, knots = knots, degree = 0, intercept = TRUE) - 1, data = dataset)
y_hat_bs <- predict(fit_bs, newdata = data.frame(times = times_seq))

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = times_seq, y = y_hat_bs), aes(x = x, y = y), col = "#1170aa") +
  theme_minimal() +
  geom_vline(xintercept = 15, linetype = "dotted") +
  geom_vline(xintercept = 25, linetype = "dotted") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

-   A [piecewise constant]{.blue} regression model is another instance
    of basis expansions, in which we consider step functions, say
    $K = 3$ $$
    h_1(x) = I(x < \xi_1), \quad h_2(x) = I(\xi_1 \le x < \xi_2), \quad h_3(x) =  I(x \ge \xi_2),
    $$ where $\xi = (\xi_1,\xi_2)$ are pre-specified cutpoints, called
    [knots]{.orange}. Here $\xi = (15, 25)$.

## Piecewise regression II

-   The previous choice of knots is [not working]{.orange} very well.
    The model is not flexible enough.

. . .

-   To improve the fit, we could consider [piecewise polynomial]{.blue}
    functions rather than constant. For example, a piecewise [quadratic]{.orange}
    function with $K = 30$ is $$
    \begin{aligned}
    h_1(x) &= I(x < \xi_1), && h_2(x) = x\:I(x < \xi_1), &&& h_3(x) = x^2\:I(x < \xi_1),  \\
    h_4(x) &= I(\xi_1 \le x  <\xi_2), &&h_5(x) = x\:I(\xi_1 \le x < \xi_2), &&& h_6(x) = x^2\:I(\xi_1 \le x < \xi_2), \\
    \vdots & &&\vdots &&&\vdots\\
    h_{28}(x) &= I(x \ge \xi_9),  &&h_{29}(x) = x\:I(x \ge \xi_9), &&& h_{30}(x) = x^2\:I(x \ge \xi_9).\\
    \end{aligned}
    $$

. . .

-   The piecewise quadratic
    $f(x; \beta) = \sum_{j=1}^{30} h_j(x) \beta_j$ is [not continuous]{.orange} e.g. at the knot $\xi_1$: $$
    \beta_1 + \beta_2 \xi_1 + \beta_3\xi_1^2 \neq \beta_4 + \beta_5 \xi_1 + \beta_6\xi_1^2.
    $$ To achieve [smoothness]{.blue}, it would be appealing to add
    some [continuity constraints]{.orange}.


## Piecewise polynomial functions

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center

library(splines)
knots <- quantile(dataset$times, ppoints(n = 9))

dataset$time_cut <- cut(dataset$times, breaks = c(2.3, knots, 57.6))

fit_piece <- lm(accel ~ time_cut + times * time_cut + I(times^2) * time_cut, data = dataset)
y_hat_piece <- predict(fit_piece, newdata = data.frame(times = times_seq, time_cut = cut(times_seq, breaks = c(2.3, knots, 57.6))))

fit_bs <- lm(accel ~ bsp(times, knots = knots, degree = 2, intercept = TRUE) - 1, data = dataset)
y_hat_bs <- predict(fit_bs, newdata = data.frame(times = times_seq))

data_plot <- data.frame(times = times_seq, pred = c(y_hat_piece, y_hat_bs), 
                        Method = rep(c("Piecewise quadratic","Piecewise quadratic with continuity constraints"), 
                                     each = length(times_seq)))
ggplot(data = data_plot, aes(x = times, y = pred, col = Method)) +
  geom_line() +
  geom_point(data = dataset, aes(x = times, y = accel), size = 0.7, col = "black") +
  theme_minimal() +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Splines I

- Splines are [piecewise polynomial functions]{.blue} with [smoothness]{.orange} and
[continuity]{.orange} constraints.

- Originally developed for [ship-building]{.blue} to draw a smooth curve through a
set of points. 

- The solution was to place metal weights (called
[knots]{.orange}) at the control points, and bend a thin metal or wooden beam
(called a [spline]{.blue}) through the weights.


::: columns
::: {.column width="50%"}
![](img/ship.jpg){fig-align="center" width="70%"}

:::

::: {.column width="50%"}
![](img/spline_draw.png){fig-align="center" width="65%"}
:::
:::

## Splines II

::: callout-note
#### Definition (Spline of order $M$th)
Let $\xi_1 < \cdots < \xi_k$ be a set of ordered points called [knots]{.orange} belonging to the interval $(a, b)$.  

A [spline]{.blue} $f(x; \beta) : (a, b) \rightarrow \mathbb{R}$ of order $M$th is a piecewise polynomial function of degree $M - 1$ that has continuous derivatives up to order $M - 2$.
:::

. . .

- [Cubic splines]{.blue} ($M = 4$) are the most common spline used in practice.

::: callout-tip
#### Definition (Cubic spline, $M = 4$)

Letting $\xi_1 < \dots < \xi_k$ denote a set ordered knots, a [cubic spline]{.blue} $f(x; \beta)$ is a piecewise cubic polynomial that has [continuous first]{.orange} and [second derivatives]{.orange}.
:::

## Splines III

- Figure 5.2 of [HTF (2011)]{.grey}, in which are shown piecewise [cubic polynomials]{.orange} with [increasing regularity]{.blue}. The bottom-right plot (green line) depicts a [cubic spline]{.orange}. 

![](img/splines_HTF.png){fig-align="center" width="120%"}

## Splines IV

- To [recap]{.orange}: a spline of order $M$ is a piecewise polynomial $f(x; \beta)$ of order $M-1$ such that
$$
f(\xi_j^+) = f(\xi_j^-), \dots, f^{(M-2)}(\xi_j^+) = f^{(M-2)}(\xi_j^-), \qquad j=1,\dots,k,
$$
where $\xi_j^+$ and $\xi_j^-$ denote the left and the right limits. 

. . .

- The order $M$ controls the amount of smoothness:
    - $M = 1$ is a [piecewise constant]{.blue} function
    - $M = 2$ is a [polygonal line]{.orange} (continuous, but with discontinous first derivative)

- Higher values of $M$ increase the smoothness, but the spline behave more and more like a global polynomial. In practice one almost never goes beyond $M = 4$. 

. . .

- The current definition of spline is quite [abstract]{.blue} and [non-operative]{.orange}. How do we actually fit a regression model whose $f(x; \beta)$ is a spline? 

## Truncated power basis I

::: callout-warning
#### Theorem (Truncated power basis) 
Let $\xi_1 < \cdots < \xi_k$ be a set of ordered points called [knots]{.orange} belonging to $(a, b)$. Let $$
h_j(x) = x^{j-1}, \qquad j=1,\dots,M,
$$
and $$
h_j(x) = (x - \xi_j)_+^M, \qquad j = 1 + M, \dots, k + M.
$$
Then, the functions $\{h_1,\dots,h_{k+M}\}$ form a basis for the
set of splines of order $M$th at these knots, called the [truncated power
basis]{.blue}. 

Thus, any [$M$th order spline]{.orange} $f(x, \beta)$ with these knots can be written as a basis expansion $$
f(x; \beta) = \sum_{j=1}^{k+M}h_j(x) \beta_j.
$$
:::

## Truncated power basis II

- The truncated power basis of a spline makes is a [constructive]{.orange} way of defining splines. Moreover, it clarifies that splines are [linear in the parameters]{.blue}.

. . .

- Let $\bm{B}$ be a $n \times K$ design matrix whose elements are obtained from the basis functions:
$$
[\bm{B}]_{ij} = h_j(x_i), \qquad j=1,\dots,K; \quad i=1,\dots,n.
$$

- Let $f(x;\beta) = \sum_{j=1}^K h_j(x)\beta_j$. Then, the [ordinary least squares]{.blue} for $\beta$ are obtained as usual:
$$
\hat{\beta} = (\bm{B}^T\bm{B})^{-1}\bm{B}^T\bm{y} \implies \hat{f}(x) = \sum_{j=1}^K h_j(x)\hat{\beta}_j = \sum_{i=1}^n s_i(x) y_i.
$$

. . .

- We call this approach [regression splines]{.orange}, which are another instance of [linear smoother]{.blue}. The smoothing matrix in this case is $\bm{S} = \bm{B} (\bm{B}^T\bm{B})^{-1}\bm{B}^T$. 

- The number  and the placement of the knots are [complexity parameters]{.orange}, which should be chosen via cross-validation or other tools. 



## Regression splines ($k = 15$)

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center

knots <- quantile(dataset$times, ppoints(n = 15))

fit_bs_3 <- lm(accel ~ bs(times, knots = knots, degree = 3, intercept = TRUE) - 1, data = dataset)
y_hat_bs_3 <- predict(fit_bs_3, newdata = data.frame(times = times_seq))

fit_bs_2 <- lm(accel ~ bs(times, knots = knots, degree = 2, intercept = TRUE) - 1, data = dataset)
y_hat_bs_2 <- predict(fit_bs_2, newdata = data.frame(times = times_seq))


fit_bs_1 <- lm(accel ~ bs(times, knots = knots, degree = 1, intercept = TRUE) - 1, data = dataset)
y_hat_bs_1 <- predict(fit_bs_1, newdata = data.frame(times = times_seq))


data_plot <- data.frame(times = times_seq, pred = c(y_hat_bs_3, y_hat_bs_2, y_hat_bs_1), 
                        Method = rep(c("Regression splines (M = 4)", "Regression splines (M = 3)", "Regression splines (M = 2)"), 
                                     each = length(times_seq)))
ggplot(data = data_plot, aes(x = times, y = pred, col = Method)) +
  geom_line() +
  geom_point(data = dataset, aes(x = times, y = accel), size = 0.7, col = "black") +
  theme_minimal() +
  theme(legend.position = "top") +
  #geom_vline(xintercept = knots, linetype = "dotted") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```


## Natural cubic splines I

- Polynomials fit [beyond]{.blue} the [boundary knots]{.blue} $\xi_1$ and $\xi_k$ tends to be [erratic]{.orange}. Prediction and extrapolations can be dangerous. 

. . .

:::callout-note
#### Definition (Natural cubic spline)
A [natural cubic spline]{.blue} $f(x; \beta)$ is a cubic spline which is [linear]{.orange} beyond the boundary knots $\xi_1$ and $\xi_k$, which means $f''(\xi_1) = f''(\xi_k) = 0$.
:::

- Natural cubic splines add $4$ [additional constraints]{.orange}; these degrees of freedom can be used more efficiently to place more internal knots. 

. . .

:::callout-note
#### Proposition
A set of $n$ points ($x_i, y_i$) can be exactly [interpolated]{.blue} using a natural cubic spline with the data points $x_1 < \dots < x_n$ as knots. The interpolating natural cubic spline is [unique]{.orange}.
:::


## Natural cubic splines II

- In practice, the [truncated power basis]{.blue} can be easily [modified]{.orange}, to get the following basis
$$
\begin{aligned}
&N_1(x) = 1, \quad N_2(x) = x, \\
&N_{j + 2}(x) = \frac{(x - \xi_j)^3_+ - (x - \xi_K)^3_+}{\xi_K - \xi_j} - \frac{(x - \xi_{K-1})^3_+ - (x - \xi_K)^3_+}{\xi_K - \xi_{K-1}}, \quad j = 1,\dots,k - 2.
\end{aligned}
$$

. . .

- This formula is a [scaled version]{.orange} of the truncated power basis for any $x \le \xi_{K-1}$, namely
$$
N_{j+2}(x) = \frac{(x - \xi_j)^3_+}{\xi_K - \xi_j}, \qquad x \le \xi_{K-1}, \quad j = 1,\dots,k - 2.
$$
The formula becomes more complicated when $x > \xi_{K-1}$ and the constraint is enforced. 

. . .

- Hence, [natural cubic splines]{.blue} are linear in the parameters and can be express as follows
$$
f(x; \beta) = \sum_{j=1}^k N_j(x)\beta_j.
$$


## Natural cubic splines ($k = 15$)

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center

knots <- quantile(dataset$times, ppoints(n = 15))

fit_bs_3 <- lm(accel ~ bs(times, knots = knots, degree = 3, intercept = TRUE) - 1, data = dataset)
y_hat_bs_3 <- predict(fit_bs_3, newdata = data.frame(times = times_seq))

# Here it's tricky, because by default ns does a different thing (it uses the boundary knots as regular knots)
fit_ns_3 <- lm(accel ~ ns(times, knots = knots[-c(1, 15)], Boundary.knots = c(knots[1], knots[15]), intercept = TRUE) - 1, data = dataset)
y_hat_ns_3 <- predict(fit_ns_3, newdata = data.frame(times = times_seq))

data_plot <- data.frame(times = times_seq, pred = c(y_hat_bs_3, y_hat_ns_3), 
                        Method = rep(c("Cubic regression splines", "Natural cubic splines"), 
                                     each = length(times_seq)))
ggplot(data = data_plot, aes(x = times, y = pred, col = Method)) +
  geom_line() +
  geom_point(data = dataset, aes(x = times, y = accel), size = 0.7, col = "black") +
  theme_minimal() +
  theme(legend.position = "top") +
  geom_vline(xintercept = knots[c(1, 15)], linetype = "dotted") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Computations: B-splines I

::: incremental
- Despite its conceptual simplicity, the truncated power basis is not used in practice. Indeed, the [condition number]{.orange} of $\bm{B}^T\bm{B}$ is often very large, leading to numerical inaccuracies.

- For this reason, more [computationally convenient]{.blue} bases are preferred. This means that we consider another set of functions $\mathcal{B}_1(x),\dots,\mathcal{B}_K(x)$ such that
$$
\mathcal{B}_j(x) = \sum_{\ell=1}^K\gamma_{\ell j} h_\ell(x), \qquad j=1,\dots,K,
$$
for some set of weights $\gamma_{\ell j}$ that makes this transformation [one-to-one]{.orange}. 

- This means we will consider a a new design matrix $\tilde{\bm{B}} = \bm{\Gamma}\bm{B}$, having set $[\bm{\Gamma}]_{\ell j} = \gamma_{\ell j}$. Hence, if ordinary least squares are used, this does not change the outcome. 

- A particularly convenient basis are [B-splines]{.blue}, which are [numerically stable]{.orange} and admit a direct construction; i.e. we do not need to compute the coefficients $\gamma_{\ell j}$.
:::

## Computations: B-splines II

## Computations: B-splines III


# Smoothing splines


# References
