---
title: "Nonparametric regression"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Universit√† degli Studi di Milano-Bicocca_"
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    df-print: tibble
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_D_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 200
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
    highlight-style: github
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 200
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_D.qmd", output = "../code/un_D.R")
styler:::style_file("../code/un_D.R")
```

::: columns
::: {.column width="30%"}
![](img/nonparametric.jpg)

::: {style="font-size: 70%;"}
*"Nonparametric regression might, like linear regression, become an
object treasured both for its artistic merit as well as usefulness."*
:::

[Leo Breiman]{.grey}
:::

::: {.column width="70%"}
-   In this unit we will cover the following [topics]{.orange}:

    -   Kernel methods and local regression;
    -   Regression splines;
    -   Smoothing splines.

-   Let us consider again the [relationship]{.blue} between a response
    variable $Y_i$ and a set of covariates $\bm{x}_i$: $$
      Y_i = f(\bm{x}_i) + \epsilon_i, \qquad
      $$ where $\epsilon_i$ are [iid]{.orange} with
    $\mathbb{E}(\epsilon_i) = 0$ and
    $\text{var}(\epsilon_i) = \sigma^2$.

-   We do not believe $f(\bm{x})$ is a polynomial or any other
    parametric function.

-   Can we fit a [nonparametric]{.blue} relationship that does
    [not]{.orange} make strong [assumptions]{.orange} on $f(\bm{x})$?
    Let us review some old dataset...
:::
:::

# Motivating applications

## The `cholesterol` data

::: columns
::: {.column width="40%"}
```{r}
#| fig-width: 5
#| fig-height: 4.5
#| warning: false

library(tidyverse)
library(ggplot2)
library(ggthemes)
rm(list = ls())
# The dataset can be downloaded here: https://tommasorigon.github.io/datamining/data/cholesterol.txt
dataset <- read.table("../data/cholesterol.txt", header = TRUE)
ggplot(data = dataset, aes(x = compliance, y = cholesterol.decrease)) +
  geom_point() +
  theme_minimal() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Compliance") +
  ylab("Cholesterol Decrease")
```
:::

::: {.column width="60%"}
-   A drug called "cholestyramine" is administered to $n = 164$ men.

-   For each man, we observe the pair $(x_i, y_i)$.

-   The response $y_i$ is the [decrease in cholesterol level]{.orange}
    over the experiment.

-   The covariate $x_i$ is a measure of [compliance]{.blue}.

-   We assume, as before, that the data are generated according to $$
        Y_i = f(x_i) + \epsilon_i, \quad i=1,\dots,n.
        $$

-   In [Unit B](un_B.html#another-example-cholesterol-data) we fit a
    [polynomial]{.orange} with degree $3$, although there was some
    [uncertainty]{.blue}.
:::
:::

## The `auto` dataset

::: columns
::: {.column width="40%"}
```{r}
#| fig-width: 5
#| fig-height: 4

rm(list = ls())
# The dataset can be also downloaded here: https://tommasorigon.github.io/datamining/data/auto.txt
auto <- read.table("../data/auto.txt", header = TRUE) %>% select(city.distance, engine.size, n.cylinders, curb.weight, fuel)

ggplot(data = auto, aes(x = engine.size, y = city.distance)) +
  geom_point() +
  theme_minimal() +
  xlab("Engine size (L)") +
  ylab("Urban distance (km/L)")
```
:::

::: {.column width="60%"}

-   In [Unit A](un_A.html) we considered the `auto` dataset.

-   We wanted to model the relationship between `city.distance` ($y$)
    and `engine.size` ($x$).

-   The chosen model involved a [non-linear]{.blue} function $$
    Y_i = f(x_i) + \epsilon_i, \qquad i=1,\dots,n,
    $$ where $f(x)$ was "manually" selected.

-   There are [no]{.orange} reasons to believe that
    $f(x) = \alpha + \beta\log{x}$ or $f(x)$ has any other [parametric
    function]{.orange}.

-   We would like the data to "speak for themselves".


:::
:::

## The `mcycle` dataset

::: columns
::: {.column width="40%"}
```{r}
#| fig-width: 5
#| fig-height: 4

rm(list = ls())
dataset <- MASS::mcycle

x <- dataset$times
y <- dataset$accel

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_point() +
  theme_minimal() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```
:::

::: {.column width="60%"}
-   Data consist of variables $y$ [accelerometer]{.blue} (`accel`)
    readings, taken through time $x$ (`times`).

-   The $n = 133$ observations were measured during a simulated
    [motor-cycle crash]{.blue} experiment, for testing the
    [efficacy]{.orange} of [crash helmets]{.orange}.

-   Some characteristics of the data:

    -   The time points are [not regularly spaced]{.orange} and
        sometimes there are [multiple observations]{.blue};
    -   The observations are subject to [error]{.orange};
    -   The errors $\epsilon_i$ are probably heteroschedastic, but let
        us ignore this for the moment.

-   It is of interest to discern the [general shape]{.blue} of the
    underlying acceleration curve.
:::
:::

## Old friends: polynomials

-   In the `mcycle` dataset it is [not]{.orange} obvious which
    [parametric]{.orange} function we should consider, therefore this
    route is not an option.

. . .

-   The [theory]{.orange} says that [polynomials]{.blue} can
    [approximate]{.blue} a large class of functions, as a consequence of
    Taylor's expansion theorem.

-   In the statistical [practice]{.orange}, however, polynomial
    regression is not very well suited for modelling complex
    relationships.

. . .

-   When performing flexible regression we would expect the prediction
    at $x_i$ to depend on observations close to $x_i$. However,
    polynomials are [not local]{.orange}.

-   Instead, in polynomial regression points that are far away from
    $x_i$ have a big impact on $\hat{f}(x_i)$. This produces [spurious
    oscillations]{.blue} at the boundaries and [unstable]{.orange}
    estimates.

-   This is known as [Runge's
    phenomenon](https://en.wikipedia.org/wiki/Runge%27s_phenomenon) in
    numerical analysis.

## Old friends: polynomials (`mcycle` data)

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center

# Degrees of the polynomials
degree_list <- c(9, 11, 13, 15, 17, 19)

# I am using 30.000 obs to improve the quality of the graph
times_seq <- seq(from = min(dataset$times), to = max(dataset$times), length = 30000)

# Actual fitting procedure
data_pred <- NULL
for (degree in degree_list) {
  # Fitting a polynomial of degree p - 1
  fit <- lm(accel ~ poly(times, degree = degree, raw = FALSE), data = dataset)
  # Fitted values
  y_hat <- predict(fit, newdata = data.frame(times = times_seq))
  data_pred <- rbind(data_pred, data.frame(x = times_seq, y_hat = y_hat, degree = paste("Number of parameters p:", degree + 1)))
}

# Graphical adjustment to get the plots in the right order
data_pred$degree <- factor(data_pred$degree)

# Final plot
ggplot(data = data_pred) +
  geom_line(aes(x = x, y = y_hat, col = degree)) +
  geom_point(data = dataset, aes(x = times, y = accel), size = 0.4) +
  theme_light() +
  theme(legend.position = "none") +
  facet_wrap(. ~ degree, ncol = 3) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("x") +
  ylab("y") # Manual identification of an "interesting" region
```

# Local regression

## The regression function

::: incremental
-   Let us recall again that the only assumption we are making is that
    $$
      Y_i = f(x_i) + \epsilon_i, \qquad i=1,\dots,n,
    $$ where $\epsilon_i$ are [iid]{.orange} with
    $\mathbb{E}(\epsilon_i) = 0$ and
    $\text{var}(\epsilon_i) = \sigma^2$.

-   In [Unit B](un_B.html#regression-under-quadratic-loss-ii) we showed
    that under the following [quadratic loss]{.orange}$$
    \mathbb{E}\left[\{\tilde{Y}_i - \hat{f}(x_i)\}^2\right],
    $$ the best prediction $\hat{f}(x_i)$, i.e. the one minimizing the
    loss, coincides with $$
    \hat{f}(x_i) = \mathbb{E}(\tilde{Y}_i) = f(x_i),
    $$ which is the [conditional expectation]{.blue} of $Y_i$ given the
    value $x_i$, called [regression function]{.blue}.

-   The regression function $f(x_i) = \mathbb{E}(\tilde{Y}_i)$ is the
    [optimal prediction]{.blue} even in presence of
    [heteroschedastic]{.orange} data or when the above
    [additive]{.orange} decomposition does [not hold]{.orange}.
:::

## Local estimates of the prediction

-   We do not know $f(x)$, but the previous formulas suggest that we
    could consider an [arithmetic average]{.blue} of the data points.

-   Hence, a [prediction]{.orange} for a generic value $x$ could be
    obtained as follows: $$
    \hat{f}(x) = \frac{1}{n_x}\sum_{i : x_i = x} y_i, \qquad n_x = \sum_{i=1}^n I(x_i = x).
    $$

. . .

-   This idea, unfortunately, [does not work]{.orange} in most practical
    cases.

-   Indeed, in a typical dataset it is very unlikely that there exist
    observations exactly equal to $x$ among the points $(x_i, y_i)$.

-   Even if there were values such that $x_i = x$, the [sample
    size]{.blue} $n_x$ would often be so [small]{.orange} (e.g.
    $n_x = 1$) that the variance of $\hat{f}(x)$ would be too high,
    making this estimator useless.

. . .

-   On the other hand, this idea seems [intuitively appealing]{.orange}.
    Is there a way to "fix" it?

## K-nearest neighbours

-   Instead of considering the values exactly equal to $x$, we could identify the pairs $(x_i, y_i)$ that are [close]{.blue} to
    (i.e. in a [neighbour]{.orange} of) $x$.

-   A natural measure of proximity between $x$ and the data points $x_i$
    is the [Euclidean distance]{.orange} $|x_i - x|$, but in principle
    any other metric could be used.

. . .

-   We consider an average of the [$k$ values]{.orange} $y_i$ whose $x_i$ are
    [nearest]{.orange} to $x$, that is: $$
    \hat{f}(x) = \frac{1}{|\mathcal{N}_x|}\sum_{i \in \mathcal{N}_x} y_i,
    $$ where $\mathcal{N}_x$ is indeed the set of $k$ points nearest to $x$ in
    Euclidean distance.

-   This method is called [$k$-nearest neighbours]{.blue} (KNN).

## K-nearest neighbours ($k = 6$)

```{r}
#| fig-width: 7.8
#| fig-height: 4.55
#| fig-align: center
library(kknn)
fit_knn <- fitted(kknn(accel ~ times, train = dataset, test = data.frame(times = times_seq), kernel = "rectangular", k = 6))

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_ribbon(aes(xmin = 51.7, xmax = 57.7), fill = "#fc7d0b", alpha = 0.6) +
  geom_ribbon(aes(xmin = 19, xmax = 21), fill = "#fc7d0b", alpha = 0.6) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = times_seq, y = fit_knn), aes(x = x, y = y), col = "#1170aa") +
  geom_vline(xintercept = 20, lty = "dashed", linewidth = 0.4) +
  geom_vline(xintercept = 54.7, lty = "dashed", linewidth = 0.4) +
  theme_minimal() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Limitations of the $k$-nearest neighbor method

-   The number of neighbours $k$ influences how "local" is the estimate.

-   The case $k = 1$ is unsatisfactory, because the average is based on
    a single data point. On the other hand, the case $k = n$ produces a
    constant (average of all the observations).

-   Hence, there is a clear [bias-variance trade-off]{.orange} in the
    choice of $k$, which should be selected e.g. via
    [cross-validation]{.blue}.

. . .

-   The $k$-nearest neighbours produces a [sensible result]{.blue}, but
    the method [can be improved]{.orange}.

-   The [blue curve]{.blue} is bumpy, because $\hat{f}(x)$ is
    [discontinuous]{.orange} in $x$.

-   Indeed, as we move $x$ from left to right, the $k$-nearest
    neighborhood remains constant, until a new point $x_i$ to the right of
    $x$ is included and one to the left is excluded.

-   This discontinuity is [ugly]{.orange} and [unnecessary]{.orange}. We
    are looking instead for a [smooth]{.blue} prediction.

## Nadaraya-Watson estimator

-   The [Nadaraya-Watson]{.blue} estimator addresses these issues. It is
    a [weighted average]{.orange} $$
    \hat{f}(x) = \frac{1}{\sum_{i'=1}^n w_{i'}(x)}\sum_{i=1}^n w_i(x) y_i = \sum_{i=1}^n s_i(x) y_i,
    $$ where $s_i(x) = w_i(x) / \sum_{i'=1}^n w_{i'}(x)$ are the
    [normalized weights]{.blue}.

. . .

-   The values $w_i(x) \ge 0$ are chosen so that the points $x_i$
    [close]{.orange} to $x$ are [weighted more]{.orange}.

. . .

-   A convenient way of selecting these weights is through [kernel
    functions]{.blue}: $$
    w_i(x) = \frac{1}{h}w\left(\frac{x_i - x}{h}\right), \qquad i=1,\dots,n,
    $$ where $w(\cdot)$ is a [symmetric density]{.orange} function
    around the origin, called kernel in this context.

-   The value $h > 0$ is a [scale factor]{.blue}, sometimes called
    [bandwidth]{.orange} or [smoothing parameter]{.orange}.

## Nadaraya-Watson estimator: comments

-   The fitted function $\hat{f}(x)$ is [continuous]{.blue} and is
    obtained by computing several weighted averages, one for each value
    of $x$.

-   A popular kernel is the [Gaussian kernel]{.orange}, that is: $$
    w_i(x) = \frac{1}{h} \phi\left(\frac{x_i - x}{h}\right), \qquad i=1,\dots,n,
    $$ so that $h^2$ represents the [variance]{.blue}. We will discuss
    alternative choices later on.

. . .

-   The most important factor, however, is not the functional form of
    $w(\cdot)$, but rather the [smoothing parameter]{.orange} $h$, which
    is a complexity parameter.

-   Indeed, $h$ defines the "[smoothing window]{.blue}" on the $x$-axis,
    i.e. the relevant data points that are considered for $\hat{f}(x)$.

-   As any complexity parameter, $h$ should be chosen via
    cross-validation or related ideas.

## Nadaraya-Watson (Gaussian kernel)

::: panel-tabset
## Smoothing $h = 1$

```{r}
#| fig-width: 7.8
#| fig-height: 4
#| fig-align: center
#| message: false
library(KernSmooth)

h_param <- 1
band <- 4 * qnorm(0.75) * h_param # Bandwidth as parametrized in ksmooth
fit_nw <- ksmooth(x, y, kernel = "normal", bandwidth = band, x.points = times_seq)

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_ribbon(aes(xmin = qnorm(0.05, 20, sd = h_param), xmax = qnorm(0.95, 20, sd = h_param)), fill = "#fc7d0b", alpha = 0.6) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = fit_nw$x, y = fit_nw$y), aes(x = x, y = y), col = "#1170aa") +
  geom_vline(xintercept = 20, lty = "dashed", linewidth = 0.4) +
  theme_minimal() +
  geom_function(fun = function(x) 150 * dnorm(x, 20, h_param) - 134, linetype = "dotted", n = 500, xlim = c(qnorm(0.001, 20, sd = h_param), xmax = qnorm(0.999, 20, sd = h_param))) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Smoothing $h = 0.3$

```{r}
#| fig-width: 7.8
#| fig-height: 4
#| fig-align: center

h_param <- 0.3
band <- 4 * qnorm(0.75) * h_param # Bandwidth as parametrized in ksmooth
fit_nw <- ksmooth(x, y, kernel = "normal", bandwidth = band, x.points = times_seq)

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_ribbon(aes(xmin = qnorm(0.05, 20, sd = h_param), xmax = qnorm(0.95, 20, sd = h_param)), fill = "#fc7d0b", alpha = 0.6) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = fit_nw$x, y = fit_nw$y), aes(x = x, y = y), col = "#1170aa") +
  geom_vline(xintercept = 20, lty = "dashed", linewidth = 0.4) +
  theme_minimal() +
  geom_function(fun = function(x) 50 * dnorm(x, 20, h_param) - 134, linetype = "dotted", n = 500, xlim = c(qnorm(0.001, 20, sd = h_param), xmax = qnorm(0.999, 20, sd = h_param))) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Smoothing $h = 2$

```{r}
#| fig-width: 7.8
#| fig-height: 4
#| fig-align: center
h_param <- 2
band <- 4 * qnorm(0.75) * h_param # Bandwidth as parametrized in ksmooth
fit_nw <- ksmooth(x, y, kernel = "normal", bandwidth = band, x.points = times_seq)

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_ribbon(aes(xmin = qnorm(0.05, 20, sd = h_param), xmax = qnorm(0.95, 20, sd = h_param)), fill = "#fc7d0b", alpha = 0.6) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = fit_nw$x, y = fit_nw$y), aes(x = x, y = y), col = "#1170aa") +
  geom_vline(xintercept = 20, lty = "dashed", linewidth = 0.4) +
  theme_minimal() +
  geom_function(fun = function(x) 180 * dnorm(x, 20, h_param) - 134, linetype = "dotted", n = 500, xlim = c(qnorm(0.001, 20, sd = h_param), xmax = qnorm(0.999, 20, sd = h_param))) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Smoothing $h = 4$

```{r}
#| fig-width: 7.8
#| fig-height: 4
#| fig-align: center
h_param <- 4
band <- 4 * qnorm(0.75) * h_param # Bandwidth as parametrized in ksmooth
fit_nw <- ksmooth(x, y, kernel = "normal", bandwidth = band, x.points = times_seq)

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_ribbon(aes(xmin = qnorm(0.05, 20, sd = h_param), xmax = qnorm(0.95, 20, sd = h_param)), fill = "#fc7d0b", alpha = 0.6) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = fit_nw$x, y = fit_nw$y), aes(x = x, y = y), col = "#1170aa") +
  geom_vline(xintercept = 20, lty = "dashed", linewidth = 0.4) +
  theme_minimal() +
  geom_function(fun = function(x) 600 * dnorm(x, 20, h_param) - 134, linetype = "dotted", n = 500, xlim = c(qnorm(0.001, 20, sd = h_param), xmax = qnorm(0.999, 20, sd = h_param))) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```
:::

## Local linear regression I

::: incremental
-   [Local linear regression]{.blue} is a refinement of the
    Nadaraya-Watson estimator that has typically [lower bias]{.orange},
    especially at the [boundaries]{.orange}, without noticeable
    increases in variance.

-   If $f(x)$ is differentiable, then it can be [approximated]{.orange}
    with a linear function tangent in $x_0$: $$
    f(x) = \underbrace{f(x_0)}_{\beta_1} + \underbrace{f'(x_0)}_{\beta_2}(x - x_0) + \text{rest}.
    $$

-   Hence, instead of computing a [local average]{.orange}
    ($\beta_2 = 0$), we consider a [local linear model]{.blue}. In other
    words, for every $x$ we seek the coefficients solving: $$
    (\hat{\beta}_1, \hat{\beta}_2) = \arg\min_{(\beta_1, \beta_2)} \sum_{i=1}^n \textcolor{darkblue}{w_i(x)}\left\{y_i - \beta_1 - \textcolor{red}{\beta_2(x_i - x)}\right\}^2.
    $$

-   Once the parameter $\hat{\beta}_1$ and $\beta_2$ are obtained, the
    local [linear regression estimator]{.blue} is $$
    \hat{f}(x) = \hat{\beta}_1 + \hat{\beta}_2 (x - x) = \hat{\beta}_1.
    $$
:::

## Local linear regression II

-   The local linear regression, as we have seen in [Unit A](un_A.html),
    has an [explicit solution]{.orange}: $$
    \hat{\beta} = (\bm{X}^T\bm{W}_x\bm{X})^{-1}\bm{X}^T\bm{W}_x\bm{y},
    $$ where the rows of $\bm{X}$ are $\bm{x}_i = (1, x_i - x)$ and
    $\bm{W}_x = \text{diag}\{w_1(x),\dots,w_n(x)\}$.

-   In practice, we do [not]{.orange} need to solve this [linear
    algebra]{.orange} problem. An even more explicit and
    [non-iterative]{.orange} solution can be found (see Exercises).

. . .

::: callout-note
#### Theorem (Local linear smoothing)

The local linear regression smoother, evaluated in $x$, admits an
[explicit expression]{.blue}: $$
\hat{f}(x) = \frac{1}{n}\sum_{i=1}^n  \frac{w_i(x) \{a_2(x) - (x_i - x) a_1(x)\}}{a_2(x)a_0(x) - a_1(x)^2 } y_i = \sum_{i=1}^n s_i(x) y_i,
$$ where $a_j(x) = n^{-1}\sum_{i=1}^n w_i(x) (x_i - x)^j$, for
$j=0,1,2$.
:::

## Local linear regression ($h = 1.45$, Gaussian kernel)

```{r}
library(KernSmooth)

h_param <- dpill(x, y)
fit_locpoly <- locpoly(x, y, bandwidth = h_param, gridsize = 2000)

ggplot(data = dataset, aes(x = times, y = accel)) +
  geom_point(size = 0.7) +
  geom_line(data = data.frame(x = fit_locpoly$x, y = fit_locpoly$y), aes(x = x, y = y), col = "#1170aa") +
  theme_minimal() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Time (ms)") +
  ylab("Head acceleration (g)")
```

## Linear smoothers I

-   The Nadaraya-Watson estimator and local linear regression are
    special instances of [linear smoothers]{.blue}, which are estimators
    admitting the following form: $$
    \hat{f}(x) = \sum_{i=1}^ns_i(x) y_i.
    $$

. . .

-   We will study other members of this class, such as regression and
    smoothing splines.

-   Polynomial regression, ridge regression, [Gaussian processes]{.blue}
    and [moving averages]{.orange} are also linear smoothers.

. . .

-   The [mean]{.blue} (and hence the bias), and the [variance]{.orange}
    of a linear smoother can be easily obtained: $$
    \mathbb{E}\{\hat{f}(x)\} = \sum_{i=1}^n s_i(x)f(x_i), \qquad \text{var}\{\hat{f}(x)\} = \sigma^2\sum_{i=1}^ns_i(x)^2.
    $$

## Linear smoothers II

-   In linear smoothers, we can express the predicted values
    $\hat{\bm{y}}$ using [matrix notation]{.orange} $$
    \hat{\bm{y}} = \sum_{i=1}^n\bm{s}_i \bm{y} = \bm{S}\bm{y}, \qquad \bm{s}_i = (s_1(x_i), \dots, s_n(x_i))^T,
    $$ where $\bm{S} = (\bm{s}_1,\dots,\bm{s}_n)^T$ is the so-called
    $n \times n$ [smoothing]{.blue} matrix.

. . .

-   Each row of the smoothing matrix $\bm{s}_i$ is called [equivalent
    kernel]{.orange} for estimating $\hat{f}(x_i)$; in the Nadaraya
    Watson estimator $\bm{s}_i$ is indeed a normalized kernel.

. . .

-   The weights of all the smoothers we will use have are such that
    $\sum_{i=1}^ns_i(x) = 1$ for all $x$.

-   Hence, the smoother [preserves constant curves]{.blue}, namely if
    all $y_i = c$, then $\hat{f}(x) = c$.

## On the choice of the kernel

-   As mentioned before, the choice of the kernel is [not crucial]{.orange}. Some alternatives are:


| Kernel       | $w(x)$                                                    | Support      |
|---------------------|-----------------------------|-----------------------|
| Gaussian     | $\frac{1}{\sqrt{2 \pi}}\exp{\left(-\frac{x^2}{2}\right)}$ | $\mathbb{R}$ |
| Rectangular  | $\frac{1}{2}$                                             | $(-1, 1)$    |
| Epanechnikov | $\frac{3}{4}(1 - x^2)$                                    | $(-1, 1)$    |
| Bi-quadratic | $\frac{15}{16}(1 - x^2)^2$                                | $(-1, 1)$    |
| Tri-cubic    | $\frac{70}{81}(1 - |x|^3)^3$                              | $(-1, 1)$    |

. . .

- Some [asymptotic]{.blue} considerations lead to the choice of the "[optimal]{.orange}" Epanechnikov kernel.

. . .

- [Bounded]{.blue} kernels have [computational]{.blue} advantages, because one needs to compute averages of a limited number of data points. 

- On the other hand, bounded kernels may lead to [discontinuous derivatives]{.orange} of $\hat{f}(x)$ that could be unappealing in certain contexts. 

## Bias-variance tradeoff

::: callout-warning
#### Theorem (Fan and Gijbels, 1996, Theorem 3.1)
Let $(X_i, Y_i)$ be iid random vectors with $g(x)$ denoting the [marginal density]{.orange} of $X_i$. The conditional law is such that $Y_i = f(X_i) + \epsilon_i$, with $\epsilon_i$ iid and $\mathbb{E}(\epsilon_i) = 0$,  $\text{var}(\epsilon_i) = \sigma^2$. 

Moreover, suppose $g(x) > 0$ and that $g(\cdot)$ and $f''(\cdot)$ are continuous in a neighborhood of $x$.  Then, as $h \rightarrow 0$ and $n h \rightarrow \infty$ we have that for the [local linear regression]{.blue} $\hat{f}(x)$ the [bias]{.orange} is
$$
\mathbb{E}\{\hat{f}(x) - f(x) \} \approx  \frac{\textcolor{red}{h^2}}{2}\sigma^2_w f''(x),
$$
where $\sigma^2_w = \int z^2 w(z)\mathrm{d}z$. In addition, the [variance]{.blue} is
$$
\text{var}\{\hat{f}(x)\} \approx \frac{\sigma^2}{\textcolor{darkblue}{n h}} \frac{\alpha_w}{g(x)},
$$
where $\alpha_w = \int w^2(z)\mathrm{d}z$. 
:::

## Bias-variance tradeoff II

- The previous theorem shows that [bias]{.orange} is of [order $h^2$]{.orange} and the [variance]{.blue} is of [order $(1 / nh)$]{.blue}. 

- Once again, there is a trade-off because we would like $h \rightarrow 0$ but, at the same time, we need to keep the variance under control. 

. . .

- We can select $h$ so that the [asymptotic mean squared error]{.blue} is [minimal]{.orange}. This leads to the following optimal choice for the bandwidth: $$
h_\text{opt}(x) = \left(\frac{1}{n} \frac{\sigma^2 \alpha_w}{\sigma^4_w f''(x)^2 g(x)}\right)^{1/5}.
$$

. . .

- Unfortunately, $h_\text{opt}(x)$ is of [little practical utility]{.orange}, as it involves the [unknown]{.orange} terms $f''(x)$, $g(x)$ and $\sigma^2$. However, it highlights two important facts:

-  The bandwidth $h$ should decrease at the rate $n^{-1/5}$, i.e. [quite slowly]{.blue}.

-  If we plug-in $h_\text{opt}(x)$ into the bias/variance formulas, we get that the mean squared error $\rightarrow 0$ at the rate $n^{-4/5}$, which is much [slower]{.orange} than the [parametric]{.orange} case $n^{-1}$.

## Choice of the bandwidth I

## Choice of the bandwidth II

## ‚ò†Ô∏è - Bias reduction of local linear regression

## Loess I

## Loess II

## Local likelihoods I

## Local likelihoods II

## The bivariate case I

## The bivariate case II

# Regression splines

# Smoothing splines

# Penalized splines

# References
