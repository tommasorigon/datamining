<!DOCTYPE html>
<html lang="en"><head>
<script src="un_C_files/libs/clipboard/clipboard.min.js"></script>
<script src="un_C_files/libs/quarto-html/tabby.min.js"></script>
<script src="un_C_files/libs/quarto-html/popper.min.js"></script>
<script src="un_C_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="un_C_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="un_C_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="un_C_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="un_C_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.433">

  <meta name="author" content="Tommaso Rigon">
  <title>Shrinkage and variable selection</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="un_C_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="un_C_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="un_C_files/libs/revealjs/dist/theme/quarto.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
  <link href="un_C_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="un_C_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="un_C_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="un_C_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Shrinkage and variable selection</h1>
  <p class="subtitle">Data Mining - CdL CLAMSES</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<span class="orange">Tommaso Rigon</span> 
</div>
        <p class="quarto-title-affiliation">
            <em>Università degli Studi di Milano-Bicocca</em>
          </p>
    </div>
</div>

</section>
<section id="homepage" class="slide level2 center">
<h2><a href="../index.html">Homepage</a></h2>
<div class="columns">
<div class="column" style="width:30%;">
<p><img data-src="img/cowboy.jpg"></p>
</div><div class="column" style="width:70%;">
<ul>
<li><p>In this unit we will cover the following <span class="orange">topics</span>:</p>
<ul>
<li>Best subset regression</li>
<li>Principal component regression</li>
<li>Ridge regression</li>
<li>Lasso, LARS, elastic-net</li>
</ul></li>
<li><p>The common themes are called <span class="blue">variable selection</span> and <span class="orange">shrinkage estimation</span>.</p></li>
<li><p>The issue we face is the presence of a high number <span class="math inline">p</span> of covariates that are <span class="blue">potentially irrelevant</span>.</p></li>
<li><p>This problem is quite challenging when the <span class="blue">ratio</span> <span class="math inline">p / n</span> is <span class="blue">large</span>.</p></li>
<li><p>In the <span class="orange">extreme case</span> <span class="math inline">p &gt; n</span>, is there any hope to fit a meaningful model?</p></li>
</ul>
</div>
</div>
</section>
<section id="the-prostate-dataset" class="slide level2 center">
<h2>The <code>prostate</code> dataset</h2>
<ul>
<li>The <code>prostate</code> cancer data investigates the relationship between the prostate-specific <span class="orange">antigen</span> and a number of clinical measures, in men about to receive a prostatectomy.</li>
</ul>
<div class="fragment">
<ul>
<li>This <a href="https://hastie.su.domains/ElemStatLearn/datasets/prostate.data">dataset</a> has been used in the <span class="orange">original paper</span> by Tibshirani (1996) to present the lasso. A description is given in <span class="blue">Section 3.2.1</span> of HTF (2009).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><p>We want to <span class="blue">predict</span> the logarithm of a <span class="orange">prostate-specific antigen</span> (<code>lpsa</code>) as a function of:</p>
<ul>
<li>logarithm of the cancer volume (<code>lcavol</code>);</li>
<li>logarithm of the prostate weight (<code>lweight</code>);</li>
<li>age each man (<code>age</code>);</li>
<li>logarithm of the benign prostatic hyperplasia amount (<code>lbph</code>);</li>
<li>seminal vesicle invasion (<code>svi</code>), a binary variable;</li>
<li>logarithm of the capsular penetration (<code>lcp</code>);</li>
<li>Gleason score (<code>gleason</code>), an ordered categorical variable;</li>
<li>Percentage of Gleason scores <span class="math inline">4</span> and <span class="math inline">5</span> (<code>pgg45</code>).</li>
</ul></li>
</ul>
</div>
</section>
<section id="a-glimpse-of-the-prostate-dataset" class="slide level2 center">
<h2>A <code>glimpse</code> of the <code>prostate</code> dataset</h2>
<ul>
<li><p>Summarizing, there are in total <span class="math inline">8</span> <span class="orange">variables</span> that can be used to predict the antigen <code>lpsa</code>.</p></li>
<li><p>We <span class="orange">centered</span> and <span class="blue">standardized</span> all the covariates before the training/test split.</p></li>
<li><p>There are <span class="math inline">n = 67</span> observations in the <span class="orange">training</span> set and <span class="math inline">30</span> in the <span class="blue">test</span> set.</p></li>
</ul>
<div class="fragment">
<div class="panel-tabset">
<ul id="tabset-1" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-1-1">Original dataset</a></li><li><a href="#tabset-1-2">Standardized dataset</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1">
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 97
Columns: 10
$ lcavol  &lt;dbl&gt; -0.5798185, -0.9942523, -0.5108256, -1.2039728, 0.7514161, -1.…
$ lweight &lt;dbl&gt; 2.769459, 3.319626, 2.691243, 3.282789, 3.432373, 3.228826, 3.…
$ age     &lt;int&gt; 50, 58, 74, 58, 62, 50, 64, 58, 47, 63, 65, 63, 63, 67, 57, 66…
$ lbph    &lt;dbl&gt; -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1…
$ svi     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
$ lcp     &lt;dbl&gt; -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1…
$ gleason &lt;int&gt; 6, 6, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 6, 7, 6, 6, 6, 6,…
$ pgg45   &lt;int&gt; 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 30, 5, 5, 0, 30, 0, 0, 0,…
$ lpsa    &lt;dbl&gt; -0.4307829, -0.1625189, -0.1625189, -0.1625189, 0.3715636, 0.7…
$ train   &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE,…</code></pre>
</div>
</div>
</div>
<div id="tabset-1-2">
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 97
Columns: 10
$ lcavol  &lt;dbl&gt; -1.63735563, -1.98898046, -1.57881888, -2.16691708, -0.5078744…
$ lweight &lt;dbl&gt; -2.00621178, -0.72200876, -2.18878403, -0.80799390, -0.4588340…
$ age     &lt;dbl&gt; -1.86242597, -0.78789619, 1.36116337, -0.78789619, -0.25063130…
$ lbph    &lt;dbl&gt; -1.0247058, -1.0247058, -1.0247058, -1.0247058, -1.0247058, -1…
$ svi     &lt;dbl&gt; -0.5229409, -0.5229409, -0.5229409, -0.5229409, -0.5229409, -0…
$ lcp     &lt;dbl&gt; -0.8631712, -0.8631712, -0.8631712, -0.8631712, -0.8631712, -0…
$ gleason &lt;dbl&gt; -1.0421573, -1.0421573, 0.3426271, -1.0421573, -1.0421573, -1.…
$ pgg45   &lt;dbl&gt; -0.8644665, -0.8644665, -0.1553481, -0.8644665, -0.8644665, -0…
$ lpsa    &lt;dbl&gt; -0.4307829, -0.1625189, -0.1625189, -0.1625189, 0.3715636, 0.7…
$ train   &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE,…</code></pre>
</div>
</div>
</div>
</div>
</div>
<!-- -   The variable `train` splits the data into a training and test set, -->
<!--     as in the textbook. -->
</div>
</section>
<section id="correlation-matrix-of-prostate" class="slide level2 center">
<h2>Correlation matrix of <code>prostate</code></h2>

<img data-src="un_C_files/figure-revealjs/unnamed-chunk-4-1.png" width="2250" class="r-stretch quarto-figure-center"></section>
<section id="the-variable-selection-problem" class="slide level2 center">
<h2>The variable selection problem</h2>
<ul>
<li>We consider a <span class="orange">linear model</span> in which the relationship between the response variable <span class="math inline">Y_i</span> (<code>lpsa</code>) and the covariates is modelled through the function<span class="math display">
  f(\bm{x}_i; \beta_0, \beta) = \beta_0+ \beta_1 x_{i1} + \cdots + \beta_p x_{ip} =\beta_0 + \bm{x}_i^T\beta.
  </span> In this unit the <span class="blue">intercept</span> <span class="math inline">\beta_0</span> will play a special role, therefore we use this slightly different notation compared to <a href="unit_A.html">Unit A</a>.</li>
</ul>
<div class="fragment">
<ul>
<li><p>Including a lot of covariates into the model is not necessarily a good thing!</p></li>
<li><p>Indeed, some variables are likely to be <span class="blue">irrelevant</span>:</p>
<ul>
<li>they might be <span class="orange">correlated</span> with other covariates and therefore <span class="orange">redundant</span>;</li>
<li>they could be uncorrelated with the response <code>lpsa</code>.</li>
</ul></li>
</ul>
</div>
<div class="fragment">
<ul>
<li><p>If we use all the <span class="math inline">p = 8</span> available covariates, the estimated <span class="math inline">f(\bm{x}; \hat{\beta_0}, \hat{\beta})</span> might have a <span class="orange">high variance</span>, without important gain in term of bias, i.e.&nbsp;a <span class="blue">large mean squared error</span>.</p></li>
<li><p>We are looking for a <span class="orange">simpler model</span> having, hopefully, a lower mean squared error.</p></li>
</ul>
</div>
</section>
<section id="a-naïve-approach-abusing-p-values" class="slide level2 center">
<h2>A naïve approach: (ab)using p-values</h2>
<div style="font-size: 75%;">
<div class="cell">
<div class="cell-output-display">
<table>
<colgroup>
<col style="width: 13%">
<col style="width: 16%">
<col style="width: 9%">
<col style="width: 10%">
<col style="width: 8%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 8%">
<col style="width: 10%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">(Intercept)</th>
<th style="text-align: right;">lcavol</th>
<th style="text-align: right;">lweight</th>
<th style="text-align: right;">age</th>
<th style="text-align: right;">lbph</th>
<th style="text-align: right;">svi</th>
<th style="text-align: right;">lcp</th>
<th style="text-align: right;">gleason</th>
<th style="text-align: right;">pgg45</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">estimate</td>
<td style="text-align: right;">2.46</td>
<td style="text-align: right;">0.68</td>
<td style="text-align: right;">0.26</td>
<td style="text-align: right;">-0.14</td>
<td style="text-align: right;">0.21</td>
<td style="text-align: right;">0.31</td>
<td style="text-align: right;">-0.29</td>
<td style="text-align: right;">-0.02</td>
<td style="text-align: right;">0.27</td>
</tr>
<tr class="even">
<td style="text-align: left;">std.error</td>
<td style="text-align: right;">0.09</td>
<td style="text-align: right;">0.13</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.10</td>
<td style="text-align: right;">0.12</td>
<td style="text-align: right;">0.15</td>
<td style="text-align: right;">0.15</td>
<td style="text-align: right;">0.15</td>
</tr>
<tr class="odd">
<td style="text-align: left;">statistic</td>
<td style="text-align: right;">27.60</td>
<td style="text-align: right;">5.37</td>
<td style="text-align: right;">2.75</td>
<td style="text-align: right;">-1.40</td>
<td style="text-align: right;">2.06</td>
<td style="text-align: right;">2.47</td>
<td style="text-align: right;">-1.87</td>
<td style="text-align: right;">-0.15</td>
<td style="text-align: right;">1.74</td>
</tr>
<tr class="even">
<td style="text-align: left;">p.value</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.00</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">0.17</td>
<td style="text-align: right;">0.04</td>
<td style="text-align: right;">0.02</td>
<td style="text-align: right;">0.07</td>
<td style="text-align: right;">0.88</td>
<td style="text-align: right;">0.09</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div>
<ul>
<li class="fragment"><p>It is common practice to use the <span class="orange">p-values</span>, e.g.&nbsp;those obtained through the <code>summary</code> function, to perform <span class="blue">model selection</span> in a stepwise fashion. <!-- -   A typical procedure is to omit "non significant" coefficients, refit --> <!--     the model, and repeat this scheme until we obtain only "significant" --> <!--     coefficients. --></p></li>
<li class="fragment"><p>This is <span class="orange">not a good idea</span>, at least when done without appropriate <span class="blue">multiplicity corrections</span>.</p></li>
<li class="fragment"><p>The above p-values are meant to be used in the context of a single hypothesis testing problem, <span class="orange">not</span> to make <span class="orange">iterative choices</span>.</p></li>
</ul>
<!-- -   Such an iterative usage of "univariate" p-values is formally [incorrect]{.orange} -->
<!--     because it leads to the well-known [multiple testing -->
<!--     problem]{.blue}. -->
</div>
</section>
<section id="to-explain-or-to-predict" class="slide level2 center">
<h2>To explain or to predict?</h2>
<ul>
<li>“<em>All models are approximations. Essentially, all models are wrong, but some are useful</em>.” George E. P. Box</li>
</ul>
<div class="fragment">
<ul>
<li><p>If the <span class="blue">focus</span> is on <span class="blue">prediction</span>, we do not necessarily care about selecting the “true” set of parameters.</p></li>
<li><p>In many data mining problems, the focus is on <span class="orange">minimizing</span> the <span class="orange">prediction errors</span>.</p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Hence, often times we may <span class="blue">accept some bias</span> (i.e.&nbsp;we use a “wrong” but useful model), if this leads to a <span class="orange">reduction in variance</span>.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><p>Besides, in certain cases it does not even make much sense to speak about the “true parameters”.</p></li>
<li><p>For example, what if the true <span class="math inline">f(\bm{x})</span> were not linear? In this context, a <span class="blue">linear model</span> is simply an approximation of the unknown <span class="math inline">f(\bm{x})</span> and hypothesis testing procedures are ill-posed.</p></li>
</ul>
</div>
</section>
<section id="best-subset-selection" class="slide level2 center">
<h2>Best subset selection</h2>
<ul>
<li><p>Let us get back to our <span class="blue">variable selection problem</span>.</p></li>
<li><p>In principle, we could perform an <span class="orange">exhaustive search</span> considering all the <span class="math inline">2^p</span> possible models and then selecting the one having the best out-of-sample predictive performance.</p></li>
</ul>
<div class="fragment">
<div class="callout callout-note no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Best subset procedure</strong></p>
</div>
<div class="callout-content">
<ol type="1">
<li><p>Let <span class="math inline">\mathcal{M}_0</span> be the <span class="blue">null model</span>, which contains no predictors, i.e.&nbsp;set <span class="math inline">\hat{y}_i = \hat{\beta}_0 = \bar{y}</span>.</p></li>
<li><p>For <span class="math inline">k =1,\dots,p</span>, do:</p>
<ol type="i">
<li><p>Estimate <span class="orange">all</span> the <span class="math inline">\binom{p}{k}</span> models that contain exactly <span class="math inline">k</span> covariates;</p></li>
<li><p>Identify the “best” model with <span class="math inline">k</span> covariates having the smallest <span class="math inline">\text{MSE}_{k, \text{train}}</span>; call it <span class="math inline">\mathcal{M}_k</span>.</p></li>
</ol></li>
</ol>
</div>
</div>
</div>
<ul>
<li>A model with more variables has lower <span class="orange">training</span> error, namely <span class="math inline">\text{MSE}_{k + 1, \text{train}} \le \text{MSE}_{k, \text{train}}</span> by construction. Hence, the optimal subset size <span class="math inline">k</span> must be chosen e.g.&nbsp;via <span class="blue">cross-validation</span>.</li>
</ul>
</div>
</section>
<section id="step-1.-and-2.-of-best-subset-selection" class="slide level2 center">
<h2>Step 1. and 2. of best subset selection</h2>

<img data-src="un_C_files/figure-revealjs/unnamed-chunk-8-1.png" width="1500" class="r-stretch quarto-figure-center"></section>
<section id="the-best-models-mathcalm_1dots-mathcalm_p" class="slide level2 center">
<h2>The “best” models <span class="math inline">\mathcal{M}_1,\dots, \mathcal{M}_p</span></h2>
<ul>
<li>The output of the <span class="orange">best subset selection</span>, on the training set is:</li>
</ul>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>         lcavol lweight age lbph svi lcp gleason pgg45
1  ( 1 ) "*"    " "     " " " "  " " " " " "     " "  
2  ( 1 ) "*"    "*"     " " " "  " " " " " "     " "  
3  ( 1 ) "*"    "*"     " " " "  "*" " " " "     " "  
4  ( 1 ) "*"    "*"     " " "*"  "*" " " " "     " "  
5  ( 1 ) "*"    "*"     " " "*"  "*" " " " "     "*"  
6  ( 1 ) "*"    "*"     " " "*"  "*" "*" " "     "*"  
7  ( 1 ) "*"    "*"     "*" "*"  "*" "*" " "     "*"  
8  ( 1 ) "*"    "*"     "*" "*"  "*" "*" "*"     "*"  </code></pre>
</div>
</div>
<div>
<ul>
<li class="fragment"><p>The above table means that the best model with <span class="math inline">k = 1</span> uses the variable <code>lcavol</code>, whereas when <span class="math inline">k = 2</span> the selected variables are <code>lcavol</code> and <code>lweight</code>, and so on and so forth.</p></li>
<li class="fragment"><p>Note that, in general, these models are <span class="orange">not</span> necessarily <span class="orange">nested</span>, i.e.&nbsp;a variable selected at step <span class="math inline">k</span> is not necessarily included at step <span class="math inline">k +1</span>. Here they are, but it is a coincidence.</p></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>What is the <span class="orange">optimal subset size</span> <span class="math inline">k</span> in terms on out-of-sample mean squared error?</li>
</ul>
</div>
</section>
<section id="the-wrong-way-of-doing-cross-validation" class="slide level2 center">
<h2>The wrong way of doing cross-validation</h2>
<div>
<ul>
<li class="fragment"><p>Consider a regression problem with a <span class="orange">large number of predictors</span> (relative to <span class="math inline">n</span>) such as the <code>prostate</code> dataset.</p></li>
<li class="fragment"><p>A typical strategy for analysis might be as follows:</p>
<ol type="1">
<li class="fragment"><p>Screen the predictors: find a subset of “good” predictors that show fairly strong correlation with the response;</p></li>
<li class="fragment"><p>Using this subset of predictors (e.g.&nbsp;<code>lcavol</code>, <code>lweight</code> and <code>svi</code>), build a regression model;</p></li>
<li class="fragment"><p>Use cross-validation to estimate the prediction error of the model of the step 2.</p></li>
</ol></li>
<li class="fragment"><p>Is this a correct application of cross-validation?</p></li>
<li class="fragment"><p>If your reaction was “<span class="orange">this is absolutely wrong!</span>”, it means you correctly understood the principles of cross-validation.</p></li>
<li class="fragment"><p>If you though this was an ok-ish idea, you may want to read <span class="blue">Section 7.10.2</span> of HTF (2009), called “the wrong way of doing cross-validation”.</p></li>
</ul>
</div>
</section>
<section id="step-3.-of-best-subset-selection-via-cross-validation" class="slide level2 center">
<h2>Step 3. of best subset selection via cross-validation</h2>

<img data-src="un_C_files/figure-revealjs/unnamed-chunk-11-1.png" width="1500" class="r-stretch quarto-figure-center"><ul>
<li>By applying the “1 standard error rule”, we select <span class="math inline">k = 2</span>, i.e.&nbsp;<code>lcavol</code> and <code>lweight</code>.</li>
</ul>
</section>
<section id="comments-and-computations" class="slide level2 center">
<h2>Comments and computations</h2>
<div>
<ul>
<li class="fragment"><p>The correct way of doing cross-validation requires that the <span class="blue">best subset selection</span> is performed on <span class="orange">every fold</span>, possibly obtaining different “best” models with the same size.</p></li>
<li class="fragment"><p>Best subset selection is conceptually appealing, but it has a <span class="orange">major limitation</span>. There are <span class="math display">
\sum_{k=1}^p \binom{n}{k} = 2^p
</span> models to consider, which is <span class="orange">computationally prohibitive</span>!</p></li>
<li class="fragment"><p>There exists algorithms (i.e.&nbsp;<span class="blue">leaps and bounds</span>) that makes this feasible for <span class="math inline">p \approx 30</span>.</p></li>
<li class="fragment"><p>Recently, <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-44/issue-2/Best-subset-selection-via-a-modern-optimization-lens/10.1214/15-AOS1388.full">Bertsimas et al., 2016</a> proposed the usage of a mixed integer optimization formulation, allowing <span class="math inline">p</span> to be in the order of hundreds.</p></li>
<li class="fragment"><p>Despite these advances, this problem remains <span class="orange">computationally very expensive</span>. See also the recent paper <a href="https://projecteuclid.org/journals/statistical-science/volume-35/issue-4/Best-Subset-Forward-Stepwise-or-Lasso-Analysis-and-Recommendations-Based/10.1214/19-STS733.full">Hastie et al.&nbsp;(2020)</a> for additional considerations and comparisons.</p></li>
</ul>
</div>
</section>
<section id="forward-regression" class="slide level2 center">
<h2>Forward regression</h2>
<ul>
<li>Forward regression is <span class="orange">greedy approximation</span> of best subset selection, that produces a sequence of <span class="blue">nested</span> models. It is computationally feasible and can be applied when <span class="math inline">p &gt; n</span>.</li>
</ul>
<div class="fragment">
<div class="callout callout-note no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Forward regression</strong></p>
</div>
<div class="callout-content">
<ol type="1">
<li><p>Let <span class="math inline">\mathcal{M}_0</span> be the <span class="blue">null model</span>, which contains no predictors, i.e.&nbsp;set <span class="math inline">\hat{y}_i = \hat{\beta}_0 = \bar{y}</span>.</p></li>
<li><p>For <span class="math inline">k = 0,\dots, \min(n - 1, p - 1)</span>, do:</p>
<ol type="i">
<li><p>Consider the <span class="math inline">p − k</span> models that augment the predictors in <span class="math inline">\mathcal{M}_k</span> with <span class="orange">one additional covariate</span>.</p></li>
<li><p>Identify the “best” model among the above <span class="math inline">p - k</span> competitors having the smallest <span class="math inline">\text{MSE}_{k, \text{train}}</span> and call it <span class="math inline">\mathcal{M}_k</span>.</p></li>
</ol></li>
</ol>
</div>
</div>
</div>
</div>
<div class="fragment">
<ul>
<li>It can be shown that the identification of the <span class="blue">optimal new predictor</span> can be efficiently computed using the <span class="orange">QR decomposition</span> (see Exercises).</li>
</ul>
</div>
</section>
<section id="backward-regression" class="slide level2 center">
<h2>Backward regression</h2>
<ul>
<li>When <span class="math inline">p &lt; n</span>, an alternative greedy approach is <span class="orange">backward regression</span>, which also produces a sequence of <span class="blue">nested</span> models.</li>
</ul>
<div class="fragment">
<div class="callout callout-note no-icon callout-titled callout-style-simple">
<div class="callout-body">
<div class="callout-title">
<p><strong>Backward regression</strong></p>
</div>
<div class="callout-content">
<ol type="1">
<li><p>Let <span class="math inline">\mathcal{M}_p</span> be the <span class="blue">full model</span>, which contains all the predictors.</p></li>
<li><p>For <span class="math inline">k = p, p - 1,\dots, 1</span>, do:</p>
<ol type="i">
<li><p>Consider the <span class="math inline">k</span> models that contain <span class="orange">all but one</span> of the predictors in <span class="math inline">\mathcal{M}_k</span>, for a total of <span class="math inline">k − 1</span> predictors.</p></li>
<li><p>Identify the “best” model <span class="math inline">\mathcal{M}_k</span> among these <span class="math inline">k</span> models having the smallest <span class="math inline">\text{MSE}_{k, \text{train}}</span>.</p></li>
</ol></li>
</ol>
</div>
</div>
</div>
</div>
<div class="fragment">
<ul>
<li>It can be shown that the <span class="blue">dropped predictor</span> is the one with the lowest absolute <span class="math inline">Z</span>-score or, equivalently, the <span class="orange">highest p-value</span> (see Exercises).</li>
</ul>
</div>
</section>
<section id="forward-backward-and-best-subset" class="slide level2 center">
<h2>Forward, backward and best subset</h2>

<img data-src="un_C_files/figure-revealjs/unnamed-chunk-13-1.png" width="1500" class="r-stretch quarto-figure-center"><ul>
<li>In the <code>prostate</code> dataset, forward, backward and best subset selection all gave exactly the <span class="orange">same path of solutions</span> on the full training set.</li>
</ul>
</section>
<section id="extension-to-generalized-linear-models" class="slide level2 center">
<h2>Extension to generalized linear models</h2>
</section>
<section>
<section id="principal-components-regression" class="title-slide slide level1 center">
<h1>Principal components regression</h1>

</section>
<section id="data-compression" class="slide level2 center">
<h2>Data compression</h2>
<div class="columns">
<div class="column" style="width:30%;">
<p><img data-src="img/cowboy_pixel.jpg"></p>
</div><div class="column" style="width:70%;">
<ul>
<li><p>At this point we established that <span class="orange">many covariates = many problems</span>.</p></li>
<li><p>Instead of selecting the “best” variables, let us consider a different perspective.</p></li>
<li><p>We consider a <span class="blue">compressed</span> version of the covariates that has smaller dimension <span class="math inline">k</span> but retains most information.</p></li>
<li><p>Intuitively, we want to <span class="orange">reduce the variance</span> by finding a good compression, without sacrificing too much bias.</p></li>
<li><p>The main statistical tool, unsurprisingly, will be the celebrated <span class="blue">principal components analysis</span> (PCA).</p></li>
<li><p>You should already know PCA as a tool for explorative analysis… We will focus on its usage for regression purposes.</p></li>
</ul>
</div>
</div>
</section>
<section id="centering-the-predictors-i" class="slide level2 center">
<h2>Centering the predictors I</h2>
<div>
<ul>
<li class="fragment"><p>Let us consider a <span class="blue">reparametrization</span> of the linear model using <span class="orange">centered predictors</span> <span class="math display">
\begin{aligned}
f(\bm{x}_i; \alpha, \beta) &amp; = \alpha + \beta_1 (x_{i1} - \bar{x}_1) + \cdots + \beta_p (x_{ip} - \bar{x}_{ip}) = \alpha + (\bm{x}_i -\bar{\bm{x}})^T\beta \\
&amp; = \alpha - \bar{\bm{x}}^T\beta + \bm{x}_i^T\beta = \beta_0 + \bm{x}_i^T\beta, \quad \text{where} \quad \beta_0 = \alpha - \bar{\bm{x}}^T\beta.
\end{aligned}
</span></p></li>
<li class="fragment"><p>In this reparametrization the centered predictors are <span class="blue">orthogonal</span> to the <span class="blue">intercept</span>. Thus, the estimates for <span class="math inline">(\alpha, \beta)</span> can be computed <span class="orange">in two steps</span>.</p></li>
<li class="fragment"><p>The <span class="orange">estimate</span> of the <span class="orange">intercept</span> with centered predictors is <span class="math inline">\hat{\alpha} = \bar{y}</span>. In fact: <span class="math display">
\hat{\alpha} = \arg\min_{\alpha \in \mathbb{R}}\sum_{i=1}^n\{y_i - \alpha - (\bm{x}_i -\bar{\bm{x}})^T\beta\}^2 = \frac{1}{n}\sum_{i=1}^n\{y_i - (\bm{x}_i -\bar{\bm{x}})^T\beta\} = \frac{1}{n}\sum_{i=1}^ny_i.
</span></p></li>
<li class="fragment"><p>Then, the <span class="blue">estimate of <span class="math inline">\beta</span></span> can be obtained considering a linear model <span class="blue">without intercept</span>: <span class="math display">
    f(\bm{x}_i; \beta) = (\bm{x}_i -\bar{\bm{x}})^T\beta,
</span> used to predict the <span class="blue">centered responses</span> <span class="math inline">y_i - \bar{y}</span>.</p></li>
</ul>
</div>
</section>
<section id="centering-the-predictors-ii" class="slide level2 center">
<h2>Centering the predictors II</h2>
<div>
<ul>
<li class="fragment"><p>From here on, and without loss of generality, we assume that everything is <span class="orange">centered</span>: <span class="math display">
\sum_{i=1}^ny_i = 0, \qquad \sum_{i=1}^nx_{ij} = 0, \qquad j=1,\dots,p.
</span></p></li>
<li class="fragment"><p>Hence, because of the previous results, we can focus on linear models <span class="blue">without intercept</span>: <span class="math display">
f(\bm{x}_i; \beta) = x_{i1}\beta_1 + \cdots + x_{ip}\beta_p = \bm{x}_i^T\beta.
</span></p></li>
<li class="fragment"><p>Under such an assumption the <span class="orange">covariance matrix</span> of the data is simply <span class="math display">
S = \frac{1}{n} \bm{X}^T\bm{X}.
</span></p></li>
<li class="fragment"><p>If in addition each variable has been <span class="blue">scaled</span> by their standard deviations, then <span class="math inline">n^{-1} \bm{X}^T\bm{X}</span> corresponds to the <span class="blue">correlation</span> matrix.</p></li>
</ul>
</div>
</section>
<section id="singular-value-decomposition-svd" class="slide level2 center">
<h2>Singular value decomposition (SVD)</h2>
<ul>
<li><p>Let <span class="math inline">\bm{X}</span> be a <span class="math inline">n \times p</span> matrix. Then, its full form <span class="orange">singular value decomposition</span> is: <span class="math display">
\bm{X} = \bm{U} \bm{D} \bm{V}^T = \sum_{j=1}^m d_j \tilde{\bm{u}}_j \tilde{\bm{v}}_j^T,
</span> with <span class="math inline">m =\min\{n, p\}</span> and where:</p>
<ul>
<li>the <span class="math inline">n \times n</span> matrix <span class="math inline">\bm{U} = (\tilde{\bm{u}}_1, \dots, \tilde{\bm{u}}_n)</span> is <span class="orange">orthogonal</span>, namely: <span class="math inline">\bm{U}^T \bm{U} = \bm{U}\bm{U}^T= I_n</span>;</li>
<li>the <span class="math inline">p \times p</span> matrix <span class="math inline">\bm{V} = (\tilde{\bm{v}}_1,\dots,\tilde{\bm{v}}_p)</span> is <span class="orange">orthogonal</span>, namely: <span class="math inline">\bm{V}^T \bm{V} = \bm{V}\bm{V}^T= I_p</span>;</li>
<li>the <span class="math inline">n \times p</span> matrix <span class="math inline">\bm{D}</span> has <span class="blue">diagonal</span> entries <span class="math inline">[\bm{D}]_{jj} = d_j</span>, for <span class="math inline">j=1,\dots,m</span>, and zero entries elsewhere;</li>
</ul></li>
<li><p>The real numbers <span class="math inline">d_1 \ge d_2 \ge \cdots \ge d_m \ge 0</span> are called <span class="blue">singular values</span>.</p></li>
<li><p>If one or more <span class="math inline">d_j = 0</span>, then the matrix <span class="math inline">\bm{X}</span> is singular.</p></li>
</ul>
</section>
<section id="principal-component-analysis-i" class="slide level2 center">
<h2>Principal component analysis I</h2>
<div>
<ul>
<li class="fragment"><p>Le us assume that <span class="math inline">p &lt; n</span> and that <span class="math inline">\text{rk}(\bm{X}) = p</span>, recalling that <span class="math inline">\bm{X}</span> is a <span class="orange">centered</span> matrix.</p></li>
<li class="fragment"><p>Using SVD, the matrix <span class="math inline">\bm{X}^T\bm{X}</span> can be expressed as <span class="math display">
\bm{X}^T\bm{X} = (\bm{U} \bm{D} \bm{V}^T)^T \bm{U} \bm{D} \bm{V}^T = \bm{V} \bm{D}^T \textcolor{red}{\bm{U}^T \bm{U}} \bm{D} \bm{V}^T = \bm{V} \bm{\Delta}^2 \bm{V},
</span> where <span class="math inline">\bm{\Delta}^2 = \bm{D}^T\bm{D}</span> is a <span class="math inline">p \times p</span> <span class="blue">diagonal</span> matrix with entries <span class="math inline">d_1^2,\dots,d_p^2</span>.</p></li>
<li class="fragment"><p>This equation is at the heart of <span class="blue">principal component analysis</span> (PCA). Define the matrix <span class="math display">
\bm{Z} = \bm{X}\bm{V} = \bm{U}\bm{D},
</span> whose columns <span class="math inline">\tilde{\bm{z}}_1,\dots,\tilde{\bm{z}}_p</span> are called <span class="orange">principal components</span>.</p></li>
<li class="fragment"><p>The matrix <span class="math inline">\bm{Z}</span> is orthogonal, because <span class="math inline">\bm{Z}^T\bm{Z} = \bm{D}^T\textcolor{red}{\bm{U}^T \bm{U}} \bm{D} = \bm{\Delta}^2</span>, which is diagonal.</p></li>
<li class="fragment"><p>Moreover, by definition the entries of <span class="math inline">\bm{Z}</span> are linear combination of the original variables: <span class="math display">
z_{ij} = x_{i1}v_{i1} + \cdots + x_{ip} v_{ip} = \bm{x}_i^T\tilde{\bm{v}}_j.
</span> The columns <span class="math inline">\tilde{\bm{v}}_1,\dots,\tilde{\bm{v}}_p</span> of <span class="math inline">\bm{V}</span> are sometimes called <span class="blue">loadings</span>.</p></li>
</ul>
</div>
</section>
<section id="principal-component-analysis-ii" class="slide level2 center">
<h2>Principal component analysis II</h2>
<div>
<ul>
<li class="fragment"><p>Principal components form an orthogonal basis of <span class="math inline">\bm{X}</span>, but they are not a “random” choice and they do <span class="orange">not</span> coincide with them <span class="orange">Gram-Schmidt</span> basis of <a href="un_A.html#the-qr-decomposition-i">Unit A</a>.</p></li>
<li class="fragment"><p>Indeed, the <span class="blue">first principal component</span> is the linear combination having <span class="orange">maximal variance</span>: <span class="math display">
\tilde{\bm{v}}_1 = \arg\max_{\bm{v} \in \mathbb{R}^p} \text{var}(\bm{X}\bm{v})= \arg\max_{\bm{v} \in \mathbb{R}^p} \frac{1}{n} \bm{v}^T\bm{X}^T\bm{X} \bm{v}, \quad \text{ subject to } \quad \bm{v}^T \bm{v} = 1.
</span></p></li>
<li class="fragment"><p>The <span class="blue">second principal component</span> maximizes the variance under the additional constraint of being <span class="orange">orthogonal</span> to the former. And so on and so forth.</p></li>
<li class="fragment"><p>The values <span class="math inline">d_1^2 \ge d_2^2 \ge \dots \ge d_p^2 &gt; 0</span> are the <span class="orange">eigenvalues</span> of <span class="math inline">\bm{X}^T\bm{X}</span> and correspond to the rescaled <span class="blue">variances</span> of each principal component, that is <span class="math inline">\text{var}(\tilde{\bm{z}}_j) = \tilde{\bm{z}}_j^T \tilde{\bm{z}}_j/n = d^2_j / n</span>.</p></li>
<li class="fragment"><p>Hence, the quantity <span class="math inline">d_j^2 / \sum_{j'=1}^p d_{j'}^2</span> measures the amount of total variance captured by principal components.</p></li>
</ul>
</div>
</section>
<section id="principal-component-analysis-prostate-data" class="slide level2 center">
<h2>Principal component analysis: <code>prostate</code> data</h2>

<img data-src="un_C_files/figure-revealjs/unnamed-chunk-14-1.png" width="1500" class="r-stretch quarto-figure-center"></section>
<section id="principal-components-regression-pcr" class="slide level2 center">
<h2>Principal components regression (PCR)</h2>
<div>
<ul>
<li class="fragment"><p>We use the first <span class="math inline">k \le p</span> <span class="blue">principal components</span> to predict the responses <span class="math inline">Y_i</span> via <span class="math display">
f(\bm{z}_i; \gamma) = \gamma_1 z_{i1} + \cdots + \gamma_kz_{ik}, \qquad i=1,\dots,n,
</span></p></li>
<li class="fragment"><p>Because of orthogonality, the least squares solution is straightforward to compute: <span class="math display">
\hat{\gamma}_j = \frac{\tilde{\bm{z}}_j^T\bm{y}}{\tilde{\bm{z}}_j^T\tilde{\bm{z}}_j} = \frac{1}{d_j^2}\tilde{\bm{z}}_j^T\bm{y}, \qquad j=1,\dots,k.
</span></p></li>
<li class="fragment"><p>The principal components are in <span class="blue">order of importance</span> and effectively <span class="blue">compressing the information</span> contained in <span class="math inline">\bm{X}</span> using only <span class="math inline">k \le p</span> variables.</p></li>
<li class="fragment"><p>When <span class="math inline">k = p</span> we are simply rotating the original matrix <span class="math inline">\bm{X} = \bm{Z}\bm{V}</span>, i.e.&nbsp;performing <span class="orange">no compression</span>. The predicted values coincide with OLS.</p></li>
<li class="fragment"><p>The number <span class="math inline">k</span> is a <span class="blue">complexity parameter</span> which should be chosen via information criteria or cross-validation.</p></li>
</ul>
</div>
</section>
<section id="selection-of-k-cross-validation" class="slide level2 center">
<h2>Selection of <span class="math inline">k</span>: cross-validation</h2>

<img data-src="un_C_files/figure-revealjs/unnamed-chunk-16-1.png" width="1500" class="r-stretch quarto-figure-center"></section>
<section id="shrinkage-effect-of-principal-components-i" class="slide level2 center">
<h2>Shrinkage effect of principal components I</h2>
<div>
<ul>
<li class="fragment"><p>A closer look to the PCR solution reveals some interesting aspects. Recall that: <span class="math display">
\tilde{\bm{z}}_j = \bm{X}\tilde{\bm{v}}_j = d_j \tilde{\bm{u}}_j,  \qquad j=1,\dots,p.
</span></p></li>
<li class="fragment"><p>The <span class="orange">predicted values</span> <span class="math inline">\hat{\bm{y}} = (\hat{y}_1,\dots,\hat{y}_n)</span> of the PCR with <span class="math inline">k</span> components are: <span class="math display">
\hat{\bm{y}} = \sum_{j=1}^k \tilde{\bm{z}}_j \hat{\gamma}_j = \bm{X} \sum_{j=1}^k \tilde{\bm{v}}_j \hat{\gamma}_j = \bm{X}\hat{\beta}_\text{pcr}, \quad \text{ where } \quad \hat{\beta}_\text{pcr} = \sum_{j=1}^k \tilde{\bm{v}}_j \hat{\gamma}_j.
</span></p></li>
<li class="fragment"><p>This representation highlights two important aspects:</p>
<ul>
<li class="fragment">It is possible to express the PCR solution in the original scale, for better <span class="orange">interpretability</span>;</li>
<li class="fragment">The vector <span class="math inline">\hat{\beta}_\text{pcr}</span> is a <span class="blue">constrained solution</span>, being a combination of <span class="math inline">k \le p</span> coefficients, therefore <span class="orange">reducing</span> the <span class="orange">complexity</span> of the model and <span class="blue">shrinking</span> the coefficients.</li>
</ul></li>
<li class="fragment"><p>When <span class="math inline">k = 1</span>, then the <span class="math inline">\hat{\beta}_\text{pcr}</span> estimate coincide with the scaled loading vector <span class="math inline">\hat{\beta}_\text{pcr} = \hat{\gamma}_1 \tilde{\bm{v}}_1</span>;</p></li>
<li class="fragment"><p>When <span class="math inline">k = p</span> then the <span class="math inline">\hat{\beta}_\text{pcr}</span> coincides with <span class="blue">ordinary least squares</span> (see Exercises).</p></li>
</ul>
</div>
</section>
<section id="shrinkage-effect-of-principal-components-ii" class="slide level2 center">
<h2>Shrinkage effect of principal components II</h2>
<div>
<ul>
<li class="fragment"><p>The <span class="orange">variance</span> of <span class="math inline">\hat{\beta}_\text{pcr}</span>, assuming iid errors <span class="math inline">\epsilon_i</span> and after some algebraic manipulation, results: <span class="math display">
\text{var}(\hat{\beta}_\text{pcr}) = \sigma^2\sum_{j=1}^k \frac{1}{d_j^2} \tilde{\bm{v}}_j\tilde{\bm{v}}_j^T.
</span></p></li>
<li class="fragment"><p>Thus, if a <span class="blue">multicollinearity</span> exists, then it appears as a principal component with very small variance, i.e.&nbsp;a small <span class="math inline">d_j^2</span>. Its removal therefore drastically <span class="orange">reduces</span> the <span class="orange">variance</span> of <span class="math inline">\hat{\beta}_\text{pcr}</span>.</p></li>
<li class="fragment"><p>Furthermore, the predicted values can be expressed as <span class="math display">
\hat{\bm{y}} = \sum_{j=1}^k \tilde{\bm{z}}_j \hat{\gamma}_j = \sum_{j=1}^k \tilde{\bm{z}}_j  \frac{\tilde{\bm{z}}_j^T\bm{y}}{\tilde{\bm{z}}_j^T\tilde{\bm{z}}_j} = \sum_{j=1}^k \textcolor{darkblue}{d_j} \tilde{\bm{u}}_j  \frac{\textcolor{darkblue}{d_j}}{\textcolor{darkblue}{d_j^2}} \frac{\tilde{\bm{u}}_j^T\bm{y}}{\textcolor{red}{\tilde{\bm{u}}_j^T\tilde{\bm{u}}_j}} = \sum_{j=1}^k \tilde{\bm{u}}_j \tilde{\bm{u}}_j^T \bm{y}.
</span></p></li>
<li class="fragment"><p>The columns of <span class="math inline">\bm{U}</span>, namely the vectors <span class="math inline">\tilde{\bm{u}}_j</span> are the <span class="blue">normalized principal components</span>.</p></li>
<li class="fragment"><p>Hence, we are shrinking the predictions towards the main <span class="orange">principal directions</span>.</p></li>
</ul>
</div>
</section>
<section id="shrinkage-effect-of-principal-components-iii" class="slide level2 center">
<h2>Shrinkage effect of principal components III</h2>

<img data-src="un_C_files/figure-revealjs/unnamed-chunk-17-1.png" width="1350" class="r-stretch quarto-figure-center"></section>
<section id="extension-to-generalized-linear-models-1" class="slide level2 center">
<h2>Extension to generalized linear models</h2>
</section>
<section id="summary-and-next-steps" class="slide level2 center">
<h2>Summary and next steps</h2>
<ul>
<li><p>We have see so far two “discrete” methods: best subset selection and PCR</p></li>
<li><p>Best subset selection perform variable selection, whereas PCR reduces the variance of the coefficients.</p></li>
</ul>
<table>
<colgroup>
<col style="width: 17%">
<col style="width: 47%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><span class="orange">Shrinkage</span></th>
<th><span class="orange">Variable selection</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="blue">Discrete</span></td>
<td>Principal component regression</td>
<td>Best subset selection, stepwise</td>
</tr>
<tr class="even">
<td><span class="blue">Continuous</span></td>
<td>Ridge regression</td>
<td>Relaxed Lasso</td>
</tr>
</tbody>
</table>
</section></section>
<section>
<section id="ridge-regression" class="title-slide slide level1 center">
<h1>Ridge regression</h1>
<!-- ## Ridge regression -->
<!-- ```{r} -->
<!-- df_ridge <- function(lambda, X) { -->
<!--   X_tilde <- scale(X, TRUE, FALSE) -->
<!--   d2 <- eigen(crossprod(X_tilde))$values -->
<!--   sum(d2 / (d2 + lambda)) -->
<!-- } -->
<!-- df_ridge <- Vectorize(df_ridge, vectorize.args = "lambda") -->
<!-- ``` -->
<!-- ```{r} -->
<!-- library(glmnet) -->
<!-- my_ridge <- function(X, y, lambda){ -->
<!--   n <- nrow(X) -->
<!--   p <- ncol(X) -->
<!--   y_mean <- mean(y) -->
<!--   y <- y - y_mean -->
<!--   X_mean <- colMeans(X) -->
<!--   X <- X - rep(1,n) %*% t(X_mean) -->
<!--   X_scale <- sqrt( diag( (1/n) * crossprod(X) ) ) -->
<!--   X <- X %*% diag( 1 / X_scale ) -->
<!--   beta_scaled <- solve(crossprod(X) + lambda*diag(rep(1,p)), t(X) %*% y)  -->
<!--   beta <- diag( 1 / X_scale ) %*% beta_scaled -->
<!--   beta0 <- y_mean - X_mean %*% beta -->
<!--   return(c(beta0, beta)) -->
<!-- } -->
<!-- l = 1 -->
<!-- my_ridge(X,y,lambda = l) -->
<!-- coef(glmnet(X, y, alpha=0, lambda = l/n, thresh = 1e-20)) -->
<!-- y_std <- scale(y, center=TRUE, scale=sd(y)*sqrt((n-1)/n) )[,] -->
<!-- my_ridge(X,y_std,lambda = l) -->
<!-- coef(glmnet(X, y_std, alpha=0, lambda = l/n, thresh = 1e-20)) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- #  -->
<!-- #  -->
<!-- # lambda <- 100 -->
<!-- # XX <- X[,-1] #scale(X[, -1], TRUE, scale = apply(X[, -1], 2, function(x)  sqrt(mean(x^2) - mean(x)^2))) -->
<!-- # yy <- y #(y - mean(y)) / sqrt(mean(y^2) - mean(y)^2) -->
<!-- #  -->
<!-- # cv_ridge_fit <- cv.glmnet(XX, yy, family = "gaussian", standardize = FALSE, lambda = exp(seq(-10, 12, length = 500)), -->
<!-- #                     alpha = 0, thresh = 1e-16) -->
<!-- # plot(cv_ridge_fit) -->
<!-- #  -->
<!-- # c(solve((crossprod(XX) + lambda * diag(p-1)), crossprod(XX, yy))) -->
<!-- # c(coef(fit_ridge)[-1, ]) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- # plot(log(cv_ridge_fit$lambda), cv_ridge_fit$cvm, type = "l") -->
<!-- # plot(1 + df_ridge(nrow(X[, -1]) * cv_ridge_fit$lambda, X[, -1]),  -->
<!-- #      cv_ridge_fit$cvm, type = "b", xlab = "Model complexity", ylab = "MSE") -->
<!-- # lines(1 + df_ridge(nrow(X[, -1]) * cv_ridge_fit$lambda, X[, -1]),  -->
<!-- #       cv_ridge_fit$cvup, type = "b", xlab = "Model complexity", ylab = "MSE", lty = "dashed", col = "red") -->
<!-- #  -->
<!-- # 1 + df_ridge(nrow(X[, -1]) * cv_ridge_fit$lambda.min, X[, -1]) -->
<!-- # 1 + df_ridge(nrow(X[, -1]) * cv_ridge_fit$lambda.1se, X[, -1]) -->
<!-- #  -->
<!-- # ridge_fit <- cv_ridge_fit$glmnet.fit -->
<!-- # coef(ridge_fit) -->
<!-- # plot(ridge_fit, , label = TRUE) -->
<!-- ``` -->
<!-- # Lasso, LARS, and elastic-net -->
<!-- ## Lasso -->
<!-- ::: columns -->
<!-- ::: {.column width="25%"} -->
<!-- ![](img/lasso.png) -->
<!-- ::: -->
<!-- ::: {.column width="75%"} -->
<!-- -   asdasd -->
<!-- ::: -->
<!-- ::: -->
<!-- ## Lasso -->
<!-- ```{r} -->
<!-- library(lars) -->
<!-- lambda <- 100 -->
<!-- XX <- scale(X[, -1], TRUE, scale = apply(X[, -1], 2, function(x)  sqrt(mean(x^2) - mean(x)^2))) -->
<!-- yy <- y#(y - mean(y)) / sqrt(mean(y^2) - mean(y)^2) -->
<!-- cv_lars <- cv.lars(x = XX, y = yy, K = 10, type = "lasso", mode = "step") -->
<!-- fit_lars <- lars(x = XX, y = yy, type ="lasso", normalize = FALSE) -->
<!-- cv_lasso_fit <- cv.glmnet(XX, yy, standardize = FALSE,  -->
<!--                           family = "gaussian", alpha = 1, nfolds = 10, -->
<!--                           lambda = 1 / n * fit_lars$lambda, thresh = 1e-16) -->
<!-- plot(cv_lasso_fit) -->
<!-- lasso_fit <- cv_lasso_fit$glmnet.fit -->
<!-- lambda_sel <- 4 -->
<!-- round(coef(fit_lars)[lambda_sel, ], 5) -->
<!-- round(coef(lasso_fit, mode = "lambda")[-1, lambda_sel], 5) -->
<!-- ``` -->
<!-- ## Summary of the estimated coefficients -->
<!-- ::: {style="font-size: 70%;"} -->
<!-- ```{r} -->
<!-- library(DT) -->
<!-- tab <- data.frame(OLS = rep(0, p), best_subset = rep(0, p), PCR = rep(0, p)) -->
<!-- rownames(tab) <- colnames(sum_best$which) -->
<!-- tab$OLS <- coef(lm(lpsa ~ ., data = prostate_train)) -->
<!-- tab$best_subset <- c(coef(lm(lpsa ~ lcavol + lweight, data = prostate_train)), rep(0, 6)) -->
<!-- # Principal components regression (PCR) -->
<!-- fit_pcr <- pcr(lpsa ~ ., data = prostate_train, center = TRUE, scale = FALSE) -->
<!-- beta <- c(coef(fit_pcr, 3)) -->
<!-- beta <- c(mean(prostate_train$lpsa) - colMeans(X[, -1]) %*% beta, beta) -->
<!-- tab$PCR <- beta -->

<!-- datatable(tab, colnames = c("OLS", "Best subset", "PCR"), options = list( -->
<!--   pageLength = 9, -->
<!--   dom = "t")) %>% -->
<!--   formatRound(columns = 1:3, digits = 3) %>% -->
<!--   formatStyle( -->
<!--     columns = 0, fontWeight = "bold" -->
<!--   ) %>% -->
<!--   formatStyle( -->
<!--     columns = 1:3, -->
<!--     backgroundColor = styleInterval(0, c("#FED8B1", "#DBE9FA")) -->
<!--   ) %>% -->
<!--   formatStyle( -->
<!--     columns = 1:3, -->
<!--     backgroundColor = styleEqual(0, c("white")) -->
<!--   ) -->
<!-- ``` -->
<!-- ::: -->
</section>
<section id="references" class="slide level2 center">
<h2>References</h2>

<img src="img/logoB.png" class="slide-logo r-stretch"><div class="footer footer-default">
<p><a href="https://tommasorigon.github.io/datamining">Home page</a></p>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="un_C_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="un_C_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="un_C_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="un_C_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="un_C_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="un_C_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="un_C_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="un_C_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="un_C_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>