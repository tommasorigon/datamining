---
title: "Shrinkage and variable selection"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
execute:
  cache: true
filters: 
  - remove-pause.lua
format:
  revealjs:
    df-print: tibble
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_C_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 200
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
    highlight-style: github
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 200
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_C.qmd", output = "../code/un_C.R", documentation = 0)
styler:::style_file("../code/un_C.R")
```

::: columns
::: {.column width="30%"}
![](img/cowboy.jpg)
:::

::: {.column width="70%"}
- This unit will cover the following [topics]{.orange}:

    -   Best subset regression
    -   Principal component regression
    -   Ridge regression
    -   Lasso, LARS, elastic-net

-   The common themes are called [variable selection]{.blue} and
    [shrinkage estimation]{.orange}.

-   The issue we face is the presence of a high number $p$ of covariates
    that are [potentially irrelevant]{.blue}.

-   This problem is quite challenging when the [ratio]{.blue} $p / n$ is
    [large]{.blue}.

-   In the [extreme case]{.orange} $p > n$, is there any hope of fitting a
    meaningful model?
:::
:::

# A biostatistical motivation

## The `prostate` dataset

-   The `prostate` cancer data investigates the relationship between the
    prostate-specific [antigen]{.orange} and a number of clinical
    measures in men about to receive a prostatectomy.

. . .

-   This
    [dataset](https://hastie.su.domains/ElemStatLearn/datasets/prostate.data)
    has been used in the [original paper]{.orange} by Tibshirani (1996)
    to present the lasso. A description is given in [Section
    3.2.1]{.blue} of HTF (2009).

. . .

-   We want to [predict]{.blue} the logarithm of a [prostate-specific
    antigen]{.orange} (`lpsa`) as a function of:

    -   logarithm of the cancer volume (`lcavol`);
    -   logarithm of the prostate weight (`lweight`);
    -   age each man (`age`);
    -   logarithm of the benign prostatic hyperplasia amount (`lbph`);
    -   seminal vesicle invasion (`svi`), a binary variable;
    -   logarithm of the capsular penetration (`lcp`);
    -   Gleason score (`gleason`), an ordered categorical variable;
    -   Percentage of Gleason scores $4$ and $5$ (`pgg45`).

## A `glimpse` of the `prostate` dataset

-   Summarizing, there are in total $8$ [variables]{.orange} that can be
    used to predict the antigen `lpsa`.

-   We [centered]{.orange} and [standardized]{.blue} all the covariates
    before the training/test split.

-   There are $n = 67$ observations in the [training]{.orange} set and
    $30$ in the [test]{.blue} set.

. . .

::: panel-tabset
## Original dataset

```{r}
#| message: false
rm(list = ls())
library(tidyverse)
prostate <- read.table("../data/prostate_data.txt")
glimpse(prostate)
```

## Standardized dataset

```{r}
# Standardize the predictors, as in Tibshirani (1996)
which_vars <- which(colnames(prostate) %in% c("lpsa", "train"))
prostate[, -which_vars] <- apply(prostate[, -which_vars], 2, function(x) (x - mean(x)) / sd(x))

# Split in training and test
prostate_train <- filter(prostate, train) %>% select(-train)
prostate_test <- filter(prostate, train == FALSE) %>% select(-train)

glimpse(prostate)
```
:::

<!-- -   The variable `train` splits the data into a training and test set, -->

<!--     as in the textbook. -->

## Correlation matrix of `prostate`

```{r}
#| fig-width: 15
#| fig-height: 7
#| fig-align: center
library(ggcorrplot)
corr <- cor(subset(prostate_train, select = -lpsa)) # Remove the outcome lpsa
ggcorrplot(corr,
  hc.order = FALSE,
  outline.col = "white",
  ggtheme = ggplot2::theme_bw,
  colors = c("#fc7d0b", "white", "#1170aa")
)
```

## The regression framework

::: incremental
-   In this unit, we will assume that the response variables $Y_i$
    (`lpsa`) are obtained as $$
    Y_i = f(\bm{x}_i) + \epsilon_i, \qquad
    $$ where $\epsilon_i$ are [iid]{.orange} random variables with
    $\mathbb{E}(\epsilon_i) = 0$ and
    $\text{var}(\epsilon_i) = \sigma^2$.

-   Unless specifically stated, we will [not]{.orange} assume the
    [Gaussianity]{.orange} of the errors $\epsilon_i$ nor make any
    specific assumption about $f(\bm{x})$, which could be
    [non-linear]{.blue}.

-   In practice, we [approximate]{.orange} the true $f(\bm{x})$ using a
    [linear model]{.blue}, e.g., by considering the following function$$
        f(\bm{x}_i; \beta_0, \beta) = \beta_0+ \beta_1 x_{i1} + \cdots + \beta_p x_{ip} =\beta_0 + \bm{x}_i^T\beta,
        $$ in which the regression coefficients must be estimated.

-   In this unit, the [intercept]{.blue} $\beta_0$ will often play a
    special role therefore we use a slightly different notation
    compared to [Unit A](unit_A.html).
:::

## The variable selection problem

-   Including a lot of covariates in the model is not necessarily a
    good thing!

. . .

-   Indeed, some variables are likely to be [irrelevant]{.blue}:

    -   they might be [correlated]{.orange} with other covariates and
        therefore [redundant]{.orange};
    -   they could be uncorrelated with the response `lpsa`.

. . .

-   If we use all the $p = 8$ available covariates, the estimated
    $f(\bm{x}; \hat{\beta_0}, \hat{\beta})$ might have a [high
    variance]{.orange}, without an important gain in terms of bias, i.e., a
    [large mean squared error]{.blue}.

-   We are looking for a [simpler model]{.orange} having, hopefully, a
    lower mean squared error.

. . .

-   These considerations are particularly relevant in cases in which
    $p > n$!

## A naïve approach: (ab)using p-values

::: {style="font-size: 75%;"}
```{r}
#| output: false
tab <- data.frame(broom::tidy(lm(lpsa ~ ., data = prostate_train), conf.int = FALSE))
rownames(tab) <- tab[, 1]
tab <- t(as.matrix(tab[, -1]))
knitr::kable(tab, digits = 2)
```

|          | `(Intercept)`| `lcavol`| `lweight`|   `age`| `lbph`|  `svi`|   `lcp`| `gleason`| `pgg45`|
|:---------|-----------:|------:|-------:|-----:|----:|----:|-----:|-------:|-----:|
|estimate  |        2.46|   0.68|    0.26| -0.14| 0.21| 0.31| -0.29|   -0.02|  0.27|
|std.error |        0.09|   0.13|    0.10|  0.10| 0.10| 0.12|  0.15|    0.15|  0.15|
|statistic |       27.60|   5.37|    2.75| -1.40| 2.06| 2.47| -1.87|   -0.15|  1.74|
|p.value   |        0.00|   0.00|    0.01|  0.17| 0.04| 0.02|  0.07|    0.88|  0.09|
:::

. . .

-   It is common practice to use the [p-values]{.orange} to perform
    [model selection]{.blue} in a stepwise fashion.

-   However, what if the true $f(\bm{x})$ were not linear?

-   In many data mining problems, a [linear model]{.blue} is simply an
    approximation of the unknown $f(\bm{x})$ and hypothesis testing
    procedures are ill-posed.

. . .

-   Even if the true function were linear, using p-values would [not be
    a good idea]{.orange}, at least if done without appropriate
    [multiplicity corrections]{.blue}.

-   The above p-values are meant to be used in the context of a single
    hypothesis testing problem, [not]{.orange} to make [iterative
    choices]{.orange}.

## The predictive culture

::: columns
::: {.column width="25%"}
![George E. P. Box](img/box.jpg){fig-align="left"}
:::

::: {.column width="75%"}
-   "*All models are approximations. Essentially, all models are wrong,
    but some are useful*."

    [George E. P. Box]{.grey}

-   If the [focus]{.blue} is on [prediction]{.blue}, we do not
    necessarily care about selecting the "true" set of parameters.

-   In many data mining problems, the focus is on [minimizing]{.orange}
    the [prediction errors]{.orange}.

-   Hence, often we may [accept some bias]{.blue} (i.e., we use a
    "wrong" but useful model), if this leads to a [reduction in
    variance]{.orange}.
:::
:::

## Overview of this unit

-   In this unit, we will discuss two "discrete" methods:

    -   Best subset selection and its greedy approximations: forward /
        backward regression;
    -   Principal components regression (PCR).

-   Best subset selection perform [variable selection]{.orange}, whereas
    principal components regression [reduces the variance]{.orange} of
    the coefficients.

-   These "discrete" methods can be seen as the naïve counterpart of
    more advanced and [continuous]{.blue} ideas that are presented in
    the second part of the Unit.

. . .

|                     | [Shrinkage]{.orange}           | [Variable selection]{.orange}   |
|------------------|-------------------------------|-----------------------|
| [Discrete]{.blue}   | Principal component regression | Best subset selection, stepwise |
| [Continuous]{.blue} | Ridge regression               | Relaxed Lasso                   |

. . .

-   Finally, the [lasso]{.blue} and the [elastic-net]{.orange} perform
    both shrinkage and variable selection.

## Overview of the final results

|               | Least squares | Best subset |    PCR |  Ridge | Lasso |
|:--------------|--------------:|------------:|-------:|-------:|------:|
| `(Intercept)` |         2.465 |       2.477 |  2.455 |  2.467 | 2.468 |
| `lcavol`      |         0.680 |       0.740 |  0.287 |  0.588 | 0.532 |
| `lweight`     |         0.263 |       0.316 |  0.339 |  0.258 | 0.169 |
| `age`         |        -0.141 |           . |  0.056 | -0.113 |     . |
| `lbph`        |         0.210 |           . |  0.102 |  0.201 |     . |
| `svi`         |         0.305 |           . |  0.261 |  0.283 | 0.092 |
| `lcp`         |        -0.288 |           . |  0.219 |  -0.172|     . |
| `gleason`     |        -0.021 |           . | -0.016 |  0.010 |     . |
| `pgg45`       |         0.267 |           . |  0.062 |  0.204 |     . |

# Best subset selection

## Best subset selection

-   Let us return to our [variable selection problem]{.blue}.

-   In principle, we could perform an [exhaustive search]{.orange}
    considering all the $2^p$ possible models and then selecting the one
    having the best out-of-sample predictive performance.

. . .

::: callout-note
#### Best subset procedure

1.  Let $\mathcal{M}_0$ be the [null model]{.blue}, which contains no
    predictors, i.e. set $\hat{y}_i = \hat{\beta}_0 = \bar{y}$.

2.  For $k =1,\dots,p$, do:

    i.  Estimate [all]{.orange} the $\binom{p}{k}$ models that contain
        exactly $k$ covariates;

    ii. Identify the "best" model with $k$ covariates having the
        smallest $\text{MSE}_{k, \text{train}}$; call it
        $\mathcal{M}_k$.
:::

-   A model with more variables has lower [training]{.orange} error,
    namely
    $\text{MSE}_{k + 1, \text{train}} \le \text{MSE}_{k, \text{train}}$
    by construction. Hence, the optimal subset size $k$ must be chosen
    e.g., via [cross-validation]{.blue}.

## Step 1. and 2. of best subset selection

```{r}
# Here, I compute some basic quantities
X <- model.matrix(lpsa ~ ., data = prostate_train)[, -1]
y <- prostate_train$lpsa
n <- nrow(X)
p <- ncol(X) # This does not include the intercept
```

```{r}
library(leaps)
fit_best <- regsubsets(lpsa ~ ., data = prostate_train, method = "exhaustive", nbest = 40, nvmax = p)
sum_best <- summary(fit_best)
sum_best$p <- rowSums(sum_best$which) - 1 # Does not include the intercept here
```

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

library(ggplot2)
library(ggthemes)
data_best_subset <- data.frame(p = sum_best$p, MSE = sum_best$rss / n)
data_best_subset <- reshape2::melt(data_best_subset, id = c("p"))
colnames(data_best_subset) <- c("p", "MSE", "value")

data_best_subset2 <- data.frame(p = unique(sum_best$p), MSE = tapply(sum_best$rss / n, sum_best$p, min))

ggplot(data = data_best_subset, aes(x = p, y = value)) +
  geom_point() +
  theme_light() +
  theme(legend.position = "top") +
  geom_line(data = data_best_subset2, aes(x = p, y = MSE), col = "#fc7d0b") +
  geom_point(data = data_best_subset2, aes(x = p, y = MSE), col = "#fc7d0b", size = 1.5) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of covariates") +
  ylab("Mean squared error (training)")
```

## The "best" models $\mathcal{M}_1,\dots, \mathcal{M}_p$

-   The output of the [best subset selection]{.orange}, on the training
    set is:

```{r}
summary(regsubsets(lpsa ~ ., data = prostate_train, method = "exhaustive", nbest = 1, nvmax = p))$outmat
```

::: incremental
-   The above table means that the best model with $k = 1$ uses the
    variable `lcavol`, whereas when $k = 2$ the selected variables are
    `lcavol` and `lweight`, and so on.

-   Note that, in general, these models are [not]{.orange} necessarily
    [nested]{.orange}, i.e. a variable selected at step $k$ is not
    necessarily included at step $k +1$. Here they are, but it is a
    coincidence.
:::

. . .

-   What is the [optimal subset size]{.orange} $k$ in terms of
    out-of-sample mean squared error?

## The wrong way of doing cross-validation

::: incremental
-   Consider a regression problem with a [large number of
    predictors]{.orange} (relative to $n$) such as the `prostate`
    dataset.

-   A typical strategy for analysis might be as follows:

    1.  Screen the predictors: find a subset of "good" predictors that
        show a reasonably strong correlation with the response;

    2.  Using this subset of predictors (e.g., `lcavol`, `lweight` and
        `svi`), build a regression model;

    3.  Use cross-validation to estimate the prediction error of the
        model of the step 2.

-   Is this a correct application of cross-validation?

-   If your reaction was "[this is absolutely wrong!]{.orange}", it
    means you correctly understood the principles of cross-validation.

-   If you thought this was an ok-ish idea, you may want to read [Section
    7.10.2]{.blue} of HTF (2009), called "the wrong way of doing
    cross-validation".
:::

## Step 3. of best subset selection via cross-validation

```{r}
library(rsample)

set.seed(123)
cv_fold <- vfold_cv(prostate_train, v = 10)
resid_subs <- matrix(0, n, p + 1)

for (k in 1:10) {
  # Estimation of the null model
  fit_null <- lm(lpsa ~ 1, data = analysis(cv_fold$splits[[k]]))
  # Best subset using branch and bound
  fit <- regsubsets(lpsa ~ ., data = analysis(cv_fold$splits[[k]]), method = "exhaustive", nbest = 1, nvmax = p)
  sum <- summary(fit)

  # Hold-out quantities
  X_k <- as.matrix(cbind(1, assessment(cv_fold$splits[[k]]) %>% select(-lpsa)))
  y_k <- assessment(cv_fold$splits[[k]])$lpsa

  # MSE of the null model
  resid_subs[complement(cv_fold$splits[[k]]), 1] <- y_k - predict(fit_null, assessment(cv_fold$splits[[k]]))

  # MSE of the best models for different values of p
  for (j in 2:(p + 1)) {
    y_hat <- X_k[, sum$which[j - 1, ]] %*% coef(fit, j - 1)
    resid_subs[complement(cv_fold$splits[[k]]), j] <- y_k - y_hat
  }
}
```

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

data_cv <- data.frame(
  p = 0:p,
  MSE = apply(resid_subs^2, 2, mean),
  SE = apply(resid_subs^2, 2, function(x) sd(x) / sqrt(n))
)

se_rule <- data_cv$MSE[which.min(data_cv$MSE)] + data_cv$SE[which.min(data_cv$MSE)]
p_optimal <- which(data_cv$MSE < se_rule)[1] - 1

ggplot(data = data_cv, aes(x = p, y = MSE)) +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymax = MSE + SE, ymin = MSE - SE)) +
  geom_hline(yintercept = se_rule, linetype = "dotted") +
  geom_vline(xintercept = p_optimal, linetype = "dotted") +
  theme_light() +
  scale_x_continuous(breaks = 0:9) +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of covariates") +
  ylab("Mean squared error (10-fold cv)")
```

-   By applying the "1 standard error rule", we select $k = 2$, i.e.
    `lcavol` and `lweight`.

## Comments and computations

::: incremental
-   The correct way of doing cross-validation requires that the [best
    subset selection]{.blue} is performed on [every fold]{.orange},
    possibly obtaining different "best" models with the same size.

-   Best subset selection is conceptually appealing, but it has a [major
    limitation]{.orange}. There are $$
    \sum_{k=1}^p \binom{p}{k} = 2^p
    $$ models to consider, which is [computationally
    prohibitive]{.orange}!

-   There exist algorithms (i.e. [leaps and bounds]{.blue}) that make
    this feasible for $p \approx 30$.

-   Recently, [Bertsimas et al.,
    2016](https://projecteuclid.org/journals/annals-of-statistics/volume-44/issue-2/Best-subset-selection-via-a-modern-optimization-lens/10.1214/15-AOS1388.full)
    proposed the usage of a mixed integer optimization formulation,
    allowing $p$ to be in the order of hundreds.

-   Despite these advances, this problem remains [computationally very
    expensive]{.orange}. See also the recent paper [Hastie et al.
    (2020)](https://projecteuclid.org/journals/statistical-science/volume-35/issue-4/Best-Subset-Forward-Stepwise-or-Lasso-Analysis-and-Recommendations-Based/10.1214/19-STS733.full)
    for additional considerations and comparisons.
:::

## Forward regression

-   Forward regression is [greedy approximation]{.orange} of best subset
    selection that produces a sequence of [nested]{.blue} models. It is
    computationally feasible and can be applied when $p > n$.

. . .

::: callout-note
#### Forward regression

1.  Let $\mathcal{M}_0$ be the [null model]{.blue}, which contains no
    predictors, i.e. set $\hat{y}_i = \hat{\beta}_0 = \bar{y}$.

2.  For $k = 0,\dots, \min(n - 1, p - 1)$, do:

    i.  Consider the $p − k$ models that augment the predictors in
        $\mathcal{M}_k$ with [one additional covariate]{.orange}.

    ii. Identify the "best" model among the above $p - k$ competitors
        having the smallest $\text{MSE}_{k, \text{train}}$ and call it
        $\mathcal{M}_k$.
:::

. . .

-   It can be shown that the identification of the [optimal new
    predictor]{.blue} can be efficiently computed e.g. using the [QR
    decomposition]{.orange}.

## Backward regression

-   When $p < n$, an alternative greedy approach is [backward
    regression]{.orange}, which also produces a sequence of
    [nested]{.blue} models.

. . .

::: callout-note
#### Backward regression

1.  Let $\mathcal{M}_p$ be the [full model]{.blue}, which contains all
    the predictors.

2.  For $k = p, p - 1,\dots, 1$, do:

    i.  Consider the $k$ models that contain [all but one]{.orange} of
        the predictors in $\mathcal{M}_k$, for a total of $k − 1$
        predictors.

    ii. Identify the "best" model $\mathcal{M}_k$ among these $k$ models
        having the smallest $\text{MSE}_{k, \text{train}}$.
:::

. . .

-   It can be shown that the [dropped predictor]{.blue} is the one with
    the lowest absolute $Z$-score or, equivalently, the [highest
    p-value]{.orange}.

## Forward, backward and best subset

```{r}
fit_forward <- regsubsets(lpsa ~ ., data = prostate_train, method = "forward", nbest = 1, nvmax = p)
sum_forward <- summary(fit_forward)
fit_backward <- regsubsets(lpsa ~ ., data = prostate_train, method = "backward", nbest = 1, nvmax = p)
sum_backward <- summary(fit_backward)
```

```{r}
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: center

# Organization of the results for graphical purposes
data_stepwise <- data.frame(
  p = c(1:p, 1:p, 1:p), MSE = c(
    sum_forward$rss,
    sum_backward$rss,
    tapply(sum_best$rss, sum_best$p, min)
  ) / n,
  Stepwise = rep(c("Forward", "Backward", "Best subset"), each = p)
)
data_stepwise <- reshape2::melt(data_stepwise, id = c("p", "Stepwise"))
colnames(data_stepwise) <- c("p", "Stepwise", "MSE", "value")

ggplot(data = data_stepwise, aes(x = p, y = value, col = Stepwise)) +
  geom_line() +
  geom_point() +
  facet_grid(. ~ Stepwise) +
  theme_light() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = 0:9) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of covariates") +
  ylab("MSE (training)")
```

-   In the `prostate` dataset, forward, backward and best subset
    selection all gave precisely the [same path of solutions]{.orange} on
    the full training set.

## Pros and cons of subset selection strategies

::: callout-tip
#### Pros

-   Best subset selection is appealing because of its [conceptual
    simplicity]{.blue}.

-   Best subset and forward regression can be used when computations are not problematic, even when $p > n$.
:::

. . .

::: callout-warning
#### Cons

-   Subset strategies tend to select models that are "[too
    simple]{.orange}", especially in presence of correlated variables.

-   Despite the recent advances, when $p$ is large best subset selection
    is [computationally unfeasible]{.orange}.

-   Leaps and bounds computational strategies can not be easily
    generalized to GLMs.
:::

# Principal components regression

## Data compression

::: columns
::: {.column width="30%"}
![](img/cowboy_pixel.jpg)
:::

::: {.column width="70%"}
-   At this point, we established that [many covariates = many
    problems]{.orange}.

-   Instead of selecting the "best" variables, let us consider a
    different perspective.

-   We consider a [compressed]{.blue} version of the covariates that has
    smaller dimension $k$ but retains most information.

-   Intuitively, we want to [reduce the variance]{.orange} by finding a
    good compression without sacrificing too much bias.

-   The main statistical tool, unsurprisingly, will be the celebrated
    [principal components analysis]{.blue} (PCA).

-   We will compress the covariate information $\bm{X}$ using a smaller
    set of variables $\bm{Z}$, i.e. the principal components.
:::
:::

## The intercept term

::: incremental
-   In principal component regression and other related methods
    (ridge, lasso, and elastic-net), we do [not]{.orange} wish to
    [compress]{.orange} the [intercept]{.orange} term $\beta_0$. We would like to "remove it".

-   Let us consider a [reparametrization]{.blue} of the linear model, in
    which $\alpha = \beta_0 + \bar{\bm{x}}^T\beta$. This is equivalent
    to a linear model with [centered predictors]{.orange}: $$
    \begin{aligned}
    f(\bm{x}_i; \alpha, \beta) & = \beta_0 + \bm{x}_i^T\beta = \alpha - \bar{\bm{x}}^T\beta + \bm{x}_i^T\beta  = \alpha + (\bm{x}_i -\bar{\bm{x}})^T\beta. \\
    \end{aligned}
    $$

-   The estimates for $(\alpha, \beta)$ can be now computed separately
    and [in two steps]{.orange}.

-   The [estimate]{.orange} of the [intercept]{.orange} with centered
    predictors is $\hat{\alpha} = \bar{y}$. In fact: $$
    \hat{\alpha} = \arg\min_{\alpha \in \mathbb{R}}\sum_{i=1}^n\{y_i - \alpha - (\bm{x}_i -\bar{\bm{x}})^T\beta\}^2 = \frac{1}{n}\sum_{i=1}^n\{y_i - (\bm{x}_i -\bar{\bm{x}})^T\beta\} = \frac{1}{n}\sum_{i=1}^ny_i.
    $$

-   Then, the [estimate of $\beta$]{.blue} can be obtained considering a
    linear model [without intercept]{.orange}: $$
        f(\bm{x}_i; \beta) = (\bm{x}_i -\bar{\bm{x}})^T\beta,
    $$ employed to predict the [centered responses]{.blue}
    $y_i - \bar{y}$.
:::

## Centering the predictors I

-   In principal components regression, we replace [original
    data]{.orange} $Y_i = f(\bm{x}_i) + \epsilon_i$ with their
    [centered]{.blue} version: $$
    x_{ij} - \bar{x}_j, \qquad y_i - \bar{y}, \qquad i=1,\dots,n; \ \ j=1,\dots,p.
    $$

. . .

-   In the end, we will make predictions in the [original scale]{.blue},
    which requires a simple [final adjustment]{.orange}. One need
    to compute the intercept term $$
    \hat{\beta}_0 = \bar{y} - \bar{\bm{x}}\hat{\beta},
    $$ and then compute the predictions via the formula
    $\hat{\beta}_0 + \bm{x}_i^T\hat{\beta} = \hat{\alpha} + \bm{x}_{i}^T\hat{\beta}$.

. . .

-   [Remark]{.orange}. The centering operation is a mathematical trick
    that facilitate the exposition but is unconsequential from an
    estimation point of view.

## Centering the predictors II

::: callout-note
#### Centering assumption

In principal components regression, we assume the data have been
previously [centered]{.orange}: $$
    \frac{1}{n}\sum_{i=1}^n y_{i} = 0, \qquad \frac{1}{n}\sum_{i=1}^nx_{ij} = 0, \qquad j=1,\dots,p.
    $$
:::

. . .

-   Using centered predictors means that we can focus on linear models
    [without intercept]{.blue}: $$
    f(\bm{x}_{i}; \beta) = x_{i1}\beta_1 + \cdots + x_{ip}\beta_p = \bm{x}_{i}^T\beta.
    $$

. . .

-   Under the centering assumption, the [covariance matrix]{.orange} of
    the data is simply $$
    S = \frac{1}{n} \bm{X}^T\bm{X}.
    $$

<!-- -   If in addition each variable has been [scaled]{.blue} by their -->

<!--     standard deviations, then $n^{-1} \bm{X}^T\bm{X}$ corresponds to the -->

<!--     [correlation]{.blue} matrix. -->

## Singular value decomposition (SVD)

-   Let $\bm{X}$ be a $n \times p$ matrix. Then, its full form [singular
    value decomposition]{.orange} is: $$
    \bm{X} = \bm{U} \bm{D} \bm{V}^T = \sum_{j=1}^m d_j \tilde{\bm{u}}_j \tilde{\bm{v}}_j^T,
    $$ with $m =\min\{n, p\}$ and where:

    -   the $n \times n$ matrix
        $\bm{U} = (\tilde{\bm{u}}_1, \dots, \tilde{\bm{u}}_n)$ is
        [orthogonal]{.orange}, namely:
        $\bm{U}^T \bm{U} = \bm{U}\bm{U}^T= I_n$;
    -   the $p \times p$ matrix
        $\bm{V} = (\tilde{\bm{v}}_1,\dots,\tilde{\bm{v}}_p)$ is
        [orthogonal]{.orange}, namely:
        $\bm{V}^T \bm{V} = \bm{V}\bm{V}^T= I_p$;
    -   the $n \times p$ matrix $\bm{D}$ has [diagonal]{.blue} entries
        $[\bm{D}]_{jj} = d_j$, for $j=1,\dots,m$, and zero entries
        elsewhere;

-   The real numbers $d_1 \ge d_2 \ge \cdots \ge d_m \ge 0$ are called
    [singular values]{.blue}.

-   If one or more $d_j = 0$, then the matrix $\bm{X}$ is singular.

## Principal component analysis I

::: incremental
-   Le us assume that $p < n$ and that $\text{rk}(\bm{X}) = p$,
    recalling that $\bm{X}$ is a [centered]{.orange} matrix.

-   Using SVD, the matrix $\bm{X}^T\bm{X}$ can be expressed as $$
    \bm{X}^T\bm{X} = (\bm{U} \bm{D} \bm{V}^T)^T \bm{U} \bm{D} \bm{V}^T = \bm{V} \bm{D}^T \textcolor{red}{\bm{U}^T \bm{U}} \bm{D} \bm{V}^T = \bm{V} \bm{\Delta}^2 \bm{V}^T,
    $$ where $\bm{\Delta}^2 = \bm{D}^T\bm{D}$ is a $p \times p$
    [diagonal]{.blue} matrix with entries $d_1^2,\dots,d_p^2$.

-   This equation is at the heart of [principal component
    analysis]{.blue} (PCA). Define the matrix $$
    \bm{Z} = \bm{X}\bm{V} = \bm{U}\bm{D},
    $$ whose columns $\tilde{\bm{z}}_1,\dots,\tilde{\bm{z}}_p$ are
    called [principal components]{.orange}.

-   The matrix $\bm{Z}$ is orthogonal, because
    $\bm{Z}^T\bm{Z} = \bm{D}^T\textcolor{red}{\bm{U}^T \bm{U}} \bm{D} = \bm{\Delta}^2$,
    which is diagonal.

-   Moreover, by definition the entries of $\bm{Z}$ are linear
    combination of the original variables: $$
    z_{ij} = x_{i1}v_{i1} + \cdots + x_{ip} v_{ip} = \bm{x}_{i}^T\tilde{\bm{v}}_j.
    $$ The columns $\tilde{\bm{v}}_1,\dots,\tilde{\bm{v}}_p$ of $\bm{V}$
    are sometimes called [loadings]{.blue}.
:::

## Principal component analysis II

::: incremental
-   Principal components form an orthogonal basis of $\bm{X}$, but they
    are not a "random" choice, and they do [not]{.orange} coincide with
    them [Gram-Schmidt]{.orange} basis of [Unit
    A](un_A.html#the-qr-decomposition-i).

-   Indeed, the [first principal component]{.blue} is the linear
    combination having [maximal variance]{.orange}: $$
    \tilde{\bm{v}}_1 = \arg\max_{\bm{v} \in \mathbb{R}^p} \text{var}(\bm{X}\bm{v})= \arg\max_{\bm{v} \in \mathbb{R}^p} \frac{1}{n} \bm{v}^T\bm{X}^T\bm{X} \bm{v}, \quad \text{ subject to } \quad \bm{v}^T \bm{v} = 1.
    $$

-   The [second principal component]{.blue} maximizes the variance under
    the additional constraint of being [orthogonal]{.orange} to the
    former. And so on.

-   The values $d_1^2 \ge d_2^2 \ge \dots \ge d_p^2 > 0$ are the
    [eigenvalues]{.orange} of $\bm{X}^T\bm{X}$ and correspond to the
    rescaled [variances]{.blue} of each principal component, that is
    $\text{var}(\tilde{\bm{z}}_j) = \tilde{\bm{z}}_j^T \tilde{\bm{z}}_j/n = d^2_j / n$.

-   Hence, the quantity $d_j^2 / \sum_{j'=1}^p d_{j'}^2$ measures the
    amount of total variance captured by principal components.
:::

## Principal component analysis: `prostate` data

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center
pr <- princomp(prostate_train[, -9], cor = FALSE)
ggplot(data = data.frame(p = 1:p, vars = pr$sdev^2 / sum(pr$sdev^2)), aes(x = p, xmin = p, xmax = p, y = vars, ymax = vars, ymin = 0)) +
  geom_pointrange() +
  theme_light() +
  scale_x_continuous(breaks = 1:9) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of principal components") +
  ylab("Fraction of explained variance")
```

## Principal components regression (PCR)

::: incremental
-   We use the first $k \le p$ [principal components]{.blue} to predict
    the responses $y_{i}$ via $$
    f(\bm{z}_i; \gamma) = \gamma_1 z_{i1} + \cdots + \gamma_kz_{ik}, \qquad i=1,\dots,n,
    $$

-   Because of orthogonality, the least squares solution is
    straightforward to compute: $$
    \hat{\gamma}_j = \frac{\tilde{\bm{z}}_j^T\bm{y}}{\tilde{\bm{z}}_j^T\tilde{\bm{z}}_j} = \frac{1}{d_j^2}\tilde{\bm{z}}_j^T\bm{y}, \qquad j=1,\dots,k.
    $$

-   The principal components are in [order of importance]{.orange} and
    effectively [compressing the information]{.blue} contained in
    $\bm{X}$ using only $k \le p$ variables.

-   When $k = p$, we are simply rotating the original matrix
    $\bm{X} = \bm{Z}\bm{V}$, i.e. performing [no compression]{.orange}.
    The predicted values coincide with OLS.

-   The number $k$ is a [complexity parameter]{.blue} which should be
    chosen via information criteria or cross-validation.
:::

## Selection of $k$: cross-validation

```{r}
#| message: false
library(pls)
resid_pcr <- matrix(0, n, p)

for (k in 1:10) {
  # Estimation of the null model
  fit_null <- lm(lpsa ~ 1, data = analysis(cv_fold$splits[[k]]))
  # Hold-out dataset
  y_k <- assessment(cv_fold$splits[[k]])$lpsa
  # MSE of the null model
  resid_pcr[complement(cv_fold$splits[[k]]), 1] <- y_k - predict(fit_null, assessment(cv_fold$splits[[k]]))
  # Fitting PCR (all the components at once)
  fit_pcr <- pcr(lpsa ~ ., data = analysis(cv_fold$splits[[k]]), center = TRUE, scale = FALSE)

  for (j in 2:p) {
    # Predictions
    y_hat <- predict(fit_pcr, newdata = assessment(cv_fold$splits[[k]]))[, , j - 1]
    # MSE of the best models for different values of p
    resid_pcr[complement(cv_fold$splits[[k]]), j] <- y_k - y_hat
  }
}

data_cv <- data.frame(
  p = 1:p,
  MSE = apply(resid_pcr^2, 2, mean),
  SE = apply(resid_pcr^2, 2, function(x) sd(x) / sqrt(n))
)

se_rule <- data_cv$MSE[which.min(data_cv$MSE)] + data_cv$SE[which.min(data_cv$MSE)]
p_optimal <- which(data_cv$MSE < se_rule)[1]
```

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

ggplot(data = data_cv, aes(x = p, y = MSE)) +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymax = MSE + SE, ymin = MSE - SE)) +
  geom_hline(yintercept = se_rule, linetype = "dotted") +
  geom_vline(xintercept = p_optimal, linetype = "dotted") +
  theme_light() +
  scale_x_continuous(breaks = 1:9) +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of principal components") +
  ylab("Mean squared error (10-fold cv)")
```

## Shrinkage effect of principal components I

::: incremental
-   A closer look at the PCR solution reveals some interesting aspects.
    Recall that: $$
    \tilde{\bm{z}}_j = \bm{X}\tilde{\bm{v}}_j = d_j \tilde{\bm{u}}_j,  \qquad j=1,\dots,p.
    $$

-   The [predicted values]{.orange} for the [centered responses]{.blue}
    $\bm{y}$ of the PCR with $k$ components are: $$
    \sum_{j=1}^k \tilde{\bm{z}}_j \hat{\gamma}_j = \bm{X} \sum_{j=1}^k \tilde{\bm{v}}_j \hat{\gamma}_j = \bm{X}\hat{\beta}_\text{pcr}, \qquad \text{ where } \qquad \hat{\beta}_\text{pcr} = \sum_{j=1}^k \tilde{\bm{v}}_j \hat{\gamma}_j.
    $$

-   This representation highlights two important aspects:

    -   It is possible to express the PCR solution in the original
        scale, for better [interpretability]{.orange};
    -   The vector $\hat{\beta}_\text{pcr}$ is a [constrained
        solution]{.blue}, being a combination of $k \le p$ coefficients,
        therefore [reducing]{.orange} the [complexity]{.orange} of the
        model and [shrinking]{.blue} the coefficients.

-   When $k = 1$, then the $\hat{\beta}_\text{pcr}$ estimate coincide
    with the scaled loading vector
    $\hat{\beta}_\text{pcr} = \hat{\gamma}_1 \tilde{\bm{v}}_1$;

-   When $k = p$ then the $\hat{\beta}_\text{pcr}$ coincides with
    [ordinary least squares]{.blue} (see Exercises).
:::

## Shrinkage effect of principal components II {#shrinkage-effect-of-principal-components-ii}

::: incremental
-   The [variance]{.orange} of $\hat{\beta}_\text{pcr}$, assuming iid
    errors $\epsilon_i$ in the [original data]{.blue}, is: $$
    \text{var}(\hat{\beta}_\text{pcr}) = \sigma^2\sum_{j=1}^k \frac{1}{d_j^2} \tilde{\bm{v}}_j\tilde{\bm{v}}_j^T.
    $$

-   Thus, if a [multicollinearity]{.blue} exists, then it appears as a
    principal component with very small variance, i.e., a small $d_j^2$.
    Its removal, therefore, drastically [reduces]{.orange} the
    [variance]{.orange} of $\hat{\beta}_\text{pcr}$.

-   Furthermore, the predicted values for the [centered data]{.orange}
    can be expressed as $$
    \bm{X}\hat{\beta}_\text{pcr} = \sum_{j=1}^k \tilde{\bm{z}}_j \hat{\gamma}_j = \sum_{j=1}^k \tilde{\bm{z}}_j  \frac{\tilde{\bm{z}}_j^T\bm{y}}{\tilde{\bm{z}}_j^T\tilde{\bm{z}}_j} = \sum_{j=1}^k \textcolor{darkblue}{d_j} \tilde{\bm{u}}_j  \frac{\textcolor{darkblue}{d_j}}{\textcolor{darkblue}{d_j^2}} \frac{\tilde{\bm{u}}_j^T\bm{y}}{\textcolor{red}{\tilde{\bm{u}}_j^T\tilde{\bm{u}}_j}} = \sum_{j=1}^k \tilde{\bm{u}}_j \tilde{\bm{u}}_j^T \bm{y}.
    $$

-   The columns of $\bm{U}$, namely the vectors $\tilde{\bm{u}}_j$ are
    the [normalized principal components]{.blue}.

-   Hence, we are shrinking the predictions towards the main [principal
    directions]{.orange}.
:::

## Shrinkage effect of principal components III

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center
fit_pcr <- pcr(lpsa ~ ., data = prostate_train, center = TRUE, scale = FALSE)

data_pcr <- reshape2::melt(coef(fit_pcr, 1:8))
colnames(data_pcr) <- c("Covariate", "lpsa", "Components", "value")
data_pcr$Components <- as.numeric(data_pcr$Components)
data_pcr <- rbind(data_pcr, data.frame(Covariate = data_pcr$Covariate[data_pcr$Components == 1], lpsa = NA, Components = 0, value = 0))
data_pcr$Covariate <- as.factor(as.character(data_pcr$Covariate))
ggplot(data = data_pcr, aes(x = Components, y = value, col = Covariate)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.position = "top") +
  scale_x_continuous(breaks = 0:9) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of principal components") +
  ylab("Regression coefficients")
```

## Pros and cons of PCR

::: callout-tip
#### Pros

- Principal components are a natural tool to [reduce the complexity]{.blue} of the data, especially in the presence of [highly correlated]{.orange} variables.

- If you transform back the coefficients, there is a [clean
    interpretation]{.blue} of the impact of the covariates on the
    response.

-   Principal components might be interesting in their own right, as
    they describe the [dependence structure]{.blue} among covariates.
:::

. . .

::: callout-warning
#### Cons

-   [All the variables]{.orange} are used for predictions, which could
    be computationally demanding.

-   The shrinkage effect on the regression coefficients is somewhat
    indirect and not smooth.
:::

# Ridge regression

## Shrinkage methods

::: columns
::: {.column width="30%"}
![](img/ridge.jpg){fig-align="left"}
:::

::: {.column width="70%"}


- Shrinkage methods are popular tools for handling the issue of multiple variables.

- Shrinkage [regularizes]{.blue} the estimates, [constraining]{.blue} the [size]{.blue} of the regression coefficients.

- This leads to [biased estimator]{.orange} with, hopefully, lower variance.

- As a byproduct, the induced regularization procedure enables estimation even when $p > n$.

- The first method that has been proposed is called [ridge regression]{.orange}. The lasso and the elastic-net are other examples. 

:::
:::


## The ridge regularization method

-   The ridge estimator is the most common [shrinkage method]{.blue} and
    is the [minimizer]{.orange} of $$
    \sum_{i=1}^n(y_i - \beta_0 -  \bm{x}_i^T\beta)^2 \qquad \text{subject to} \qquad \sum_{j=1}^p \beta_j^2 \le s.
    $$

. . .

-   When the [complexity parameter]{.blue} $s$ is small, the
    coefficients are explicitly [shrinked]{.orange}, i.e. biased,
    [towards zero]{.orange}.

-   On the other hand, if $s$ is large enough, then the ridge estimator
    coincides with ordinary least squares.

. . .

-   In ridge regression, the [variability]{.orange} of the estimator is
    explicitly [bounded]{.orange}, although this comes with some
    [bias]{.blue}. The parameter $s$ controls the bias-variance
    trade-off.

. . .

-   The intercept term $\beta_0$ is [not penalized]{.orange} because
    there are no strong reasons to believe that the mean of $y_i$ equals
    zero. However, as before, we want to "remove the intercept".

## Centering and scaling the predictors I

-   The ridge solutions are [not equivariant]{.orange} under [scalings
    of the input]{.orange}, so one normally [standardizes]{.blue} the
    input to have unit variance if they are not on the same scale.

. . .

-   Moreover, as for PCR, we can estimate the intercept using a
    [two-step procedure]{.orange}:
    -   The reparametrization $\alpha = \beta_0 + \bar{\bm{x}}^T\beta$
        is equivalent to centering the predictors;
    -   The estimate for the centered intercept is
        $\hat{\alpha} = \bar{y}$;
    -   The ridge estimate can be obtained by considering a model without
        intercept, using centered responses and predictors.

. . .

-   Hence, in ridge regression, we replace [original data]{.orange}
    $Y_i = f(\bm{x}_i) + \epsilon_i$ with their [standardized]{.blue}
    version: $$ 
    \frac{x_{ij} - \bar{x}_j}{s_j}, \qquad y_i - \bar{y}, \qquad i=1,\dots,n; \ \ j=1,\dots,p.
    $$ where $s_j^2 = n^{-1}\sum_{i=1}^n (x_{ij} - \bar{x}_j)^2$ is the
    [sample variance]{.blue}.

## Centering and scaling the predictors II

-   It is easy to show (see Exercises) that the [coefficients]{.orange}
    expressed in the [original scale]{.blue} are $$
    \hat{\beta}_0 = \bar{y} - \bar{\bm{x}}\hat{\beta}_\text{scaled-ridge}, \qquad \hat{\beta}_\text{scaled-ridge} = \text{diag}(1 / s_1,\dots, 1/s_p) \hat{\beta}_\text{ridge}.
    $$ Thus, the [predictions]{.orange} on the [original scale]{.blue}
    are
    $\hat{\beta}_0 + \bm{x}_i^T\hat{\beta}_\text{scaled-ridge} = \bar{y} + \bm{x}_{i}^T\hat{\beta}_\text{ridge}$.

. . .

::: callout-note
For ridge problems, we will assume the data have been previously
[standardized]{.orange}, namely $$
    \frac{1}{n}\sum_{i=1}^ny_{i} = 0, \qquad \frac{1}{n}\sum_{i=1}^nx_{ij} = 0, \qquad \frac{1}{n}\sum_{i=1}^n x_{i}^2 = 1\qquad j=1,\dots,p.
    $$
:::

. . .

-   We will say that the [ridge estimator]{.blue}
    $\hat{\beta}_\text{ridge}$ is the [minimizer]{.orange} of following
    system $$
    \sum_{i=1}^n(y_i - \bm{x}_{i}^T\beta)^2 \qquad \text{subject to} \qquad \sum_{j=1}^p \beta_j^2 \le s.
    $$

## Lagrange multipliers and ridge solution

-   The ridge regression problem can be [equivalently
    expressed]{.orange} in its [Lagrangian
    form](https://en.wikipedia.org/wiki/Karush–Kuhn–Tucker_conditions),
    which greatly facilitates computations. The ridge estimator
    $\hat{\beta}_\text{ridge}$ is the [minimizer]{.orange} of $$
    \sum_{i=1}^n(y_{i} - \bm{x}_{i}^T\beta)^2 + \lambda \sum_{j=1}^p\beta_j^2 = \underbrace{||\bm{y} - \bm{X}\beta||^2}_{\text{least squares}} + \underbrace{\lambda ||\beta||^2}_{\text{\textcolor{red}{ridge penalty}}},
    $$ where $\lambda > 0$ is a [complexity parameter]{.blue}
    controlling the [penalty]{.orange}. It holds that
    $s = ||\hat{\beta}_\text{ridge} ||^2$.

. . .

-   When $\lambda = 0$ then
    $\hat{\beta}_\text{ridge} = \hat{\beta}_\text{ols}$ whereas when
    $\lambda \rightarrow \infty$ we get $\hat{\beta}_\text{ridge} = 0$.

. . .

::: callout-note
#### Ridge regression estimator

For any $n\times p$ design matrix $\bm{X}$, not necessarily of
full-rank, the ridge estimator is $$
\hat{\beta}_\text{ridge} = (\bm{X}^T\bm{X} + \lambda I_p)^{-1}\bm{X}^T\bm{y}.
$$ Such an estimator [always exists]{.blue} and is [unique]{.orange}
(even when $p > n$).
:::

<!-- ## Mathematical details -->

<!-- ::: incremental -->

<!-- - The matrix $\bm{X}^T\bm{X} + \lambda I_p$ is [invertible]{.blue} for any $\lambda > 0$.  -->

<!-- - For any $\lambda >0$, the ridge solution is always at the [boundary]{.orange}, that is, is never an interior point. Indeed, if $\bm{X}$ is full rank, it can be shown that -->

<!-- $$ -->

<!-- ||\hat{\beta}_\text{ridge}||^2 \le ||\hat{\beta}||^2. -->

<!-- $$ -->

<!-- - The size of the spherical ridge parameter constraint [shrinks monotonously]{.orange} as $\lambda$ increases: -->

<!-- $$ -->

<!-- \frac{\partial}{\partial \lambda} ||\hat{\beta}_\text{ridge}||^2 < 0, -->

<!-- $$ -->

<!-- ::: -->

## The geometry of the ridge solution

```{r}
#| purl: false
#| warning: false

library(ggforce)

ellipses <- data.frame(
  x0 = rep(5, 4),
  y0 = rep(3, 4),
  a = seq(0.5, 2, length = 4),
  b = c(1, 2, 2.835, 3.82),
  angle = rep(2.1, 4)
)

points <- data.frame(
  x = c(5),
  y = c(3)
)

ggplot() +
  geom_circle(aes(x0 = 0, y0 = 0, r = 2), color = "#fc7d0b", fill = "#fc7d0b") +
  geom_ellipse(aes(x0 = x0, y0 = y0, a = a, b = b, angle = angle), color = "#1170aa", data = ellipses) +
  geom_point(aes(x = x, y = y), data = points, color = "#1170aa") +
  theme_minimal() +
  geom_vline(xintercept = 0, linetype = "dotted") +
  geom_hline(yintercept = 0, linetype = "dotted") +
  xlab(expression(beta[1])) +
  ylab(expression(beta[2])) +
  coord_fixed() +
  annotate("text", x = 5.4, y = 3.2, label = expression(hat(beta)[ols]), col = "#1170aa") +
  annotate("text", x = -1, y = 2.5, label = expression(beta[1]^2 + beta[2]^2 == s), col = "#fc7d0b") +
  annotate("text", x = 1.7, y = 4.8, label = expression(sum((y[i] - x[i]^T * beta)^2, i == 1, n) == c), col = "#1170aa")
```

## The ridge path

```{r}
my_ridge <- function(X, y, lambda_tilde, standardize = TRUE) {
  n <- nrow(X)
  p <- ncol(X)
  lambda <- lambda_tilde * n
  y_mean <- mean(y) # Center the response
  y <- y - y_mean

  # Centering the covariates
  X_mean <- colMeans(X)
  if (standardize) {
    X_scale <- apply(X, 2, function(x) sqrt(mean(x^2) - mean(x)^2))
  } else {
    X_scale <- rep(1, p)
  }

  X <- scale(X, center = X_mean, scale = X_scale)

  # Ridge solution (scaled data)
  beta_scaled <- solve(crossprod(X) + lambda * diag(rep(1, p)), crossprod(X, y))

  # Transform back to the original scale
  beta <- beta_scaled / X_scale
  # Compute the intercept
  beta0 <- y_mean - X_mean %*% beta
  return(c(beta0, beta))
}
```

```{r}
df_ridge <- function(lambda_tilde, X, standardize = TRUE) {
  n <- nrow(X)
  lambda <- lambda_tilde * n
  # Rescale the predictors
  X_mean <- colMeans(X)
  if (standardize) {
    X_scale <- apply(X, 2, function(x) sqrt(mean(x^2) - mean(x)^2))
  } else {
    X_scale <- rep(1, p)
  }
  X <- scale(X, center = X_mean, scale = X_scale)
  d2 <- eigen(crossprod(X))$values
  1 + sum(d2 / (d2 + lambda))
}

df_ridge <- Vectorize(df_ridge, vectorize.args = "lambda_tilde")
```

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center
lambda_tilde_seq <- exp(seq(from = -4, to = 5, length = 50))
data_ridge <- cbind(lambda_tilde_seq, matrix(0, length(lambda_tilde_seq), p))

mse_ridge <- data.frame(lambda_tilde_seq, Cp = NA, mse = NA, df = df_ridge(lambda_tilde_seq, X, standardize = TRUE), sigma2 = NA)

for (i in 1:length(lambda_tilde_seq)) {
  data_ridge[i, -1] <- my_ridge(X, y, lambda_tilde = lambda_tilde_seq[i], standardize = TRUE)[-1]
  mse_ridge$mse[i] <- mean((y - mean(y) - X %*% data_ridge[i, -1])^2)
  mse_ridge$sigma2[i] <- mse_ridge$mse[i] * n / (n - mse_ridge$df[i])
  mse_ridge$Cp[i] <- mse_ridge$mse[i] + 2 * mse_ridge$sigma2[i] / n * mse_ridge$df[i]
}

colnames(data_ridge)[-1] <- colnames(X)
data_ridge <- tidyr::gather(data.frame(data_ridge), lambda, value, lcavol:pgg45)
colnames(data_ridge) <- c("lambda_tilde", "Covariate", "value")

ggplot(data = data_ridge, aes(x = lambda_tilde, y = value, col = Covariate)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.position = "top") +
  scale_x_log10() +
  scale_color_tableau(palette = "Color Blind") +
  xlab(expression(lambda / n)) +
  ylab("Coefficients (original scale)")
```

## Comments on the ridge path

-   The values of $\lambda$ are in somewhat [arbitrary scale]{.orange}.
    The ridge penalty has a concrete effect starting from
    $\lambda / n > 0.1$ or so.

-   The variable `lcavol` is arguably the most important, followed by
    `lweight` and `svi`, which are those receiving less shrinkage
    compared to the others.

. . .

-   The coefficient of `age`, `gleason`, and `lcp`, is negative at the
    beginning and then becomes positive for large values of $\lambda$.

-   This indicate that their negative value in $\hat{\beta}_\text{ols}$
    was probably a consequence of their [correlation]{.orange} with
    other variables.

. . .

-   There is an interesting similarity between this plot and the one of
    principal component regression... is it a coincidence?

## Shrinkage effect of ridge regression I

-   Considering, once again, the [singular value
    decomposition]{.orange}, we get: $$\begin{aligned}
    \bm{X}\hat{\beta}_\text{ridge} &= \bm{X}(\bm{X}^T\bm{X} + \lambda I_p)^{-1}\bm{X}^T\bm{y} \\
    & = \bm{U}\bm{D} \bm{V}^T[\bm{V}(\bm{D}^T\bm{D} + \lambda I_p)\bm{V}^T]^{-1}(\bm{U}\bm{D}\bm{V})^T\bm{y}  \\
    & = \bm{U}\bm{D} \textcolor{red}{\bm{V}^T\bm{V}}(\bm{D}^T\bm{D} + \lambda I_p)^{-1} \textcolor{red}{\bm{V}^T \bm{V}} \bm{D}^T \bm{U} ^T\bm{y} \\
    & = \bm{U}\bm{D}(\bm{D}^T\bm{D} + \lambda I_p)^{-1}\bm{D}^T\bm{U}^T\bm{y} \\
    & = \bm{H}_\text{ridge}\bm{y} = \sum_{j=1}^p \tilde{\bm{u}}_j \frac{d_j^2}{d_j^2 + \lambda}\tilde{\bm{u}}_j^T \bm{y},
    \end{aligned}
    $$ where
    $\bm{H}_\text{ridge} = \bm{X}(\bm{X}^T\bm{X} + \lambda I_p)^{-1}\bm{X}^T$
    is the so-called [hat matrix]{.blue} of ridge regression.

. . .

-   This means that ridge regression [shrinks]{.blue} the [principal
    directions]{.blue} by an amount that depends on the
    [eigenvalues]{.orange} $d_j^2$.

-   In other words, it [smoothly reduces]{.orange} the impact of the
    [redundant]{.orange} information.

## Shrinkage effect of ridge regression II

-   A sharp [connection]{.blue} with [principal components
    regression]{.blue} is therefore revealed.

-   Compare the previous formula for $\bm{X}\hat{\beta}_\text{ridge}$
    with [the one we previously
    obtained](#shrinkage-effect-of-principal-components-ii) for
    $\bm{X}\hat{\beta}_\text{pcr}$.

. . .

-   More explicitly, for [ridge regression]{.orange} we will have that
    $$
    \hat{\beta}_\text{ridge} = \bm{V}\text{diag}\left(\frac{d_1}{d_1^2 + \lambda}, \dots, \frac{d_p}{d_p^2 + \lambda}\right)\bm{U}^T\bm{y}.
    $$ whereas for [principal components regression]{.blue} with $k$
    components we get $$
    \hat{\beta}_\text{pcr} = \bm{V}\text{diag}\left(\frac{1}{d_1}, \dots, \frac{1}{d_k}, 0, \dots, 0\right)\bm{U}^T\bm{y}.
    $$

. . .

-   Both operate on the singular values, but where principal component
    regression [thresholds]{.blue} the singular values, [ridge
    regression]{.orange} shrinks them.

## Bias-variance trade-off

-   The ridge regression [add some bias]{.orange} to the estimates, but
    it [reduces]{.blue} their [variance]{.blue}.

. . .

-   The [variance]{.orange} of $\hat{\beta}_\text{ridge}$, assuming iid
    errors $\epsilon_i$ in the [original scale]{.blue} with variance
    $\sigma^2$, results: $$
    \text{var}(\hat{\beta}_\text{ridge}) = \sigma^2\sum_{j=1}^p \frac{d_j^2}{(d_j^2 + \lambda)^2} \tilde{\bm{v}}_j\tilde{\bm{v}}_j^T,
    $$ whose diagonal elements are always smaller than those of
    $\text{var}(\hat{\beta}_\text{ols})$.

. . .

-   The above formula highlights that ridge will be very
    [effective]{.blue} in presence highly [correlated variables]{.blue},
    as they will be "shrunk" away by the penalty.

-   What typically happens is that such a reduction in variance
    [compensate]{.orange} the increase in bias, especially when $p$ is
    large relative to $n$.

## ☠️ - A historical perspective I

::: incremental
-   The ridge regression estimator was originally proposed by Hoerl and
    Kennard (1970) with a quite different motivation in mind.

-   In linear models, the estimate of $\beta$ is obtained by solving the
    [normal equations]{.blue} $$
    (\bm{X}^T\bm{X})\beta = \bm{X}^T\bm{y},
    $$ which could be [ill-conditioned]{.orange}.

-   In other words, the [condition number]{.blue} $$
    \kappa(\bm{X}^T\bm{X}) = \frac{d_1^2}{d_p^2},
    $$ might be very large, leading to [numerical
    inaccuracies]{.orange}, since the matrix $\bm{X}^T\bm{X}$ is
    [numerically singular]{.blue} and therefore not invertible in
    practice.
:::

## ☠️ - A historical perspective II

::: incremental
-   Ridge provides a [remedy]{.blue} for [ill-conditioning]{.orange}, by
    adding a "ridge" to the diagonal of $\bm{X}^T\bm{X}$, obtaining the
    modified normal equations $$
    (\bm{X}^T\bm{X} + \lambda I_p)\beta = \bm{X}^T\bm{y}.
    $$

-   The [condition number]{.orange} of the modified
    $(\bm{X}^T\bm{X} + \lambda I_p)$ matrix becomes $$
    \kappa(\bm{X}^T\bm{X} + \lambda I_p) = \frac{\lambda + d_1^2}{\lambda + d_p^2}.
    $$

-   Notice that even if $d_p = 0$, i.e. the matrix $\bm{X}$ is
    [singular]{.blue}, then the condition number will be finite as long as
    $\lambda > 0$.

-   This technique is known as [Tikhonov regularization]{.orange}, after
    the Russian mathematician Andrey Tikhonov.
:::

## ☠️ - A historical perspective III

![](img/ridge_paper.png){fig-align="center" width="40%"}

-   Figure 1 of the [original paper]{.orange} by Hoerl and Kennard
    (1970), displaying the bias-variance [trade-off]{.blue}.

## On the choice of $\lambda$

-   The [penalty parameter]{.orange} $\lambda$ determines the amount of
    bias and variance of $\hat{\beta}_\text{ridge}$ and therefore it
    must be carefully [estimated]{.blue}.

. . .

-   [Minimizing]{.blue} the loss
    $||\bm{y} - \bm{X}\hat{\beta}_\text{ridge}||^2$ over $\lambda$ is a
    [bad idea]{.orange}, because it would always lead to $\lambda = 0$,
    corresponding to
    $\hat{\beta}_\text{ridge} = \hat{\beta}_\text{ols}$.

-   Indeed, $\lambda$ is a [complexity]{.blue} parameter and, like the
    number of covariates, should be selected using [information
    criteria]{.orange} or training/test and [cross-validation]{.orange}.

. . .

-   Suppose we wish to use an [information criteria]{.blue} such as the
    AIC or BIC, of the form $$
      \text{IC}(p) = -2 \ell(\hat{\beta}_\text{ridge}) + \text{penalty}(\text{``degrees of freedom"}).
      $$ We need a careful definition of [degrees of freedom]{.blue}
    that is appropriate in this context.

-   The current definition of degrees of freedom, i.e., the number of
    [non-zero coefficients]{.orange}, is [not appropriate]{.orange} for
    ridge regression because it would be equal to $p$ for any value of
    $\lambda$.

## Effective degrees of freedom I

-   Let us recall that the original data are
    $Y_i = f(\bm{x}_i) + \epsilon_i$ and that the [optimism]{.orange}
    for a generic estimator $\hat{f}(\bm{x})$ is defined as the
    following average of covariances $$
    \text{Opt} = \frac{2}{n}\sum_{i=1}^n\text{cov}(Y_i, \hat{f}(\bm{x}_i)),
    $$ which is equal to $\text{Opt}_\text{ols} = (2\sigma^2p)/ n$ in
    [ordinary least squares]{.blue}.

. . .

::: callout-note
#### Effective degrees of freedom

Let $\hat{f}(\bm{x})$ be an estimate for the regression function
$f(\bm{x})$ based on the data $Y_1,\dots,Y_n$. The [effective degrees of
freedom]{.orange} are defined as $$
\text{df} = \frac{1}{\sigma^2}\sum_{i=1}^n\text{cov}(Y_i, \hat{f}(\bm{x}_i)).
$$
:::

## Effective degrees of freedom II

::: incremental
-   The effective degrees of freedom of [ordinary least squares]{.blue}
    and [principal component regression]{.blue} are $$
    \text{df}_\text{ols} = p + 1, \qquad \text{df}_\text{pcr} = k + 1,
    $$ where the additional term corresponds to the [intercept]{.orange}.

-   After some algebra, one finds that the effective degrees of freedom
    of [ridge regression]{.orange} are $$
    \text{df}_\text{ridge} = 1 + \text{tr}(\bm{H}_\text{ridge}) = 1 + \sum_{j=1}^p \frac{d_j^2}{d_j^2 + \lambda}.
    $$

-   Using the above result, we can [plug-in]{.orange}
    $\text{df}_\text{ridge}$ into the formula of the [$C_p$ of
    Mallows]{.blue}: $$
      \widehat{\mathrm{ErrF}} = \frac{1}{n}\sum_{i=1}^n(y_i - \bm{x}_i^T\hat{\beta}_\text{scaled-ridge})^2 + \frac{2 \hat{\sigma}^2}{n} \text{df}_\text{ridge}.
      $$ where the residual variance is estimated as
    $\hat{\sigma}^2 = (n - \text{df}_\text{ridge})^{-1} \sum_{i=1}^n(y_i - \bm{x}_i^T\hat{\beta}_\text{scaled-ridge})^2$.
:::

## Effective degrees of freedom III

::: columns
::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4
df_min_cp <- mse_ridge$df[which.min(mse_ridge$Cp)]
lambda_tilde_min_cp <- mse_ridge$lambda_tilde_seq[which.min(mse_ridge$Cp)]

ggplot(data = mse_ridge, aes(x = df, y = Cp)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = df_min_cp, linetype = "dotted") +
  theme_light() +
  scale_x_continuous(breaks = 1:9) +
  xlab("Effective degrees of freedom (df)") +
  ylab(expression(C[p]))
```
:::

::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4
ggplot(data = mse_ridge, aes(x = lambda_tilde_seq, y = Cp)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = lambda_tilde_min_cp, linetype = "dotted") +
  theme_light() +
  scale_x_log10() +
  xlab(expression(lambda / n)) +
  ylab(expression(C[p]))
```
:::
:::

## Cross-validation for ridge regression I

- Training/test strategies and [cross-validation]{.blue} are also
    valid tools for selecting $\lambda$.

-   Most statistical software packages use a slightly [different
    parametrization]{.orange} for $\lambda$, as they minimize $$
    \textcolor{red}{\frac{1}{n}}\sum_{i=1}^n(y_{i} - \bm{x}_{i}^T\beta)^2 + \tilde{\lambda} \sum_{j=1}^p\beta_j^2,
    $$ where the penalty parameter $\tilde{\lambda} = \lambda / n$.

. . .

-   This parametrization does not alter the estimate of
    $\hat{\beta}_\text{ridge}$ but is more amenable for
    [cross-validation]{.blue} as the values of $\tilde{\lambda}$ can be
    compared across datasets with [different sample sizes]{.orange}.

. . .

-   Different R packages have [different defaults]{.blue} about other
    aspects too.

-   For instance, the R package `glmnet` uses $\tilde{\lambda}$ and also
    [standardizes the response]{.blue} $\bm{y}$ and then [transforms
    back]{.orange} the estimated coefficients into the [original
    scale]{.orange}.

## Cross-validation for ridge regression II

```{r}
#| message: false
resid_ridge <- matrix(0, n, length(lambda_tilde_seq))

for (k in 1:10) {
  # Hold-out dataset
  X_test_k <- as.matrix(subset(assessment(cv_fold$splits[[k]]), select = -c(lpsa)))
  y_test_k <- assessment(cv_fold$splits[[k]])$lpsa

  X_train_k <- as.matrix(subset(analysis(cv_fold$splits[[k]]), select = -c(lpsa)))
  y_train_k <- analysis(cv_fold$splits[[k]])$lpsa

  for (j in 1:length(lambda_tilde_seq)) {
    # Estimates
    beta_hat <- my_ridge(X_train_k, y_train_k, lambda_tilde = lambda_tilde_seq[j], standardize = TRUE)
    # Predictions
    y_hat <- cbind(1, X_test_k) %*% beta_hat
    # MSE of the best models for different values of lambda
    resid_ridge[complement(cv_fold$splits[[k]]), j] <- y_test_k - y_hat
  }
}
```

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

data_cv <- data.frame(
  lambda_tilde = lambda_tilde_seq,
  df = df_ridge(lambda_tilde_seq, X, standardize = TRUE),
  MSE = apply(resid_ridge^2, 2, mean),
  SE = apply(resid_ridge^2, 2, function(x) sd(x) / sqrt(n))
)

se_rule <- data_cv$MSE[which.min(data_cv$MSE)] + data_cv$SE[which.min(data_cv$MSE)]
lambda_tilde_min_cv <- lambda_tilde_seq[tail(which(data_cv$MSE < se_rule), 1)]
df_min_cv <- data_cv$df[tail(which(data_cv$MSE < se_rule), 1)]

ggplot(data = data_cv, aes(x = lambda_tilde, y = MSE)) +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymax = MSE + SE, ymin = MSE - SE)) +
  scale_x_log10() +
  geom_hline(yintercept = se_rule, linetype = "dotted") +
  geom_vline(xintercept = lambda_tilde_min_cv, linetype = "dotted") +
  theme_light() +
  xlab(expression(tilde(lambda))) +
  ylab("Mean squared error (10-fold cv)")
```

```{r}
#| include: false
#| execute: false
# Double checks
library(glmnet)
y_std <- prostate_train$lpsa / sqrt(mean(prostate_train$lpsa^2) - mean(prostate_train$lpsa)^2)

as.numeric(coef(glmnet(X, y_std, family = "gaussian", standardize = TRUE, alpha = 0, thresh = 1e-20, lambda = lambda_tilde_min_cv)))

my_ridge(X, y_std, standardize = TRUE, lambda_tilde = lambda_tilde_min_cv)
```

<!-- ## Leave-one-out cross-validation -->

<!-- -   Ridge regression is, like ordinary least squares, a [linear smoother]{.blue}, namely: -->

<!-- $$ -->

<!-- \bm{X}\hat{\beta}_\text{ridge} = \bm{X}(\bm{X}^T\bm{X} + \lambda I_p)^{-1}\bm{X}^T\bm{y} = \bm{H}_\text{ridge}\bm{y}, -->

<!-- $$ -->

<!-- meaning that a [computational shortcut]{.orange} for LOO-CV can be exploited. -->

<!-- . . . -->

<!-- ::: callout-note -->

<!-- #### LOO-CV (Ridge regression) -->

<!-- Let $\hat{y}_{-i, \text{ridge}} = \bm{x}_i^T\hat{\beta}_{-i, \text{ridge}}$ be the leave-one-out -->

<!-- predictions of a [linear model]{.blue} with a [ridge penalty]{.orange} and let $h_{i, \text{ridge}} = [\bm{H}_\text{ridge}]_{ii}$ and -->

<!-- $\hat{y}_i$ be the leverages and the predictions. Then: $$ -->

<!-- y_i - \hat{y}_{-i, \text{ridge}} = \frac{y_i - \hat{y}_{i, \text{ridge}}}{1 - h_{i, \text{ridge}}}, \qquad i=1,\dots,n, -->

<!-- $$ -->

<!-- recalling that $\bar{y} = 0$. Therefore, the leave-one-out mean squared error is -->

<!-- $$\widehat{\mathrm{Err}} = \frac{1}{n}\sum_{i=1}^n \left(\frac{y_i - \hat{y}_{i, \text{ridge}}}{1 - h_{i, \text{ridge}}}\right)^2.$$ -->

<!-- ::: -->

## The ridge estimate

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center
ggplot(data = data_ridge, aes(x = df_ridge(lambda_tilde, X), y = value, col = Covariate)) +
  geom_point() +
  geom_line() +
  theme_light() +
  geom_vline(xintercept = df_min_cv, linetype = "dashed") +
  geom_vline(xintercept = df_min_cp, linetype = "dotted") +
  scale_x_continuous(breaks = 1:9) +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Effective degrees of freedom (df)") +
  ylab("Regression coefficients")
```

## Further properties of ridge regression

::: incremental

-   Ridge regression has a transparent [Bayesian
    interpretation]{.orange}, since the penalty can be interpreted as a
    Gaussian prior on $\beta$.
    
- If two variables are identical copies $\tilde{\bm{x}}_j = \tilde{\bm{x}}_\ell$, so are the corresponding ridge coefficients $\hat{\beta}_{j,\text{ridge}} = \hat{\beta}_{\ell,\text{ridge}}$. 

- Adding $p$ [fake observations]{.orange} all equal to $0$ to the response and then fitting ordinary least squares leads to the ridge estimator. This procedure is called [data augmentation]{.blue}.

- A computationally convenient formula for [LOO cross-validation]{.blue} is available, which requires the model to be estimated only once, as in least squares.

- In the [$p > n$ case]{.orange} there are specific [computational strategies]{.orange} that can be employed; see Section 18.3.5 of Hastie, Tibshirani and Friedman (2011).

:::

## Pros and cons of ridge regression

::: callout-tip
#### Pros

-   Ridge regression trades some [bias]{.orange} in exchange of a [lower variance]{.blue}, often resulting in more accurate predictions.

-   The ridge solution [always exists]{.orange} and is [unique]{.blue},
    even when $p > n$ or in presence of perfect [collinearity]{.orange}.

-   For fixed values of $\lambda$, [efficient computations]{.blue} are
    available using QR and Cholesky decompositions.
<!-- -   At the end of this unit, we will describe a [general optimization -->
<!--     strategy]{.blue} that recovers the entire "ridge path," i.e., the -->
<!--     estimate $\hat{\beta}_\text{ridge}$ for several values of $\lambda$. -->
:::

. . .

::: callout-warning
#### Cons

-   In ridge regression, [all variables]{.orange} are used. This is in
    contrast with best subset selection.
:::

# The lasso

## Looking for sparsity

::: columns
::: {.column width="25%"}
![Robert Tibshirani](img/tibshirani.jpg){fig-align="left"}
:::

::: {.column width="75%"}
-   Signal [sparsity]{.orange} is the assumption that only a small
    number of predictors have an effect, i.e., $$
    \beta_j = 0, \qquad \text{for most} \qquad j \in \{1,\dots,p\}.$$
-   In this case we would like our estimator $\hat{\beta}$ to be
    [sparse]{.blue}, meaning that $\hat{\beta}_j = 0$ for many
    $j \in \{1,\dots,p\}$.
-   Sparse estimators are desirable because:
    -   perform [variable selection]{.blue} and improve the
        [interpretability]{.orange} of the results;
    -   Speed up the [computations]{.blue} of the predictions because
        fewer variables are needed.
-   Best subset selection is sparse (but computationally unfeasible),
    the ridge estimator is not.
:::
:::

## The [l]{.orange}east [a]{.orange}bsolute [s]{.orange}election and [s]{.orange}hrinkage [o]{.orange}perator

-   The [lasso]{.blue} appeared in the highly influential paper of Tibshirani (1996). It is a method that performs both [shrinkage]{.blue} and [variable selection]{.orange}

. . .

-   The lasso estimator is the [minimizer]{.orange} of the following
    system $$
    \sum_{i=1}^n(y_i - \beta_0- \bm{x}_i^T\beta)^2 \qquad \text{subject to} \qquad \sum_{j=1}^p |\beta_j| \le s.
    $$ therefore when the [complexity parameter]{.blue} $s$ is small,
    the coefficients of $\hat{\beta}_\text{lasso}$ are
    [shrinked]{.orange} and when $s$ is large enough
    $\hat{\beta}_\text{lasso} = \hat{\beta}_\text{ols}$, as in ridge
    regression.

. . .

-   The lasso is [deceptively similar]{.orange} to ridge. However, the
    change from a quadratic penalty to an absolute value has a crucial
    [sparsity]{.blue} implication.

. . .

-   The intercept term $\beta_0$ is [not penalized]{.orange}, as for
    ridge, because we can remove it by centering the predictors.

<!-- ## Centering and scaling the predictors I -->

<!-- -   The lasso solutions are [not equivariant]{.orange} under [scalings -->

<!--     of the input]{.orange}, so one normally [standardizes]{.blue} the -->

<!--     input to have unit variance, if they are not in the same scale. -->

<!-- . . . -->

<!-- - Moreover, as for PCR, we can estimate the intercept using a [two-step procedure]{.orange}: -->

<!--   - The reparametrization $\alpha = \beta_0 + \bar{\bm{x}}^T\beta$ is equivalent to centering the predictors; -->

<!--   - The estimate for the centered intercept is $\hat{\alpha} = \bar{y}$; -->

<!--   - The ridge estimate can be obtained considering a model without intercept, using centered responses and predictors.  -->

<!-- . . . -->

<!-- - Hence, the [original data]{.orange} $Y_i = f(\bm{x}_i) + \epsilon_i$ are [standardized]{.blue}, using the "[*]{.blue}" to denote it: $$  -->

<!-- x_{ij} = \frac{x_{ij} - \bar{x}_j}{s_j}, \qquad y_{i} = y_i - \bar{y}, \qquad i=1,\dots,n; \ \ j=1,\dots,p. -->

<!-- $$ -->

<!-- where $s_j^2 = n^{-1}\sum_{i=1}^n (x_{ij} - \bar{x}_j)^2$ is the [sample variance]{.blue}. -->

## Centering and scaling the predictors

-   Thus, as for ridge regression, we will center and scale predictors
    and response.

-   It is easy to show that the [coefficients]{.orange} expressed in the
    [original scale]{.blue} are $$
    \hat{\beta}_0 = \bar{y} - \bar{\bm{x}}\hat{\beta}_\text{lasso}, \qquad \hat{\beta}_\text{scaled-lasso} = \text{diag}(1 / s_1,\dots, 1/s_p) \hat{\beta}_\text{lasso}.
    $$ Thus, the [predictions]{.orange} on the [original scale]{.blue}
    are
    $\hat{\beta}_0 + \bm{x}_i^T\hat{\beta}_\text{scaled-lasso} = \bar{y} + \bm{x}_{i}^T\hat{\beta}_\text{lasso}$.

. . .

::: callout-note
For lasso problems, we will assume the data have been previously
[standardized]{.orange}, namely $$
    \frac{1}{n}\sum_{i=1}^ny_{i} = 0, \qquad \frac{1}{n}\sum_{i=1}^nx_{ij} = 0, \qquad \frac{1}{n}\sum_{i=1}^n x_{i}^2 = 1\qquad j=1,\dots,p.
    $$
:::

. . .

-   We will say that the [lasso estimator]{.blue}
    $\hat{\beta}_\text{lasso}$ is the [minimizer]{.orange} of following
    system $$
    \sum_{i=1}^n(y_i - \bm{x}_{i}^T\beta)^2 \qquad \text{subject to} \qquad \sum_{j=1}^p| \beta_j| \le s.
    $$

## Lagrange multipliers and lasso solution

-   The lasso problem can be [equivalently expressed]{.blue} in its
    Lagrangian form, which is more amenable for computations.

-   Having removed the intercept, the lasso estimator
    $\hat{\beta}_\text{lasso}$ is the [minimizer]{.orange} of $$
      \underbrace{\textcolor{darkblue}{\frac{1}{2 n}}\sum_{i=1}^n(y_{i} - \bm{x}_i^T\beta)^2}_{\text{least squares}} + \underbrace{\lambda \sum_{j=1}^p|\beta_j|}_{\text{\textcolor{red}{lasso penalty}}}
      $$ where $\lambda > 0$ is a [complexity parameter]{.blue}
    controlling the [penalty]{.orange}.

. . .

-   When $\lambda = 0$ the penalty term disappears and
    $\hat{\beta}_\text{lasso} = \hat{\beta}_\text{ols}$. On the other
    hand, there exists a finite value of $\lambda_0 < \infty$
    such that $\hat{\beta}_\text{lasso} = 0$.

-   For any intermediate value $0 < \lambda < \lambda_0$ we get
    a combination of [shrinked]{.orange} but positive coefficients, and
    a set of coefficients whose value is [exactly zero]{.blue}.

. . .

-   Unfortunately, there is [no closed-form expression]{.orange} for the
    lasso solution.

## The geometry of the lasso solution

```{r}
#| purl: false
#| warning: false

geom_rhombus <- function(color = "black", fill = NA, ...) {
  x <- seq(-1, 1, length.out = 100)
  ymax <- 1 - abs(x)
  ymin <- -1 + abs(x)
  annotate("ribbon", x = x, ymin = ymin, ymax = ymax, color = color, fill = fill, ...)
}

ellipses <- data.frame(
  x0 = rep(3, 4),
  y0 = rep(3, 4),
  a = seq(0.5, 2, length = 4),
  b = c(1, 2, 2.835, 3.82),
  angle = rep(1.92, 4)
)

points <- data.frame(
  x = c(3),
  y = c(3)
)

ggplot() +
  geom_rhombus(color = "#fc7d0b", fill = "#fc7d0b") +
  geom_ellipse(aes(x0 = x0, y0 = y0, a = a, b = b, angle = angle), color = "#1170aa", data = ellipses) +
  geom_point(aes(x = x, y = y), data = points, color = "#1170aa") +
  theme_minimal() +
  geom_vline(xintercept = 0, linetype = "dotted") +
  geom_hline(yintercept = 0, linetype = "dotted") +
  xlab(expression(beta[1])) +
  ylab(expression(beta[2])) +
  coord_fixed() +
  annotate("text", x = 2.9, y = 2.9, label = expression(hat(beta)[ols]), col = "#1170aa") +
  annotate("text", x = 2, y = -0.5, label = expression(paste("|", beta[1], "|") + paste("|", beta[2], "|") == s), col = "#fc7d0b") +
  annotate("text", x = 5, y = 0.7, label = expression(sum((y[i] - x[i]^T * beta)^2, i == 1, n) == c), col = "#1170aa")
```

## Lasso with a single predictor I

::: incremental
-   To gain some understanding, let us consider the
    [single-predictor]{.blue} scenario, in which$$
     \hat{\beta}_\text{lasso} = \arg\min_{\beta}\frac{1}{2n}\sum_{i=1}^n(y_{i} - x_{i}\beta)^2 + \lambda |\beta|.
      $$

-   This simple problem admits an [explicit expression]{.orange} (see
    Exercises), which is $$
    \hat{\beta}_\text{lasso} = \begin{cases} \text{cov}(x,y) - \lambda, \qquad &\text{if} \quad \text{cov}(x,y) > \lambda \\
    0 \qquad &\text{if} \quad \text{cov}(x,y) \le \lambda\\
    \text{cov}(x,y) + \lambda, \qquad &\text{if} \quad \text{cov}(x,y) < -\lambda \\
    \end{cases}
    $$

-   The above solution can be written as
    $\hat{\beta}_\text{lasso} = \mathcal{S}_\lambda(\hat{\beta}_\text{ols})$,
    where $\mathcal{S}_\lambda(x) = \text{sign}(x)(|x| - \lambda)_+$ is
    the [soft-thresholding]{.blue} operator and $(\cdot)_+$ is the
    [positive part]{.orange} of a number (`pmax(0, x)`).

-   For [ridge regression]{.orange} with one predictor we obtain,
    instead: $$
    \hat{\beta}_\text{ridge} = \frac{1}{\lambda + 1}\text{cov}(x,y) =\frac{1}{\lambda + 1}\hat{\beta}_\text{ols} = \frac{1}{\lambda + 1}\frac{1}{n}\sum_{i=1}^n x_{i}y_{i}.
    $$
:::

## Lasso with a single predictor II

```{r}
x <- seq(-0.5, 0.5, length = 100)
data_plot <- data.frame(
  x = x,
  OLS = x,
  Ridge = x / (1 + sqrt(0.2)),
  Lasso = sign(x) * pmax(abs(x) - 0.2, 0)
)
data_plot <- pivot_longer(data_plot, cols = c(OLS, Ridge, Lasso), names_to = "Estimate")
ggplot(data = data_plot, aes(x = x, y = value, col = Estimate)) +
  geom_line() +
  theme_light() +
  coord_fixed() +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab(expression(hat(beta)[ols])) +
  ylab(expression(hat(beta)))
```

## Soft-thresholding and lasso solution

-   The single predictor special case provides further intuition of why
    the lasso perform [variable selection]{.blue} and
    [shrinkage]{.orange}.

. . .

-   Ridge regression induces shrinkage in a [multiplicative]{.orange}
    fashion, and the regression coefficients reach zero as
    $\lambda \rightarrow \infty$.

-   Conversely, the lasso shrinks the ordinary least squares in an
    [additive]{.blue} manner, [truncating]{.orange} them at
    [zero]{.orange} after a certain threshold.

. . .

-   Even though we do not have a closed-form expression for the lasso
    solution $\hat{\beta}_\text{lasso}$ when the covariates $p > 1$, the
    main intuition is preserved: lasso induces [sparsity]{.orange}!

## The lasso path

```{r}
#| warning: false
#| message: false
library(lars)

my_lasso <- function(X, y, standardize = TRUE) {
  n <- nrow(X)
  p <- ncol(X)
  y_mean <- mean(y) # Center the response
  y <- y - y_mean

  # Centering the covariates
  X_mean <- colMeans(X)
  if (standardize) {
    X_scale <- apply(X, 2, function(x) sqrt(mean(x^2) - mean(x)^2))
  } else {
    X_scale <- rep(1, p)
  }

  X <- scale(X, center = X_mean, scale = X_scale)

  fit <- lars(x = X, y = y, type = "lasso", normalize = FALSE, intercept = TRUE)
  beta_lasso <- coef(fit)

  # Transform back to the original scale
  beta <- t(t(beta_lasso) / X_scale)
  # Compute the intercept
  beta0 <- y_mean - X_mean %*% t(beta)


  return(list(beta_scaled = cbind(intercept = t(beta0), beta), lambda = c(fit$lambda, 0) / n))
}
```

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center

fit_lasso <- my_lasso(X, y)
data_lasso <- cbind(lambda_seq = fit_lasso$lambda, coef = fit_lasso$beta_scaled)
mse_lasso <- data.frame(lambda = data_lasso[, 1], Cp = NA, df = 1:9, sigma2 = NA)

for (i in 1:nrow(mse_lasso)) {
  mse_lasso$mse[i] <- mean((y - cbind(1, X) %*% data_lasso[i, -1])^2)
  mse_lasso$sigma2[i] <- mse_lasso$mse[i] * n / (n - mse_lasso$df[i])
  mse_lasso$Cp[i] <- mse_lasso$mse[i] + 2 * mse_lasso$sigma2[i] / n * mse_lasso$df[i]
}

colnames(data_lasso)[-c(1, 2)] <- colnames(X)
data_lasso <- tidyr::gather(data.frame(data_lasso[, -2]), lambda, value, lcavol:pgg45)
colnames(data_lasso) <- c("lambda", "Covariate", "value")

ggplot(data = data_lasso, aes(x = lambda, y = value, col = Covariate)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.position = "top") +
  scale_x_sqrt() +
  scale_color_tableau(palette = "Color Blind") +
  xlab(expression(lambda)) +
  ylab("Regression coefficients")
```

<!-- ## Comments on the lasso path -->

<!-- -   As for ridge regression, the values of $\lambda$ are expressed in -->
<!--     somewhat [arbitrary scale]{.orange}. -->

<!-- -   There exists a [maximum value]{.blue} $\lambda_0$, after -->
<!--     which all the coefficients are zero: $$ -->
<!--     \lambda_0 = \max_{j}\left| \text{cov}(\tilde{\bm{x}}_j, \bm{y})\right| = \max_{j}\left| \frac{1}{n}\sum_{i=1}^n x_{ij} y_i\right|. -->
<!--     $$ -->

<!-- . . . -->

<!-- -   It is confirmed that the variable `lcavol` is arguably the most -->
<!--     important, followed by `lweight` and `svi`, being the last variables -->
<!--     to be set equal to zero. -->

<!-- . . . -->

<!-- -   The coefficient of `age`, `gleason` and `lcp` are negative and then -->
<!--     are set equal to zero for higher values of $\lambda$. Compare this -->
<!--     with the [ridge path]{.orange}. -->

## Least angle regression I

-   Least angle regression (LAR) is a "[democratic]{.orange}" version of
    [forward stepwise regression]{.blue}.

. . .

-   Forward stepwise builds a model [sequentially]{.blue}, adding one
    variable at a time. At each step, the best variable is included in
    the [active set]{.orange} and then the least square fit is updated.

-   LAR uses a similar strategy, but any new variable contributes to the
    predictions only "as much" as it deserves.

. . .

::: callout-note
#### Main result of LAR

-   The LAR algorithm provides a way to [compute]{.blue} the entire
    [lasso path]{.orange} efficiently at the cost of a full
    least-squares fit.

-   LAR sheds light on important [statistical aspects]{.orange} of the
    lasso. A nice [LAR - lasso - boosting]{.blue} relationship is
    established, which is computationally and conceptually sound.
:::

## 

::: callout-note
#### Least angle regression algorithm (LAR)

::: incremental
1.  After centering and standardization, define the residuals
    $\bm{r}_0 = \bm{y}$ and let $\hat{\beta}^{(0)} = 0$.

2.  Find the predictor $\tilde{\bm{x}}_j$ [most correlated]{.blue} with
    the residuals $\bm{r}_0$, i.e. having the largest value for
    $\text{cov}(\tilde{\bm{x}}_j, \bm{r}_0) = \text{cov}(\tilde{\bm{x}}_j, \bm{y})$.
    Call this value $\lambda_0$ and let $\mathcal{A} = \{j\}$ be the
    [active set]{.orange}.

    i.  [Move]{.blue} $\beta_j(\lambda)$ from $\hat{\beta}_j^{(0)} = 0$
      [towards]{.blue} its [least squares solution]{.orange} by decreasing
      $\lambda$, i.e. $$
      \beta_j(\lambda) = \frac{\lambda_0 - \lambda}{\lambda_0} \text{cov}(\tilde{\bm{x}}_j, \bm{y}), \qquad 0 < \lambda \le \lambda_0,
      $$ keeping track of the residuals
      $\bm{r}(\lambda) = \bm{y} - \tilde{\bm{x}}_j\beta_j(\lambda)$.
      It can be shown that 
      $$
      |\text{cov}(\tilde{\bm{x}}_j, \bm{r}(\lambda))| = \lambda.
      $$

    ii.  Identify the value $\lambda > 0$ such that [another
    variable]{.blue} $\bm{x}_{\ell}$ has [as much correlation]{.orange}
    with the residuals as $\bm{x}_{j}$. Call this value $\lambda_1$,
    obtaining: $|\text{cov}(\tilde{\bm{x}}_{\ell}, \bm{r}(\lambda_1))| = \lambda_1$.

    iii.  Obtain the [estimate]{.blue}
    $\hat{\beta}^{(1)} = (0,\dots,\beta_j(\lambda_1), \dots, 0)$ and set $\bm{r}_1 = \bm{r}(\lambda_1)$. Define the new [active set]{.orange} $\mathcal{A} = \{j, \ell\}$ and let $\bm{X}_\mathcal{A}$ be the corresponding matrix.
:::
:::

## 

::: callout-note
#### Least angle regression algorithm (LAR)

::: incremental
3.  For $k =2,\dots,K = \min(n-1,p)$, do:

    i.  [Move]{.blue} the coefficients $\beta_\mathcal{A}(\lambda)$ from
        $\hat{\beta}_\mathcal{A}^{(k-1)}$ [towards]{.blue} their [least
        squares solution]{.orange}: $$
        \beta_\mathcal{A}(\lambda) = \hat{\beta}_\mathcal{A}^{(k-1)} + \frac{\lambda_{k-1} - \lambda}{\lambda_{k-1}}(\bm{X}_\mathcal{A}^T\bm{X}_\mathcal{A})^{-1}\bm{X}_\mathcal{A}^T\bm{r}_{k-1}, \qquad 0 < \lambda \le \lambda_{k-1},
        $$ keeping track of
        $\bm{r}(\lambda) = \bm{y} - \bm{X}_\mathcal{A}\beta_\mathcal{A}(\lambda)$.
        The covariances with the residuals are [tied]{.orange}: $$
        |\text{cov}(\tilde{\bm{x}}_j, \bm{r}(\lambda))| = \lambda, \qquad j \in \mathcal{A}.
        $$
    ii. Identify the largest value $\lambda > 0$ such that [another
        variable]{.blue} $\bm{x}_{\ell}$ has [as much
        correlation]{.orange} with the residuals. Call this
        value $\lambda_k$, so that $|\text{cov}(\tilde{\bm{x}}_{\ell}, \bm{r}(\lambda_k))| = \lambda_k$.
        
    iii. Set the [estimate]{.blue} $\hat{\beta}^{(k)}$ with entries
         $\hat{\beta}_\mathcal{A}^{(k)} = \beta_\mathcal{A}(\lambda_k)$ and zero otherwise. Let
         $\bm{r}_k = \bm{r}(\lambda_k)$.  Define the new [active set]{.orange}
        $\mathcal{A} \leftarrow \mathcal{A} \cup \{\ell \}$ and design
        matrix $\bm{X}_\mathcal{A}$.

4.  Return the pairs $\{\lambda_k, \hat{\beta}^{(k)}\}_0^K$.
:::
:::

## Least angle regression: remarks

- The coefficients in LAR change in a [piecewise]{.orange} fashion, with knots in $\lambda_k$. The LAR path coincides [almost always]{.blue} with the lasso. Otherwise, a simple modification is required:

. . .

::: callout-note
#### LAR: lasso modification

3.ii+. If a nonzero coefficient crosses zero before the next variable enters, drop it from $\mathcal{A}$ and recompute the joint least-squares direction using the reduced set.
:::

. . .

::: callout-tip
#### Practical details

- In [Step 3.ii]{.blue}, we do [not]{.orange} take small steps and then recheck the covariances. Instead, the new variable $\bm{x}_\ell$ "catching up" and the value $\lambda_k$ can be identified with some algebra.

- The LAR algorithm is [extremely efficient]{.orange}, requiring the same order of computation of least squares. The main bottleneck is [Step 3.i]{.blue}, but [QR decomposition]{.blue} can be exploited.
:::

## ☠️ - Lasso and LAR relationship

- What follows is [heuristic intuition]{.orange} for why LAR and lasso are so similar. By construction, at any stage of the LAR algorithm, we have that:
$$
\text{cov}(\tilde{\bm{x}}_j, \bm{r}(\lambda)) = \frac{1}{n}\sum_{i=1}^nx_{ij}\{y_i - \bm{x}_i^T\beta(\lambda)\} = \lambda s_j, \qquad j \in \mathcal{A},
$$
where $s_j \in \{-1, 1\}$ indicates the [sign of the covariance]{.blue}. 

. . .

- On the other hand, let $\mathcal{A}_\text{lasso}$ be the active set of the lasso. For these variables, the penalized lasso loss is differentiable, obtaining: $$
\text{cov}(\tilde{\bm{x}}_j, \bm{r}(\lambda)) = \frac{1}{n}\sum_{i=1}^nx_{ij}\{y_i - \bm{x}_i^T\beta(\lambda)\} = \lambda \text{sign}(\beta_j), \qquad j \in \mathcal{A}_\text{lasso},
$$
which [coincide]{.orange} with the LAR solution if $s_j =  \text{sign}(\beta_j)$, which is [almost always the case]{.blue}.

## Uniqueness of the lasso solution

- The lasso can be computed even when $p > n$. In these cases, will it be [unique]{.orange}?

. . .

::: callout-warning
#### Three uniqueness results (Tibshirani, 2013)

- If $\bm{X}$ has [full rank]{.blue} $\text{rk}(\bm{X}) = p$, which implies $p \le n$, then $\hat{\beta}_\text{lasso}$ is uniquely determined.

- If [all]{.orange} the values of $\bm{X}$ are [different]{.orange}, then $\hat{\beta}_\text{lasso}$ is uniquely determined, even when $p > n$. 

- The [predictions]{.blue} $\bm{X}\hat{\beta}_\text{lasso}$ are always uniquely determined.

:::

. . .

- Non-uniqueness may occur in the presence of [discrete-valued]{.orange} data. It is of practical concern only whenever $p > n$ and if we are interested in interpreting the coefficients. 

- Much more [general]{.blue} sufficient conditions for the uniqueness of $\hat{\beta}_\text{lasso}$ are known, but they are quite technical and complex to check in practice.  


## The degrees of freedom of the lasso

- In ridge regression, the [effective degrees of freedom]{.blue} have a simple formula.

. . .

- [Miraculously]{.orange}, for the lasso with a fixed penalty parameter $\lambda$, the number of nonzero coefficients $|\mathcal{A}_\text{lasso}(\lambda)|$ is an [unbiased estimate]{.blue} of the degrees of freedom.

. . .

:::callout-warning
#### Degrees of freedom (Zhou, Hastie, and Tibshirani, 2007, Tibshirani and Taylor, 2012)

- Suppose $\bm{X}$ has [full rank]{.orange} $\text{rk}(\bm{X}) = p$ and $\bm{y}$ follows a Gaussian law. Then:
$$
\text{df}_\text{lasso} = 1 + \mathbb{E}|\mathcal{A}_\text{lasso}(\lambda)|.
$$

- Under further regularity conditions, the above relationship is [exact]{.blue} if we consider the [LAR active set]{.orange}, therefore implicitly using a different set of $\lambda$ values for any fit:
$$
\text{df}_\text{lar} = 1 + |\mathcal{A}|.
$$
:::

## ☠️ - Effective degrees of freedom of LAR and best subset

```{r}
#| cache: true
n_sim <- 100
p_sim <- 25
R <- 5000
sigma_sim <- 0.5

set.seed(220)
X_sim <- matrix(runif(n_sim * p_sim), n_sim, p_sim)
# Here, data are not related to any of the covariates
Y_sim <- matrix(rnorm(n_sim * R, mean = 4, sigma_sim), R, n_sim)

pred_best_sim <- array(0, c(R, n_sim, p_sim + 1))
pred_lasso_sim <- array(0, c(R, n_sim, p_sim + 1))

for (r in 1:R) {
  y_sim <- Y_sim[r, ]
  # Lasso fit
  lasso_sim <- my_lasso(X_sim, y_sim, standardize = TRUE)
  # Best subset
  best_sim <- regsubsets(y_sim ~ X_sim, data = NULL, method = "exhaustive", nbest = 1, nvmax = p_sim)
  sum_best_sim <- summary(best_sim)

  for (j in 1:(p_sim + 1)) {
    pred_lasso_sim[r, , j] <- cbind(1, X_sim) %*% lasso_sim$beta_scaled[j, ]
    if (j == 1) {
      pred_best_sim[r, , 1] <- pred_lasso_sim[r, , 1]
    } else {
      pred_best_sim[r, , j] <- cbind(1, X_sim)[, sum_best_sim$which[j - 1, ]] %*% coef(best_sim, j - 1)
    }
  }
}

df_lasso <- matrix(0, n_sim, p_sim + 1)
df_best <- matrix(0, n_sim, p_sim + 1)

for (j in 1:(p_sim + 1)) {
  for (i in 1:n_sim) {
    df_lasso[i, j] <- cov(pred_lasso_sim[, i, j], Y_sim[, i])
    df_best[i, j] <- cov(pred_best_sim[, i, j], Y_sim[, i])
  }
}
df_lasso <- colSums(df_lasso) / sigma_sim^2
df_best <- colSums(df_best) / sigma_sim^2

data_df <- data.frame(active_set = 1:(p_sim + 1), df = c(df_lasso, df_best), Method = rep(c("LAR", "Best subset"), each = p_sim + 1))
```

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center
ggplot(data = data_df, aes(x = active_set, y = df, col = Method)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  xlab("Number of non-zero coefficients") +
  ylab("Effective degrees of freedom")
```


## Cross-validation for lasso

```{r}
#| message: false
resid_lasso <- matrix(0, n, (p + 1))

for (k in 1:10) {
  # Hold-out dataset
  X_test_k <- as.matrix(subset(assessment(cv_fold$splits[[k]]), select = -c(lpsa)))
  y_test_k <- assessment(cv_fold$splits[[k]])$lpsa

  X_train_k <- as.matrix(subset(analysis(cv_fold$splits[[k]]), select = -c(lpsa)))
  y_train_k <- analysis(cv_fold$splits[[k]])$lpsa

  fit_lasso <- my_lasso(X_train_k, y_train_k, standardize = TRUE)

  for (j in 1:(p + 1)) {
    # Predictions
    y_hat <- cbind(1, X_test_k) %*% fit_lasso$beta_scaled[j, ]
    # MSE of the best models for different values of lambda
    resid_lasso[complement(cv_fold$splits[[k]]), j] <- y_test_k - y_hat
  }
}
```

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

data_cv <- data.frame(
  df = 1:9,
  MSE = apply(resid_lasso^2, 2, mean),
  SE = apply(resid_lasso^2, 2, function(x) sd(x) / sqrt(n))
)

se_rule <- data_cv$MSE[which.min(data_cv$MSE)] + data_cv$SE[which.min(data_cv$MSE)]
df_min_cv <- data_cv$df[head(which(data_cv$MSE < se_rule), 1)]

ggplot(data = data_cv, aes(x = df, y = MSE)) +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymax = MSE + SE, ymin = MSE - SE)) +
  scale_x_continuous(breaks = 1:9) +
  geom_hline(yintercept = se_rule, linetype = "dotted") +
  geom_vline(xintercept = df_min_cv, linetype = "dotted") +
  theme_light() +
  xlab("Degrees of freedom") +
  ylab("Mean squared error (10-fold cv)")
```

```{r}
#| include: false
#| execute: false
# Double checks
y_std <- prostate_train$lpsa / sqrt(mean(prostate_train$lpsa^2) - mean(prostate_train$lpsa)^2)

fit_lasso <- my_lasso(X, y_std, standardize = TRUE)

t(fit_lasso$beta_scaled)

coef(glmnet(X, y_std, family = "gaussian", standardize = TRUE, alpha = 1, thresh = 1e-20, lambda = fit_lasso$lambda))
```



## The LAR (lasso) estimate

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center

data_lasso$df <- 1:9
ggplot(data = data_lasso, aes(x = df, y = value, col = Covariate)) +
  geom_point() +
  geom_line() +
  theme_light() +
  geom_vline(xintercept = 4, linetype = "dashed") +
  scale_x_continuous(breaks = 1:9) +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Degrees of freedom") +
  ylab("Regression coefficients")
```

## Other properties of LAR and lasso

::: incremental

- [Bayesian interpretation]{.orange}: the penalty can be interpreted as a Laplace prior on $\beta$.

- As mentioned, under certain conditions the LAR algorithm can be seen as the limiting case of a [boosting procedure]{.blue}, in which small corrections to predictions are iteratively performed. 

- The [nonnegative garrote]{.orange} (Breiman, 1995) is a two-stage procedure with a close relationship to the lasso. Breiman’s paper was the inspiration for Tibshirani (1996).

- There is a large body of theoretical work on the behavior of the lasso, focused on:
  - the mean-squared-error consistency of the lasso;
  - the recovery of the nonzero support set of the true regression parameters, sometimes called [sparsistency]{.orange}.

- The interested reader work may have a look at the (very technical) Chapter 11 of Hastie, Tibshirani and Wainwright (2015)

:::

## Summary of LARS and lasso

::: callout-tip
#### Pros

-   LAR and Lasso are extremely efficient approaches that perform both [variable selection]{.blue} and [shrinkage]{.orange} at the same time. 

- Lasso produces a [parsimonious]{.blue} model.

:::

. . .

::: callout-warning
#### Cons

- Lasso can be applied when $p > n$, but there might be [uniqueness]{.orange} issues. Moreover, the lasso selects at most $n$ variables.

- If there is a group of variables with high pairwise correlations, the lasso tends to "randomly" select only one variable from the group. 

- When $p < n$, if there are [high correlations]{.orange} between predictors, it has been empirically observed that the prediction performance of the lasso is dominated by ridge regression.
:::


## The `prostate` dataset. A summary of the estimates

```{r}
#| output: false
tab <- data.frame(OLS = rep(0, p + 1), best_subset = rep(0, p + 1), PCR = rep(0, p + 1), Ridge = rep(0, p + 1), Lasso = rep(0, p + 1))

rownames(tab) <- colnames(sum_best$which)

# OLS
tab$OLS <- coef(lm(lpsa ~ ., data = prostate_train))

# Best subset
tab$best_subset <- c(coef(lm(lpsa ~ lcavol + lweight, data = prostate_train)), rep(0, 6))

# Principal components regression (PCR)
fit_pcr <- pcr(lpsa ~ ., data = prostate_train, center = TRUE, scale = FALSE)
beta_pcr <- c(coef(fit_pcr, 3))
beta_pcr <- c(mean(prostate_train$lpsa) - colMeans(X) %*% beta_pcr, beta_pcr)
tab$PCR <- beta_pcr

# Ridge

tab$Ridge <- my_ridge(X, y, standardize = TRUE, lambda_tilde = lambda_tilde_min_cp)

# Lasso
tab$Lasso <- c(my_lasso(X, y)$beta_scale[4, ])

colnames(tab) <- c("OLS", "Best subset", "PCR", "Ridge", "Lasso")
round(tab, digits = 3)
```

|               | Least squares | Best subset |    PCR |  Ridge | Lasso |
|:--------------|--------------:|------------:|-------:|-------:|------:|
| `(Intercept)` |         2.465 |       2.477 |  2.455 |  2.467 | 2.468 |
| `lcavol`      |         0.680 |       0.740 |  0.287 |  0.588 | 0.532 |
| `lweight`     |         0.263 |       0.316 |  0.339 |  0.258 | 0.169 |
| `age`         |        -0.141 |           . |  0.056 | -0.113 |     . |
| `lbph`        |         0.210 |           . |  0.102 |  0.201 |     . |
| `svi`         |         0.305 |           . |  0.261 |  0.283 | 0.092 |
| `lcp`         |        -0.288 |           . |  0.219 |  -0.172|     . |
| `gleason`     |        -0.021 |           . | -0.016 |  0.010 |     . |
| `pgg45`       |         0.267 |           . |  0.062 |  0.204 |     . |


## The results on the test set

- At the beginning of this unit, we split the data into [training]{.blue} set and [test]{.orange} set. Using the training, we selected $\lambda$ via cross-validation or the $C_p$ index.

- Using the final test set with $30$ observations, we will assess which model is [preferable]{.orange}.

. . .

```{r}
X_test <- cbind(1, as.matrix(prostate_test)[, -9])
y_test <- prostate_test[, 9]

pred_ols <- X_test %*% tab$OLS
pred_best <- X_test %*% tab$best_subset
pred_pcr <- X_test %*% tab$PCR
pred_ridge <- X_test %*% tab$Ridge
pred_lasso <- X_test %*% tab$Lasso

tab_results <- matrix(c(
  mean((y_test - pred_ols)^2),
  mean((y_test - pred_best)^2),
  mean((y_test - pred_pcr)^2),
  mean((y_test - pred_ridge)^2),
  mean((y_test - pred_lasso)^2)
), nrow = 1)
colnames(tab_results) <- c("OLS", "Best subset", "PCR", "Ridge", "Lasso")
rownames(tab_results) <- "Test error (MSE)"
knitr::kable(tab_results, digits = 3)
```

. . .

- All the approaches presented in this unit perform better than ordinary least squares. 

- The [lasso]{.blue} is the approach with [lowest mean squared error]{.orange}. At the same time, it is also a [parsimonious]{.blue} choice. 

- Best subset is the second best, doing a good job in this example... but here $p = 8$, so there were no computational difficulties!

# Elastic-net and pathwise algorithms

## Elastic-net

- The [elastic-net]{.blue} is a compromise between ridge and lasso. It selects variables like the lasso and shrinks together the coefficients of correlated predictors like ridge.

. . .

-   Having removed the intercept, the elastic-net estimator $\hat{\beta}_\text{en}$ is the [minimizer]{.orange} of:
$$
    \frac{1}{2n}\sum_{i=1}^n(y_i - \bm{x}_i^T\beta)^2 + \lambda\sum_{j=1}^p\left(\alpha  |\beta_j| + \frac{(1 - \alpha)}{2}\beta_j^2\right),
$$ 
where $0 < \alpha < 1$ and the [complexity parameter]{.orange} $\lambda > 0$.

. . .

- [Ridge regression]{.blue} is a special case, when $\alpha = 0$. [Lasso]{.blue} is also a special case, when $\alpha = 1$.

- It is often [not worthwhile]{.orange} to estimate $\alpha$ using cross-validation. A typical choice is $\alpha = 0.5$. 

. . .

- An advantage of the elastic-net is that it has a [unique solution]{.orange}, even when $p > n$.

- Another nice property, shared by ridge, is that whenever $\tilde{\bm{x}}_j = \tilde{\bm{x}}_\ell$, then $\hat{\beta}_{j,\text{en}} = \hat{\beta}_{\ell,\text{en}}$. On the other hand, the [lasso]{.orange} estimator would be [undefined]{.orange}.

## Convex optimization

- The estimators OLS, ridge, lasso, and elastic-net have a huge [computational advantage]{.orange} compared, e.g., to best subset: they are all [convex optimization problems]{.blue}.

. . .

- A function $f: \mathbb{R}^p \rightarrow \mathbb{R}$ is [convex]{.blue} if for any values $\bm{b}_1, \bm{b}_2 \in \mathbb{R}^p$ and $t \in [0, 1]$ it holds that
$$
f(t \bm{b}_1 + (1-t)\bm{b}_2) \le  t f(\bm{b}_1) + (1-t) f(\bm{b}_2).
$$
Replacing $\le$ with $<$ for $t \in(0,1)$ gives the definition of [strict convexity]{.orange}.

. . .

- [OLS]{.blue} and the [lasso]{.blue} are, for a general design matrix $\bm{X}$, convex problems. On the other hand,  [ridge]{.orange} and [elastic net]{.orange} are strictly convex, as well as OLS and lasso when $\text{rk}(\bm{X}) = p$.

. . .

::: callout-warning
#### Properties of convex optimization
- In a [convex]{.blue} optimization problem, every local minimum is a [global minimum]{.blue};
- In a [strictly convex]{.orange} optimization problem, there exists a [unique]{.orange} global minimum.
:::

## Elastic-net with a single predictor

- The elastic-net estimate $\hat{\beta}_\text{en}$ is typically obtained through the [coordinate descent]{.blue} algorithm, which works well here due [convexity]{.blue} and the following [property]{.orange}.

. . .


- In the [single-predictor]{.blue} scenario the elastic-net minimization problem simplifies to $$
     \hat{\beta}_\text{en} = \arg\min_{\beta}\frac{1}{2n}\sum_{i=1}^n(y_{i} - x_{i}\beta)^2 + \lambda \left( \alpha |\beta| + \frac{(1-\alpha)}{2} \beta^2\right).
      $$
      
. . .

-   It can be shown that an [explicit expression]{.orange} for $\hat{\beta}_\text{en}$ is available, which is 
    $$\hat{\beta}_\text{en} = \frac{1}{1 + (1 - \alpha)\lambda}\mathcal{S}_{\alpha\lambda}(\hat{\beta}_\text{ols}),$$
    where $\mathcal{S}_\lambda(x) = \text{sign}(x)(|x| - \lambda)_+$ is the [soft-thresholding]{.blue} operator and the [least square estimate]{.orange} is $\hat{\beta}_\text{ols} = n^{-1}\sum_{i=1}^n x_i y_i$.


## Coordinate descent

- The [coordinate descent]{.blue} algorithm is based on a simple principle: optimize one coefficient (coordinate) at a time, keeping the others fixed.

- We can [re-write]{.orange} the objective function of the elastic-net in a more convenient form:
$$
\frac{1}{2n}\sum_{i=1}^n \left(\textcolor{red}{y_i - \sum_{j \neq k} x_{ij}\beta_j} - x_{ik} \beta_k\right)^2 + \lambda\left(\alpha  |\beta_k| + \frac{(1 - \alpha)}{2} \beta_k^2\right) + \underbrace{\lambda  \sum_{j\neq k}^p \{\alpha |\beta_j| + \frac{(1 - \alpha)}{2}\beta_j^2\}}_{\text{does not depend on } \beta_k},
$$ 

. . .

- Let us define the [partial residuals]{.blue} $r_i^{(j)} = y_i - \sum_{j \neq k} x_{ij}\beta_j$. Then the updated $\beta_k$ is
$$
\beta_k \leftarrow \frac{1}{1 + (1 - \alpha)\lambda}\mathcal{S}_{\alpha\lambda}\left(\frac{1}{n}\sum_{i=1}^n x_{ik} r_i^{(j)}\right).
$$
We [cycle]{.orange} this update for $k=1,\dots,p$, over and over until convergence.

## Coordinate descent - Example

[Objective function]{.orange}: $(1 - \beta_1 - 2 \beta_2)^2 + (3 - \beta_1 - 2 \beta_2)^2 + 5 (|\beta_1| + |\beta_2|)$.

```{r}
# Objective function
f <- function(x) {
  return((1 - x[1] - 2 * x[2])^2 + (3 - x[1] - 2 * x[2])^2 + 5 * (abs(x[1]) + abs(x[2])))
}

# Finds the minimum
m <- optim(c(0, 0), f, method = "Nelder-Mead")

# Create a data frame for contour plotting
delta <- 0.05
x <- seq(-2.5, 2.5, delta)
y <- seq(-2.5, 3, delta)
df <- expand.grid(X = x, Y = y)
df$Z <- apply(df, 1, function(row) f(row))

ggplot(data = df, aes(x = X, y = Y)) +
  theme_light() +
  geom_contour(aes(z = Z), colour = "#1170aa", bins = 25) +
  geom_point(
    data = data.frame(
      X = c(m$par[1], -1.53),
      Y = c(m$par[2], -2)
    ),
    aes(x = X, y = Y), color = "black"
  ) +
  geom_segment(aes(x = -1.53, y = -2, xend = -1.53, yend = 1.5),
    arrow = arrow(type = "closed", angle = 25, length = unit(0.05, "inches")), color = "#fc7d0b"
  ) +
  geom_segment(aes(x = -1.53, y = 1.5, xend = m$par[1], yend = 1.5),
    arrow = arrow(type = "closed", angle = 25, length = unit(0.05, "inches")), color = "#fc7d0b"
  ) +
  geom_segment(aes(x = m$par[1], y = 1.5, xend = m$par[1], yend = m$par[2]),
    arrow = arrow(type = "closed", angle = 25, length = unit(0.05, "inches")), color = "#fc7d0b"
  ) +
  labs(x = expression(beta[1]), y = expression(beta[2])) +
  theme_minimal() +
  coord_fixed()
```



## Pathwise coordinate optimization

::: incremental

In a regression model with an elastic-net penalty, the coordinate descent is [theoretically guaranteed]{.orange} to reach the global minimum.

- The coordinate descent algorithm is implemented in the `glmnet` package. It is, de facto, the default algorithm for penalized generalized linear models. 

- The `glmnet` implementation is very efficient due to several additional tricks:
  - The [warm start]{.blue}. The algorithm for $\lambda_k$ is initialized in the previously obtained solution using $\lambda_{k-1}$.
  - Partial residuals can be efficiently obtained [without re-computing]{.orange} the whole linear predictor.
  - The code is written in [Fortran]{.blue}.
  - Many other tricks are employed: active set convergence, tools for sparse $\bm{X}$ matrices, etc. 

- The same algorithm can also be used to fit ridge regression and lasso, which is often convenient even though alternatives would be available. 
:::

# Generalized linear models

## Generalized linear models

- Almost everything we discussed in this unit for [regression]{.orange} problems can be [extended to GLMs]{.blue} and, in particular, to classification problems. 

. . .

::: callout-note
#### Best subset selection for GLMs

- Best subset selection and its forward and backward greedy approximations are conceptually straightforward to extend to GLMs (using log-likelihood and ML).

- The only difficulty is computations because leaps-and-bound approaches can not be applied here.
:::

. . .

::: callout-tip
#### Principal components for GLMs

- Principal components can be straightforwardly applied to GLMs. 

- The shrinkage effect and their ability to control the variance remain unaltered, but the theory (e.g., variance of the estimator) holds only in an approximate sense. 
:::


## Shrinkage methods for GLMs

- Shrinkage methods such as ridge and lasso can also be generalized to GLMs.

- The elastic-net approach for [logistic regression]{.blue}, which covers ridge and lasso as special cases, becomes:
$$
\min_{(\beta_0, \beta)}\left\{ -\frac{1}{n}\sum_{i=1}^n y_i (\beta_0 + \bm{x}_i^T\beta) - \log\{1 + \exp(\beta_0 + \bm{x}_i^T\beta)\} + \lambda\sum_{j=1}^p\left(\alpha  |\beta_j| + \frac{(1 - \alpha)}{2}\beta_j^2\right)\right\},
$$ 
which is an instance of [penalized log-likelihood]{.orange}.

- [Most of the properties]{.blue} (e.g. ridge = variance reduction and shrinkage, lasso = variable selection) and other high-level considerations we made so far are [still valid]{.blue}.

- Computations are somewhat more cumbersome. The `glmnet` package provides the numerical routines for fitting this model, using variants of [coordinate descent]{.blue}. 

- The core idea is to obtain a [quadratic approximation]{.blue} of the log-likelihood, as for IWLS. Then, the approximated loss becomes a ([weighted]{.orange}) penalized regression problem. 

# References

## References I

-   [Main references]{.blue}
    - **Chapter 3** of Azzalini, A. and Scarpa, B. (2011), [*Data
        Analysis and Data
        Mining*](http://azzalini.stat.unipd.it/Book-DM/), Oxford
        University Press.
    - **Chapters 3 and 4** of Hastie, T., Tibshirani, R. and Friedman, J.
        (2009), [*The Elements of Statistical
        Learning*](https://hastie.su.domains/ElemStatLearn/), Second
        Edition, Springer.
    - **Chapter 16** of Efron, B. and Hastie, T. (2016), [*Computer Age Statistical
    Inference*](https://hastie.su.domains/CASI/), Cambridge University
    Press.
    - **Chapters 2,3 and 5** of Hastie, T., Tibshirani, R. and Wainwright, M. (2015). [*Statistical Learning with Sparsity: The Lasso and Generalizations*](https://hastie.su.domains/StatLearnSparsity_files/SLS_corrected_1.4.16.pdf). CRC Press.
    
- [Best subset selection]{.orange}
    - Hastie, T., Tibshirani, R., and Tibshirani, R.J. (2020). Best subset, forward stepwise or lasso? Analysis and recommendations based on extensive comparisons. *Statistical Science* **35**(4): 579--592.



## References II


-   [Ridge regression]{.orange}
    - Hoerl, A. E., and Kennard, R. W. (1970). Ridge regression: biased estimation for nonorthogonal problems. *Technometrics* **12**(1), 55–67. 
    - Hastie, T. (2020). Ridge regularization: an essential concept in
        data science. *Technometrics*, **62**(4), 426-433.

-   [Lasso]{.orange}
    - Tibshirani, R. (1996). Regression selection and shrinkage via the lasso. *Journal of the Royal Statistical Society. Series B: Statistical Methodology*, **58**(1), 267-288.
    - Efron, B., Hastie, T., Johnstone, I.,  and Tibshirani, R. (2004). Least Angle Regression. *Annals of Statistics* **32**(2), 407-499.
    -  Zou, H., Hastie, T., and Tibshirani, R. (2007). On the ‘degrees of freedom’ of the lasso. *Annals of Statistics* **35**(5), 2173-2192.
    - Tibshirani, R. J. (2013). The lasso problem and uniqueness. *Electronic Journal of Statistics* **7**(1), 1456-1490. 
    
## References III
-   [Elastic-net]{.orange}
    - Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. *Journal of the Royal Statistical Society. Series B: Statistical Methodology* **67**(2), 301-320.
    
- [Computations for penalized methods]{.orange}
    - Friedman, J., Hastie, T., Höfling, H. and Tibshirani, R. (2007). Pathwise coordinate optimization. *The Annals of Applied Statistics* **1**(2), 302-32. 
    - Tay, J. K., Narasimhan, B. and Hastie, T. (2023). Elastic net regularization paths for all generalized linear models. *Journal of Statistical Software* **106** (1).