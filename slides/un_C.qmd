---
title: "Shrinkage and variable selection"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    df-print: tibble
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_C_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 150
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
    highlight-style: github
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 150
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_C.qmd", output = "../code/un_C.R")
styler:::style_file("../code/un_C.R")
```

::: columns
::: {.column width="20%"}
![](img/lasso.png)
:::

::: {.column width="80%"}
-   In this unit we will cover the following [topics]{.orange}:

    -   Best subset regression
    -   Principal component regression
    -   Ridge regression
    -   Lasso, LARS, elastic-net

-   The common themes are called [variable
    selection]{.blue} and [shrinkage estimation]{.orange}.

-   The issue we face is the presence of a high number $p$ of covariates that are [potentially
    irrelevant]{.blue}.
    
- This problem is quite challenging when the [ratio]{.blue} $p / n$ is [large]{.blue}. 

- In the [extreme case]{.orange} $p > n$, is there any hope to fit a meaningful model?
:::
:::

## The `prostate` dataset

::: incremental
-   The `prostate` cancer data investigates the relationship between the
    prostate-specific [antigen]{.orange} and a number of clinical
    measures, in men about to receive a prostatectomy.

-   We want to [predict]{.blue} the logarithm of a [prostate-specific
    antigen]{.orange} (`lpsa`) as a function of:

    -   logarithm of the cancer volume (`lcavol`);
    -   logarithm of the prostate weight (`lweight`);
    -   age each man (`age`);
    -   logarithm of the benign prostatic hyperplasia amount (`lbph`);
    -   seminal vesicle invasion (`svi`), a binary variable;
    -   logarithm of the capsular penetration (`lcp`);
    -   Gleason score (`gleason`), an ordered categorical variable;
    -   Percentage of Gleason scores $4$ and $5$ (`pgg45`).
- This [dataset](https://hastie.su.domains/ElemStatLearn/datasets/prostate.data) has been used in the [original paper]{.orange} by Tibshirani (1996) to present the lasso and a description is given in [Section 3.2.1]{.blue} of HTF (2009).

:::

## A `glimpse` of the `prostate` dataset

::: {.panel-tabset}
## Original dataset

```{r}
#| message: false
rm(list = ls())
library(tidyverse)
prostate <- read.table("../data/prostate_data.txt")
glimpse(prostate)
```

## Standardized dataset

```{r}
# Standardize the predictors, as in Tibshirani (1996)
which_vars <- which(colnames(prostate) %in% c("lpsa", "train"))
prostate[, -which_vars] <- apply(prostate[, -which_vars], 2, function(x) (x - mean(x)) / sd(x))

# Split in training and test
prostate_train <- filter(prostate, train) %>% select(-train)
prostate_test <- filter(prostate, train == FALSE) %>% select(-train)

glimpse(prostate)
```
:::

-   The variable `train` splits the data into a training and test set,
    as in the textbook.

-   Thus, there are $n = 67$ observations in the [training]{.orange} set and
    $30$ in the [test]{.blue} set.
    
-   There are in total $8$ [variables]{.orange} that can be used to predict the antigen `lpsa`. We [centered]{.orange} and [standardized]{.blue} all the covariates before the training/test split.


## Correlation matrix of `prostate`

```{r}
#| fig-width: 15
#| fig-height: 7
#| fig-align: center
library(ggcorrplot)
corr <- cor(subset(prostate_train, select = -lpsa)) # Remove the outcome lpsa
ggcorrplot(corr,
  hc.order = FALSE,
  outline.col = "white",
  ggtheme = ggplot2::theme_bw,
  colors = c("#fc7d0b", "white", "#1170aa")
)
```

## The variable selection problem

::: incremental
-   We consider a [linear model]{.orange} in which the response variable
    $Y_i$ (`lpsa`) is related to the covariates through the function$$
      f(\bm{x}_i; \beta_0, \beta) = \beta_0+ \beta_1 x_{i1} + \cdots + \beta_p x_{ip} =\beta_0 + \bm{x}_i^T\beta.
      $$ In this unit the [intercept]{.blue} $\beta_0$ will play a
    special role, therefore we use this slightly different notation
    compared to [Unit A.1](unit_A1.html).

- Including a lot of covariates into the model is not necessarily a good thing!

-   Indeed, some variables are likely to be [irrelevant]{.blue}:

    -   they might be [correlated]{.orange} with other covariates and
        therefore [redundant]{.orange};
    -   they could be uncorrelated with the response `lpsa`.
    
-   If we use all the $p = 8$ available covariates, the estimated
    $f(\bm{x}; \hat{\beta_0}, \hat{\beta})$ might have a [high
    variance]{.orange}, without important gain in term of bias, i.e. a
    [large mean squared error]{.blue}.

-   We are looking for a [simpler model]{.orange} having,
    hopefully, a lower mean squared error.
:::

## A naïve approach: (ab)using p-values

::: {style="font-size: 75%;"}
```{r}
tab <- data.frame(broom::tidy(lm(lpsa ~ ., data = prostate_train), conf.int = FALSE))
rownames(tab) <- tab[, 1]
tab <- t(as.matrix(tab[, -1]))
knitr::kable(tab, digits = 2)
```
:::

::: incremental
-   It is common practice to use the [p-values]{.orange}, e.g. those
    obtained through the `summary` function, to perform [model
    selection]{.blue} in a stepwise fashion.

-   A typical procedure is to omit "non significant" coefficients, refit
    the model, and repeat this scheme until we obtain only "significant"
    coefficients.

-   Unfortunately, this is [not a good idea]{.orange}, at least when done without appropriate [multiplicity corrections]{.blue}.

-   In the first place, the p-values of the above table are meant to be
    used in the context of a single hypothesis testing problem, [not]{.orange} to
    make [several iterative choices]{.orange}.

-   Such an iterative usage of "univariate" p-values is formally [incorrect]{.orange}
    because it leads to the well-known [multiple testing
    problem]{.blue}.
:::

## A predictive perspective

-   "*All models are approximations. Essentially, all models are wrong,
    but some are useful*." George E. P. Box

::: incremental
-   It is important to stress that if the
    [focus]{.blue} is just on [prediction]{.blue}, we do not necessarily
    care about selecting the "true" set of parameters.

-   In many data mining problems, the focus is on the
    [minimization]{.orange} of the [prediction errors]{.orange}.

-   Hence, often times we may [accept some bias]{.blue} (i.e. we use a
    "wrong" but useful model), if this leads to a [reduction in
    variance]{.orange}.

-   Besides, in certain cases it does not even make much sense to speak
    about the "true parameters".

-   For example, what if the true $f(\bm{x})$ were not linear? In this
    context, a [linear model]{.blue} is simply an approximation of the
    unknown $f(\bm{x})$ and hypothesis testing procedures are ill-posed.
:::

## Best subset selection

-   A more principled approach is based on the tools of [Unit B](un_B.html).

-   Ideally, we could perform an [exhaustive search]{.orange}
    considering all the $2^p$ possible models and then selecting the one
    having the best out-of-sample predictive performance.

. . .

::: callout-note
#### Best subset procedure

1.  Let $\mathcal{M}_0$ be the [null model]{.blue}, which contains no
    predictors, i.e. set $\hat{y}_i = \hat{\beta}_0 = \bar{y}$.

2.  For $k =1,\dots,p$, do:

    i.  Estimate [all]{.orange} the $\binom{p}{k}$ models that contain
        exactly $k$ covariates;

    ii. Identify the "best" model with $k$ covariates
        having the smallest $\text{MSE}_{k, \text{train}}$; call it $\mathcal{M}_k$.
:::

-   A model with more variables has lower [training]{.orange} error, namely $\text{MSE}_{k + 1, \text{train}} \le \text{MSE}_{k, \text{train}}$ by construction. Hence, the optimal subset size
    $k$ must be chosen e.g. via [cross-validation]{.blue}.

## Step 1. and 2. of best subset selection

```{r}
# Here I compute some basic quantities
X <- model.matrix(lpsa ~ ., data = prostate_train)[, -1]
y <- prostate_train$lpsa
n <- nrow(X)
p <- ncol(X) # This does not include the intercept
```

```{r}
library(leaps)
fit_best <- regsubsets(lpsa ~ ., data = prostate_train, method = "exhaustive", nbest = 40, nvmax = p)
sum_best <- summary(fit_best)
sum_best$p <- rowSums(sum_best$which) - 1 # Does not include the intercept here
```

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

library(ggplot2)
library(ggthemes)
data_best_subset <- data.frame(p = sum_best$p, MSE = sum_best$rss / n)
data_best_subset <- reshape2::melt(data_best_subset, id = c("p"))
colnames(data_best_subset) <- c("p", "MSE", "value")

data_best_subset2 <- data.frame(p = unique(sum_best$p), MSE = tapply(sum_best$rss / n, sum_best$p, min))

ggplot(data = data_best_subset, aes(x = p, y = value)) +
  geom_point() +
  theme_light() +
  theme(legend.position = "top") +
  geom_line(data = data_best_subset2, aes(x = p, y = MSE), col = "#fc7d0b") +
  geom_point(data = data_best_subset2, aes(x = p, y = MSE), col = "#fc7d0b", size = 1.5) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of covariates (p)") +
  ylab("Mean squared error (training)")
```

## The "best" models $\mathcal{M}_1,\dots, \mathcal{M}_p$

- The output of the [best subset selection]{.orange}, on the training set is:

```{r}
summary(regsubsets(lpsa ~ ., data = prostate_train, method = "exhaustive", nbest = 1, nvmax = p))$outmat
```

::: incremental

- The above table means that the best model with $k = 1$ uses the variable `lcavol`, whereas when $k = 2$ the selected variables are `lcavol` and `lweight`, and so on and so forth.

- Note that, in general, these models are [not]{.orange} necessarily [nested]{.orange}, i.e. a variable selected at step $k$ is not necessarily included at step $k +1$.  

- What is the optimal subset size $k$ in terms on out-of-sample mean squared error?

- Clearly, we cannot use the training $\text{MSE}$. A possibility is cross-validation. 

:::

## The wrong way of doing cross-validation

::: incremental

-   Consider a regression problem with a [large number of predictors]{.orange} (relative to $n$) such as the `prostate` dataset.

-   A typical strategy for analysis might be as follows:

    1.  Screen the predictors: find a subset of "good" predictors that show fairly strong correlation with the response;

    2.  Using this subset of predictors (i.e. `lcavol`, `lweight` and `svi`), build a regression model;

    3.  Use cross-validation to estimate the prediction error of the model in 2.

-   Is this a correct application of cross-validation?

-   If your reaction was "[this is absolutely wrong!]{.orange}", it means you correctly understood the principles of cross-validation.

-   If you though this was an ok-ish idea, you may want to read [Section 7.10.2]{.blue} of HTF (2009).

:::


## Step 3. of best subset selection via cross-validation

```{r}
library(rsample)

set.seed(123)
cv_fold <- vfold_cv(prostate_train, v = 10)
resid_subs <- matrix(0, n, p + 1)

for (k in 1:10) {
  
  # Estimation of the null model
  fit_null <- lm(lpsa ~ 1, data = analysis(cv_fold$splits[[k]]))
  # Best subset using branch and bound
  fit <- regsubsets(lpsa ~ ., data = analysis(cv_fold$splits[[k]]), method = "exhaustive", nbest = 1, nvmax = p)
  sum <- summary(fit)

  # Hold-out quantities
  X_k <- as.matrix(cbind(1, assessment(cv_fold$splits[[k]]) %>% select(-lpsa)))
  y_k <- assessment(cv_fold$splits[[k]])$lpsa

  # MSE of the null model
  resid_subs[complement(cv_fold$splits[[k]]), 1] <- y_k - predict(fit_null, assessment(cv_fold$splits[[k]]))
  
  # MSE of the best models for different values of p
  for (j in 2:(p+1)) {
    y_hat <- X_k[, sum$which[j - 1, ]] %*% coef(fit, j - 1)
    resid_subs[complement(cv_fold$splits[[k]]), j] <- y_k - y_hat
  }
}
```

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

data_cv <- data.frame(
  p = 0:p,
  MSE = apply(resid_subs^2, 2, mean),
  SE = apply(resid_subs^2, 2, function(x) sd(x) / sqrt(n))
)

se_rule <- data_cv$MSE[which.min(data_cv$MSE)] + data_cv$SE[which.min(data_cv$MSE)]
p_optimal <- which(data_cv$MSE < se_rule)[1] - 1

ggplot(data = data_cv, aes(x = p, y = MSE)) +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymax = MSE + SE, ymin = MSE - SE)) +
  geom_hline(yintercept = se_rule, linetype = "dotted") + 
  geom_vline(xintercept = p_optimal, linetype = "dotted") + 
  theme_light() +
  scale_x_continuous(breaks = 0:9) +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of covariates (p)") +
  ylab("Mean squared error (10-fold cv)")
```

- By applying the informal "1 standard error rule", we select $k = 2$, corresponding to the model using the variables `lcavol` and `lweight` on the [full training]{.orange} set. 


## Comments and computations

::: incremental

-   The correct way of doing cross-validation requires that the [best subset selection]{.blue} is performed on [every fold]{.blue}, possibly obtaining different "best" models with the same size. 

- Best subset selection is conceptually appealing, but it has a [major limitation]{.orange}. There are: $$
\sum_{k=1}^p \binom{n}{k} = 2^p
$$
models to consider, which is [computationally prohibitive]{.orange}!

- There exists very clever algorithms (i.e. [leaps and bounds]{.blue}) that makes this feasible for $p$ as large as $30$ or $40$.  

- Recently, [Bertsimas et al., 2016](https://projecteuclid.org/journals/annals-of-statistics/volume-44/issue-2/Best-subset-selection-via-a-modern-optimization-lens/10.1214/15-AOS1388.full) proposed the usage of a mixed integer optimization formulation, allowing $p$ to be in the order of hundreds. 

- Despite these advances, this problem remains [computationally very expensive]{.orange}. See also [Hastie et al. (2020)](https://projecteuclid.org/journals/statistical-science/volume-35/issue-4/Best-Subset-Forward-Stepwise-or-Lasso-Analysis-and-Recommendations-Based/10.1214/19-STS733.full) for additional considerations and comparisons.

:::

## Forward regression

::: callout-note
#### Forward regression

1.  Let $\mathcal{M}_0$ be the [null model]{.blue}, which contains no
    predictors, i.e. set $\hat{y}_i = \hat{\beta}_0 = \bar{y}$.

2.  For $k = 0,\dots, \min(n - 1, p - 1)$, do:

    i.  Consider the $p − k$ models that augment the predictors in $\mathcal{M}_k$ with [one additional covariate]{.orange}.

    ii. Identify the "best" model among the above $p - k$ competitors
        having the smallest $\text{MSE}_{k, \text{train}}$ and call it $\mathcal{M}_k$. 
:::


- Forward regression can be regarded as [greedy approximation]{.orange} of best subset selection. The advantage is that it is computationally feasible and it can be applied even when $p > n$. 

- By construction, it produces a [nested]{.blue} sequence of models.

- It can be shown that the identification of the [optimal new predictor]{.blue} can be efficiently computed using the [QR decomposition]{.orange} (see Exercises).

## Backward regression

::: callout-note
#### Backward regression

1.  Let $\mathcal{M}_p$ be the [full model]{.blue}, which contains all the
    predictors.

2.  For $k = p, p - 1,\dots, 1$, do:

    i.  Consider the $k$ models that contain [all but one]{.orange} of the predictors in $\mathcal{M}_k$, for a total of $k − 1$ predictors.

    ii. Identify the "best" model $\mathcal{M}_k$ among these $k$ models
        having the smallest $\text{MSE}_{k, \text{train}}$. 
:::

- Backward regression is another computationally feasible [greedy approximation]{.orange} of best subset selection. 

- Note that in this case we need  $p < n$, otherwise it is not possible to estimate the full model.

- Backward regression produces a [nested]{.blue} sequence of models. 

- It can be shown that the [dropped predictor]{.blue} is the one with the lowest absolute $Z$-score or, equivalently, the [highest p-value]{.orange} (see Exercises).

## Forward, backward and best subset

```{r}
fit_forward <- regsubsets(lpsa ~ ., data = prostate_train, method = "forward", nbest = 1, nvmax = p)
sum_forward <- summary(fit_forward)
fit_backward <- regsubsets(lpsa ~ ., data = prostate_train, method = "backward", nbest = 1, nvmax = p)
sum_backward <- summary(fit_backward)
```

```{r}
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: center

# Organization of the results for graphical purposes
data_stepwise <- data.frame(p = c(1:p, 1:p, 1:p), MSE = c(sum_forward$rss, 
                                                          sum_backward$rss, 
                                                          tapply(sum_best$rss, sum_best$p, min)) / n,
                            Stepwise = rep(c("Forward", "Backward", "Best subset"), each = p))
data_stepwise <- reshape2::melt(data_stepwise, id = c("p", "Stepwise"))
colnames(data_stepwise) <- c("p", "Stepwise", "MSE", "value")

ggplot(data = data_stepwise, aes(x = p, y = value, col = Stepwise)) +
  geom_line() +
  geom_point() +
  facet_grid(. ~ Stepwise) +
  theme_light() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = 0:9) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of covariates (p)") +
  ylab("MSE (training)")
```

- In the `prostate` dataset, forward, backward and best subset selection all gave exactly the [same path of solutions]{.orange} on the full training set.

# Principal components regression

## Centering the predictors I

::: incremental

- Let us consider a [reparametrization]{.blue} of the linear model using  [centered predictors]{.orange}
$$
\begin{aligned}
f(\bm{x}_i; \alpha, \beta) & = \alpha + \beta_1 (x_{i1} - \bar{x}_1) + \cdots + \beta_p (x_{ip} - \bar{x}_{ip}) = \alpha + (\bm{x}_i -\bar{\bm{x}})^T\beta \\
& = \alpha - \bar{\bm{x}}^T\beta + \bm{x}_i^T\beta = \beta_0 + \bm{x}_i^T\beta, \quad \text{where} \quad \beta_0 = \alpha - \bar{\bm{x}}^T\beta.
\end{aligned}
$$

- In this reparametrization the centered predictors are [orthogonal]{.blue} to the [intercept]{.blue}. Thus, the estimates for $(\alpha, \beta)$ can be computed [in two steps]{.orange}.

- The [estimate]{.orange} of the [intercept]{.orange} with centered predictors is $\hat{\alpha} = \bar{y}$. In fact: $$
\hat{\alpha} = \arg\min_{\alpha \in \mathbb{R}}\sum_{i=1}^n\{y_i - \alpha - (\bm{x}_i -\bar{\bm{x}})^T\beta\}^2 = \frac{1}{n}\sum_{i=1}^n\{y_i - (\bm{x}_i -\bar{\bm{x}})^T\beta\} = \frac{1}{n}\sum_{i=1}^ny_i.
$$

- Then, the [estimate of $\beta$]{.blue} can be obtained considering a linear model [without intercept]{.blue}:
$$
      f(\bm{x}_i; \beta) = (\bm{x}_i -\bar{\bm{x}})^T\beta,
$$
used to predict the [centered responses]{.blue} $y_i - \bar{y}$. 
:::

## Centering the predictors II

- From here on, and without loss of generality, we assume that everything is [centered]{.orange}:
$$
\sum_{i=1}^ny_i = 0, \qquad \sum_{i=1}^nx_{ij} = 0, \qquad j=1,\dots,p.
$$

- Hence, because of the previous results, we can focus on linear models [without intercept]{.blue}:
$$
f(\bm{x}_i; \beta) = x_{i1}\beta_1 + \cdots + x_{ip}\beta_p = \bm{x}_i^T\beta.
$$
- Under such an assumption the [covariance matrix]{.orange} of the data is simply
$$
S = \frac{1}{n} \bm{X}^T\bm{X}.
$$
- If in addition each variable has been [scaled]{.blue} by their standard deviations, then $n^{-1} \bm{X}^T\bm{X}$ corresponds to the [correlation]{.blue} matrix.

## Principal component analysis I


## Principal component analysis II

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center
pr <- princomp(prostate_train[,-9], cor = FALSE)
ggplot(data = data.frame(p = 1:p, vars = pr$sdev^2 / sum(pr$sdev^2)), aes(x= p, xmin = p, xmax = p, y=vars, ymax = vars, , ymin = 0)) + geom_pointrange() + theme_light() +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of principal components (p)") +
  ylab("Fraction of explained variance")
```


## Principal components regression

```{r}
library(pls)

resid_pcr <- matrix(0, n, p)

for (k in 1:10) {
  # Hold-out dataset
  y_k <- assessment(cv_fold$splits[[k]])$lpsa
  # MSE of the null model
  resid_pcr[complement(cv_fold$splits[[k]]), 1] <- y_k - predict(fit_null, assessment(cv_fold$splits[[k]]))
  # Fitting PCR (all the components at once)
  fit_pcr <- pcr(lpsa ~ ., data = analysis(cv_fold$splits[[k]]), center = TRUE, scale = FALSE)

  for (j in 2:p) {
    # Predictions
    y_hat <- predict(fit_pcr, newdata = assessment(cv_fold$splits[[k]]))[, , j - 1]
    # MSE of the best models for different values of p
    resid_pcr[complement(cv_fold$splits[[k]]), j] <- y_k - y_hat
  }
}
```


## PCR

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

data_cv <- data.frame(
  p = 1:p,
  MSE = apply(resid_pcr^2, 2, mean),
  SE = apply(resid_pcr^2, 2, function(x) sd(x) / sqrt(n))
)

se_rule <- data_cv$MSE[which.min(data_cv$MSE)] + data_cv$SE[which.min(data_cv$MSE)]
p_optimal <- which(data_cv$MSE < se_rule)[1]

ggplot(data = data_cv, aes(x = p, y = MSE)) +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymax = MSE + SE, ymin = MSE - SE)) +
  geom_hline(yintercept = se_rule, linetype = "dotted") +
  geom_vline(xintercept = p_optimal, linetype = "dotted") +
  theme_light() +
  scale_x_continuous(breaks = 1:9) +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of principal components (p)") +
  ylab("Mean squared error (10-fold cv)")
```



<!-- ## Summary of the estimated coefficients -->

<!-- ::: {style="font-size: 70%;"} -->

<!-- ```{r} -->

<!-- library(DT) -->

<!-- tab <- data.frame(OLS = rep(0, p), best_subset = rep(0, p), PCR = rep(0, p)) -->

<!-- rownames(tab) <- colnames(sum_best$which) -->

<!-- tab$OLS <- coef(lm(lpsa ~ ., data = prostate_train)) -->

<!-- tab$best_subset <- c(coef(lm(lpsa ~ lcavol + lweight, data = prostate_train)), rep(0, 6)) -->

<!-- # Principal components regression (PCR) -->

<!-- fit_pcr <- pcr(lpsa ~ ., data = prostate_train, center = TRUE, scale = FALSE) -->

<!-- beta <- c(coef(fit_pcr, 3)) -->

<!-- beta <- c(mean(prostate_train$lpsa) - colMeans(X[, -1]) %*% beta, beta) -->

<!-- tab$PCR <- beta -->

<!-- datatable(tab, colnames = c("OLS", "Best subset", "PCR"), options = list( -->

<!--   pageLength = 9, -->

<!--   dom = "t")) %>% -->

<!--   formatRound(columns = 1:3, digits = 3) %>% -->

<!--   formatStyle( -->

<!--     columns = 0, fontWeight = "bold" -->

<!--   ) %>% -->

<!--   formatStyle( -->

<!--     columns = 1:3, -->

<!--     backgroundColor = styleInterval(0, c("#FED8B1", "#DBE9FA")) -->

<!--   ) %>% -->

<!--   formatStyle( -->

<!--     columns = 1:3, -->

<!--     backgroundColor = styleEqual(0, c("white")) -->

<!--   ) -->

<!-- ``` -->

<!-- ::: -->


# Ridge regression

## A summary

|            | [Shrinkage]{.orange}                      | [Variable selection]{.orange}    |
|------------|--------------------------------|-----------------------|
| [Discrete]{.blue}   | Principal component regression | Best subset selection, stepwise |
| [Continuous]{.blue} | Ridge regression               | Relaxed Lasso           |


<!-- ## Ridge regression -->

<!-- ```{r} -->

<!-- df_ridge <- function(lambda, X) { -->

<!--   X_tilde <- scale(X, TRUE, FALSE) -->

<!--   d2 <- eigen(crossprod(X_tilde))$values -->

<!--   sum(d2 / (d2 + lambda)) -->

<!-- } -->

<!-- df_ridge <- Vectorize(df_ridge, vectorize.args = "lambda") -->

<!-- ``` -->

<!-- ```{r} -->

<!-- library(glmnet) -->

<!-- my_ridge <- function(X, y, lambda){ -->

<!--   n <- nrow(X) -->

<!--   p <- ncol(X) -->

<!--   y_mean <- mean(y) -->

<!--   y <- y - y_mean -->

<!--   X_mean <- colMeans(X) -->

<!--   X <- X - rep(1,n) %*% t(X_mean) -->

<!--   X_scale <- sqrt( diag( (1/n) * crossprod(X) ) ) -->

<!--   X <- X %*% diag( 1 / X_scale ) -->

<!--   beta_scaled <- solve(crossprod(X) + lambda*diag(rep(1,p)), t(X) %*% y)  -->

<!--   beta <- diag( 1 / X_scale ) %*% beta_scaled -->

<!--   beta0 <- y_mean - X_mean %*% beta -->

<!--   return(c(beta0, beta)) -->

<!-- } -->

<!-- l = 1 -->

<!-- my_ridge(X,y,lambda = l) -->

<!-- coef(glmnet(X, y, alpha=0, lambda = l/n, thresh = 1e-20)) -->

<!-- y_std <- scale(y, center=TRUE, scale=sd(y)*sqrt((n-1)/n) )[,] -->

<!-- my_ridge(X,y_std,lambda = l) -->

<!-- coef(glmnet(X, y_std, alpha=0, lambda = l/n, thresh = 1e-20)) -->

<!-- ``` -->

<!-- ```{r} -->

<!-- #  -->

<!-- #  -->

<!-- # lambda <- 100 -->

<!-- # XX <- X[,-1] #scale(X[, -1], TRUE, scale = apply(X[, -1], 2, function(x)  sqrt(mean(x^2) - mean(x)^2))) -->

<!-- # yy <- y #(y - mean(y)) / sqrt(mean(y^2) - mean(y)^2) -->

<!-- #  -->

<!-- # cv_ridge_fit <- cv.glmnet(XX, yy, family = "gaussian", standardize = FALSE, lambda = exp(seq(-10, 12, length = 500)), -->

<!-- #                     alpha = 0, thresh = 1e-16) -->

<!-- # plot(cv_ridge_fit) -->

<!-- #  -->

<!-- # c(solve((crossprod(XX) + lambda * diag(p-1)), crossprod(XX, yy))) -->

<!-- # c(coef(fit_ridge)[-1, ]) -->

<!-- ``` -->

<!-- ```{r} -->

<!-- # plot(log(cv_ridge_fit$lambda), cv_ridge_fit$cvm, type = "l") -->

<!-- # plot(1 + df_ridge(nrow(X[, -1]) * cv_ridge_fit$lambda, X[, -1]),  -->

<!-- #      cv_ridge_fit$cvm, type = "b", xlab = "Model complexity (p)", ylab = "MSE") -->

<!-- # lines(1 + df_ridge(nrow(X[, -1]) * cv_ridge_fit$lambda, X[, -1]),  -->

<!-- #       cv_ridge_fit$cvup, type = "b", xlab = "Model complexity (p)", ylab = "MSE", lty = "dashed", col = "red") -->

<!-- #  -->

<!-- # 1 + df_ridge(nrow(X[, -1]) * cv_ridge_fit$lambda.min, X[, -1]) -->

<!-- # 1 + df_ridge(nrow(X[, -1]) * cv_ridge_fit$lambda.1se, X[, -1]) -->

<!-- #  -->

<!-- # ridge_fit <- cv_ridge_fit$glmnet.fit -->

<!-- # coef(ridge_fit) -->

<!-- # plot(ridge_fit, , label = TRUE) -->

<!-- ``` -->



<!-- # Lasso, LARS, and elastic-net -->

<!-- ## Lasso -->

<!-- ::: columns -->

<!-- ::: {.column width="25%"} -->

<!-- ![](img/lasso.png) -->

<!-- ::: -->

<!-- ::: {.column width="75%"} -->

<!-- -   asdasd -->

<!-- ::: -->

<!-- ::: -->

<!-- ## Lasso -->

<!-- ```{r} -->

<!-- library(lars) -->

<!-- lambda <- 100 -->

<!-- XX <- scale(X[, -1], TRUE, scale = apply(X[, -1], 2, function(x)  sqrt(mean(x^2) - mean(x)^2))) -->

<!-- yy <- y#(y - mean(y)) / sqrt(mean(y^2) - mean(y)^2) -->

<!-- cv_lars <- cv.lars(x = XX, y = yy, K = 10, type = "lasso", mode = "step") -->

<!-- fit_lars <- lars(x = XX, y = yy, type ="lasso", normalize = FALSE) -->

<!-- cv_lasso_fit <- cv.glmnet(XX, yy, standardize = FALSE,  -->

<!--                           family = "gaussian", alpha = 1, nfolds = 10, -->

<!--                           lambda = 1 / n * fit_lars$lambda, thresh = 1e-16) -->

<!-- plot(cv_lasso_fit) -->

<!-- lasso_fit <- cv_lasso_fit$glmnet.fit -->

<!-- lambda_sel <- 4 -->

<!-- round(coef(fit_lars)[lambda_sel, ], 5) -->

<!-- round(coef(lasso_fit, mode = "lambda")[-1, lambda_sel], 5) -->

<!-- ``` -->

## References
