---
title: "Methods for model selection"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    df-print: tibble
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_C_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 150
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
    highlight-style: github
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 150
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_C.qmd", output = "../code/un_C.R")
styler:::style_file("../code/un_C.R")
```

::: columns
::: {.column width="25%"}
![](img/lasso.png){}
:::

::: {.column width="75%"}

-   In this unit we will cover the following [topics]{.orange}:

    -   Best subset regression
    -   Principal component regression
    -   Ridge regression
    -   Lasso, LARS, elastic-net
    
- The common thread among these topics is the so-called [variable selection]{.blue} problem.

- The issue we face is the presence of several [potentially irrelevant]{.blue} variables among the covariates.

- In the [extreme case]{.orange} $p > n$, is there even hope that we can fit a useful model?

:::
:::

## The `prostate` dataset

::: incremental

- The `prostate` cancer data investigates the relationship between the prostate-specific [antigen]{.orange} and a number of clinical measures, in men about to receive a prostatectomy.

- We want to [predict]{.blue} the logarithm of a [prostate-specific antigen]{.orange} (`lpsa`) as a function of:  
  - logarithm of the cancer volume (`lcavol`);
  - logarithm of the prostate weight (`lweight`);
  - age each man (`age`);
  - logarithm of the benign prostatic hyperplasia amount (`lbph`);
  - seminal vesicle invasion (`svi`), a binary variable;
  - logarithm of the capsular penetration (`lcp`);
  - Gleason score (`gleason`), an ordered categorical variable;
  - Percentage of Gleason scores $4$ and $5$ (`pgg45`).


:::

## A `glimpse` of the `prostate` dataset

- The dataset is [available online](https://hastie.su.domains/ElemStatLearn/datasets/prostate.data). A description is given in [Section 3.2.1]{.blue} of HTF (2009).


```{r}
#| message: false
rm(list = ls())
library(tidyverse)
prostate <- read.table("../data/prostate_data.txt")
glimpse(prostate)
```


```{r}
# Standardize the predictors, as in Tibshirani (1996)
which_vars <- which(colnames(prostate) %in% c("lpsa", "train"))
prostate[, -which_vars] <- apply(prostate[, -which_vars], 2, function(x) (x - mean(x)) / sd(x))

# Split in training and test
prostate_train <- filter(prostate, train) %>% select(-train)
prostate_test <- filter(prostate, train == FALSE) %>% select(-train)
```

- There are in total $8$ [variables]{.orange} that can be used to predict the antigen `lpsa`.

- The variable `train` splits the data into a training and test set, as in the textbook. 

- There are $n = 67$ observations in the [training]{.orange} set and $30$ in the [test]{.blue} set. 

## Correlation matrix of `prostate`

```{r}
#| fig-width: 15
#| fig-height: 7
#| fig-align: center
library(ggcorrplot)
corr <- cor(subset(prostate_train, select = -lpsa)) # Remove the outcome lpsa
ggcorrplot(corr,
  hc.order = FALSE,
  outline.col = "white",
  ggtheme = ggplot2::theme_bw,
  colors = c("#fc7d0b", "white", "#1170aa")
)
```


## The variable selection problem

::: incremental

- We consider a [linear model]{.orange} in which the response variable $Y_i$ (`lpsa`) is related to the
covariates through the function$$
    \mathbb{E}(Y_i) = f(\bm{x}_i; \alpha, \beta) = \alpha+ \beta_1 x_{i1} + \cdots + \beta_p x_{ip} =\alpha + \bm{x}_i^T\beta.
    $$ 

- If we use all the $p = 8$ available covariates, the estimated $f(\bm{x}; \hat{\alpha}, \hat{\beta})$ might have a [high variance]{.orange}, without important gain in term of bias. 

- In other words, the full model is likely to have a [large mean squared error]{.blue}. 

- Indeed, among the $p = 8$ variables of the `prostate` dataset, some are likely to be [irrelevant]{.blue}. For example, these variables might be [correlated]{.orange} and therefore [redundant]{.orange}. 


- Hence, we are looking for a [simpler model]{.orange} that has low mean squared error. Besides, 

:::

## Best subset regression I

## Best subset regression II

```{r}
library(leaps)

# Here I compute some basic quantities
X <- model.matrix(lpsa ~ ., data = prostate_train)
y <- prostate_train$lpsa
n <- nrow(X)
p <- ncol(X) # This includes the intercept as well
```


```{r}
fit_best <- regsubsets(lpsa ~ ., data = prostate_train, method = "exhaustive", nbest = 10, nvmax = p)
sum_best <- summary(fit_best)
sum_best$p <- rowSums(sum_best$which)
```

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

library(ggplot2)
library(ggthemes)
data_best_subset <- data.frame(p = sum_best$p, MSE = sum_best$rss / n)
data_best_subset <- reshape2::melt(data_best_subset, id = c("p"))
colnames(data_best_subset) <- c("p", "MSE", "value")

data_best_subset2 <- data.frame(p = unique(sum_best$p), MSE = tapply(sum_best$rss / n, sum_best$p, min))

ggplot(data = data_best_subset, aes(x = p, y = value)) +
  geom_point(alpha = 0.6) +
  theme_light() +
  theme(legend.position = "top") +
  geom_line(data = data_best_subset2, aes(x = p, y = MSE), col = "#fc7d0b") +
  geom_point(data = data_best_subset2, aes(x = p, y = MSE), col = "#fc7d0b", size = 2.5) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Model complexity (p)") +
  ylab("MSE (training)")
```

## ☠️ - The multiple testing problem

## Effective degrees of freedom

## Best subset regression: cross-validation

```{r}
library(rsample)

set.seed(123)
cv_fold <- vfold_cv(prostate_train, v = 10)
resid_subs <- matrix(0, n, p)

for (k in 1:10) {
  
  # Estimation of the null model
  fit_null <- lm(lpsa ~ 1, data = analysis(cv_fold$splits[[k]]))
  # Best subset using branch and bound
  fit <- regsubsets(lpsa ~ ., data = analysis(cv_fold$splits[[k]]), method = "exhaustive", nbest = 1, nvmax = p)
  sum <- summary(fit)

  # Hold-out quantities
  X_k <- as.matrix(cbind(1, assessment(cv_fold$splits[[k]]) %>% select(-lpsa)))
  y_k <- assessment(cv_fold$splits[[k]])$lpsa

  # MSE of the null model
  resid_subs[complement(cv_fold$splits[[k]]), 1] <- y_k - predict(fit_null, assessment(cv_fold$splits[[k]]))
  
  # MSE of the best models for different values of p
  for (j in 2:p) {
    y_hat <- X_k[, sum$which[j - 1, ]] %*% coef(fit, j - 1)
    resid_subs[complement(cv_fold$splits[[k]]), j] <- y_k - y_hat
  }
}
```

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

data_cv <- data.frame(
  p = 1:p,
  MSE = apply(resid_subs^2, 2, mean),
  SE = apply(resid_subs^2, 2, function(x) sd(x) / sqrt(n))
)

se_rule <- data_cv$MSE[which.min(data_cv$MSE)] + data_cv$SE[which.min(data_cv$MSE)]
p_optimal <- which(data_cv$MSE < se_rule)[1]

ggplot(data = data_cv, aes(x = p, y = MSE)) +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymax = MSE + SE, ymin = MSE - SE)) +
  geom_hline(yintercept = se_rule, linetype = "dotted") + 
  geom_vline(xintercept = p_optimal, linetype = "dotted") + 
  theme_light() +
  scale_x_continuous(breaks = 1:9) +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Model complexity (p)") +
  ylab("Mean squared error (10-fold cv)")
```



## Forward regression

## Backward regression

## Backward and forward regression I

```{r}
fit_forward <- regsubsets(lpsa ~ ., data = prostate_train, method = "forward", nbest = 1, nvmax = p)
sum_forward <- summary(fit_forward)
fit_backward <- regsubsets(lpsa ~ ., data = prostate_train, method = "backward", nbest = 1, nvmax = p)
sum_backward <- summary(fit_backward)
```

```{r}
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: center

# Organization of the results for graphical purposes
data_stepwise <- data.frame(p = c(2:p, 2:p, 2:p), MSE = c(sum_forward$rss, 
                                                          sum_backward$rss, 
                                                          tapply(sum_best$rss, sum_best$p, min)) / n,
                            Stepwise = rep(c("Forward", "Backward", "Best subset"), each = p - 1))
data_stepwise <- reshape2::melt(data_stepwise, id = c("p", "Stepwise"))
colnames(data_stepwise) <- c("p", "Stepwise", "MSE", "value")

ggplot(data = data_stepwise, aes(x = p, y = value, col = Stepwise)) +
  geom_line() +
  geom_point() +
  facet_grid(. ~ Stepwise) +
  theme_light() +
  theme(legend.position = "none") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Model complexity (p)") +
  ylab("MSE (training)")
```

## Comments

# Principal components

## Principal component analysis

## Principal components regression

```{r}
library(pls)
resid_pcr <- matrix(0, n, p)

for (k in 1:10) {
  # Hold-out dataset
  y_k <- assessment(cv_fold$splits[[k]])$lpsa
  # MSE of the null model
  resid_pcr[complement(cv_fold$splits[[k]]), 1] <- y_k - predict(fit_null, assessment(cv_fold$splits[[k]]))
  # Fitting PCR (all the components at once)
  fit_pcr <- pcr(lpsa ~ ., data = analysis(cv_fold$splits[[k]]), center = TRUE, scale = FALSE)

  for (j in 2:p) {
    # Predictions
    y_hat <- predict(fit_pcr, newdata = assessment(cv_fold$splits[[k]]))[, , j - 1]
    # MSE of the best models for different values of p
    resid_pcr[complement(cv_fold$splits[[k]]), j] <- y_k - y_hat
  }
}
```

## PCR

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

data_cv <- data.frame(
  p = 1:p,
  MSE = apply(resid_pcr^2, 2, mean),
  SE = apply(resid_pcr^2, 2, function(x) sd(x) / sqrt(n))
)

se_rule <- data_cv$MSE[which.min(data_cv$MSE)] + data_cv$SE[which.min(data_cv$MSE)]
p_optimal <- which(data_cv$MSE < se_rule)[1]

ggplot(data = data_cv, aes(x = p, y = MSE)) +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymax = MSE + SE, ymin = MSE - SE)) +
  geom_hline(yintercept = se_rule, linetype = "dotted") + 
  geom_vline(xintercept = p_optimal, linetype = "dotted") + 
  theme_light() +
  scale_x_continuous(breaks = 1:9) +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Model complexity (p)") +
  ylab("Mean squared error (10-fold cv)")
```



## Summary of the estimated coefficients

::: {style="font-size: 70%;"}
```{r}
library(DT)

tab <- data.frame(OLS = rep(0, p), best_subset = rep(0, p), PCR = rep(0, p))
rownames(tab) <- colnames(sum_best$which)

tab$OLS <- coef(lm(lpsa ~ ., data = prostate_train))
tab$best_subset <- c(coef(lm(lpsa ~ lcavol + lweight, data = prostate_train)), rep(0, 6))

# Principal components regression (PCR)
fit_pcr <- pcr(lpsa ~ ., data = prostate_train, center = TRUE, scale = FALSE)
beta <- c(coef(fit_pcr, 3))
beta <- c(mean(prostate_train$lpsa) - colMeans(X[, -1]) %*% beta, beta)
tab$PCR <- beta

datatable(tab, colnames = c("OLS", "Best subset", "PCR"), options = list(
  pageLength = 9,
  dom = "t")) %>%
  formatRound(columns = 1:3, digits = 3) %>%
  formatStyle(
    columns = 0, fontWeight = "bold"
  ) %>%
  formatStyle(
    columns = 1:3,
    backgroundColor = styleInterval(0, c("#FED8B1", "#DBE9FA"))
  ) %>%
  formatStyle(
    columns = 1:3,
    backgroundColor = styleEqual(0, c("white"))
  )
```
:::





# Ridge regression

## Ridge regression

```{r}
df_ridge <- function(lambda, X) {
  X_tilde <- scale(X, TRUE, FALSE)
  d2 <- eigen(crossprod(X_tilde))$values
  sum(d2 / (d2 + lambda))
}
df_ridge <- Vectorize(df_ridge, vectorize.args = "lambda")
```

```{r}
library(glmnet)
my_ridge <- function(X, y, lambda){
  n <- nrow(X)
  p <- ncol(X)
  y_mean <- mean(y)
  y <- y - y_mean
  X_mean <- colMeans(X)
  X <- X - rep(1,n) %*% t(X_mean)
  X_scale <- sqrt( diag( (1/n) * crossprod(X) ) )
  X <- X %*% diag( 1 / X_scale )
  beta_scaled <- solve(crossprod(X) + lambda*diag(rep(1,p)), t(X) %*% y) 
  beta <- diag( 1 / X_scale ) %*% beta_scaled
  beta0 <- y_mean - X_mean %*% beta
  return(c(beta0, beta))
}

l = 1
my_ridge(X,y,lambda = l)
coef(glmnet(X, y, alpha=0, lambda = l/n, thresh = 1e-20))

y_std <- scale(y, center=TRUE, scale=sd(y)*sqrt((n-1)/n) )[,]
my_ridge(X,y_std,lambda = l)
coef(glmnet(X, y_std, alpha=0, lambda = l/n, thresh = 1e-20))
```

```{r}
# 
# 
# lambda <- 100
# XX <- X[,-1] #scale(X[, -1], TRUE, scale = apply(X[, -1], 2, function(x)  sqrt(mean(x^2) - mean(x)^2)))
# yy <- y #(y - mean(y)) / sqrt(mean(y^2) - mean(y)^2)
# 
# cv_ridge_fit <- cv.glmnet(XX, yy, family = "gaussian", standardize = FALSE, lambda = exp(seq(-10, 12, length = 500)),
#                     alpha = 0, thresh = 1e-16)
# plot(cv_ridge_fit)
# 
# c(solve((crossprod(XX) + lambda * diag(p-1)), crossprod(XX, yy)))
# c(coef(fit_ridge)[-1, ])
```


```{r}
# plot(log(cv_ridge_fit$lambda), cv_ridge_fit$cvm, type = "l")
# plot(1 + df_ridge(nrow(X[, -1]) * cv_ridge_fit$lambda, X[, -1]), 
#      cv_ridge_fit$cvm, type = "b", xlab = "Model complexity (p)", ylab = "MSE")
# lines(1 + df_ridge(nrow(X[, -1]) * cv_ridge_fit$lambda, X[, -1]), 
#       cv_ridge_fit$cvup, type = "b", xlab = "Model complexity (p)", ylab = "MSE", lty = "dashed", col = "red")
# 
# 1 + df_ridge(nrow(X[, -1]) * cv_ridge_fit$lambda.min, X[, -1])
# 1 + df_ridge(nrow(X[, -1]) * cv_ridge_fit$lambda.1se, X[, -1])
# 
# ridge_fit <- cv_ridge_fit$glmnet.fit
# coef(ridge_fit)
# plot(ridge_fit, , label = TRUE)
```


## The wrong way of doing cross-validation

::: incremental

- Consider a regression problem with a [large number of predictors]{.blue}, as may arise, for example, in genomic or proteomic applications. 

- A typical strategy for analysis might be as follows:

  1. Screen the predictors: find a subset of "good" predictors that show fairly strong (univariate) correlation with the class labels;
  2. Using just this subset of predictors, build a regression model;
  3. Use cross-validation to estimate the unknown tuning parameters (i.e. degree of polynomials) and to estimate the prediction error of the final model.

- Is this a correct application of cross-validation? 

- If your reaction was "[this is  absolutely wrong!]{.orange}", it means you correctly understood the principles of cross-validation. 

- If you though this was an ok-ish idea, please read [Section 7.10.2]{.blue} of HTF (2009).
:::

# Lasso, LARS, and elastic-net

## Lasso

::: columns
::: {.column width="25%"}
![](img/lasso.png){}
:::

::: {.column width="75%"}
-   asdasd
:::
:::

## Lasso

```{r}
library(lars)
lambda <- 100
XX <- scale(X[, -1], TRUE, scale = apply(X[, -1], 2, function(x)  sqrt(mean(x^2) - mean(x)^2)))
yy <- y#(y - mean(y)) / sqrt(mean(y^2) - mean(y)^2)

cv_lars <- cv.lars(x = XX, y = yy, K = 10, type = "lasso", mode = "step")
fit_lars <- lars(x = XX, y = yy, type ="lasso", normalize = FALSE)

cv_lasso_fit <- cv.glmnet(XX, yy, standardize = FALSE, 
                          family = "gaussian", alpha = 1, nfolds = 10,
                          lambda = 1 / n * fit_lars$lambda, thresh = 1e-16)
plot(cv_lasso_fit)
lasso_fit <- cv_lasso_fit$glmnet.fit
lambda_sel <- 4
round(coef(fit_lars)[lambda_sel, ], 5)
round(coef(lasso_fit, mode = "lambda")[-1, lambda_sel], 5)
```


<!-- ## 3D plots -->



<!-- ```{r} -->
<!-- #| message: false -->
<!-- library(plotly) -->
<!-- logYears <- seq(from = min(prostate$logYears), to = max(prostate$logYears), length = 100) -->
<!-- Hits <- seq(from = min(prostate$Hits), to = max(prostate$Hits), length = 100) -->
<!-- lpsa <- matrix(predict(lm(lpsa ~ Hits + logYears, data = prostate_train), -->
<!--   newdata = data.frame(expand.grid(logYears = logYears, Hits = Hits, lpsa = NA)) -->
<!-- ), ncol = length(logYears)) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- #| fig-width: 6 -->
<!-- #| fig-height: 4 -->
<!-- #| fig-align: center -->
<!-- #| warning: false -->
<!-- #| message: false -->
<!-- plot_ly() %>% -->
<!--   add_surface( -->
<!--     x = ~logYears, y = ~Hits, z = ~lpsa, colors = "Reds", -->
<!--     contours = list( -->
<!--       x = list(show = TRUE, start = 0, end = 3, size = 0.15, color = "white"), -->
<!--       y = list(show = TRUE, start = 0, end = 200, size = 25, color = "white") -->
<!--     ) -->
<!--   ) %>% -->
<!--   add_markers(x = ~ prostate$logYears, y = ~ prostate$Hits, z = ~ prostate$lpsa, size = 0.15, marker = list(color = "black"), showlegend = FALSE) %>% -->
<!--   layout( -->
<!--     scene = list( -->
<!--       camera = list( -->
<!--         eye = list(x = 0.9, y = -2, z = 0.3) -->
<!--       ) -->
<!--     ) -->
<!--   ) %>% hide_colorbar() -->
<!-- ``` -->


## References
