---
title: "Shrinkage and variable selection"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    df-print: tibble
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_C_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 150
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
    highlight-style: github
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 150
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_C.qmd", output = "../code/un_C.R")
styler:::style_file("../code/un_C.R")
```

::: columns
::: {.column width="30%"}
![](img/cowboy.jpg)
:::

::: {.column width="70%"}
-   In this unit we will cover the following [topics]{.orange}:

    -   Best subset regression
    -   Principal component regression
    -   Ridge regression
    -   Lasso, LARS, elastic-net

-   The common themes are called [variable
    selection]{.blue} and [shrinkage estimation]{.orange}.

-   The issue we face is the presence of a high number $p$ of covariates that are [potentially
    irrelevant]{.blue}.
    
- This problem is quite challenging when the [ratio]{.blue} $p / n$ is [large]{.blue}. 

- In the [extreme case]{.orange} $p > n$, is there any hope to fit a meaningful model?
:::
:::

## The `prostate` dataset

-   The `prostate` cancer data investigates the relationship between the
    prostate-specific [antigen]{.orange} and a number of clinical
    measures, in men about to receive a prostatectomy.
    
. . .
    
- This [dataset](https://hastie.su.domains/ElemStatLearn/datasets/prostate.data) has been used in the [original paper]{.orange} by Tibshirani (1996) to present the lasso. A description is given in [Section 3.2.1]{.blue} of HTF (2009).


. . .

-   We want to [predict]{.blue} the logarithm of a [prostate-specific
    antigen]{.orange} (`lpsa`) as a function of:

    -   logarithm of the cancer volume (`lcavol`);
    -   logarithm of the prostate weight (`lweight`);
    -   age each man (`age`);
    -   logarithm of the benign prostatic hyperplasia amount (`lbph`);
    -   seminal vesicle invasion (`svi`), a binary variable;
    -   logarithm of the capsular penetration (`lcp`);
    -   Gleason score (`gleason`), an ordered categorical variable;
    -   Percentage of Gleason scores $4$ and $5$ (`pgg45`).



## A `glimpse` of the `prostate` dataset

-   Summarizing, there are in total $8$ [variables]{.orange} that can be used to predict the antigen `lpsa`. 

- We [centered]{.orange} and [standardized]{.blue} all the covariates before the training/test split.

-   There are $n = 67$ observations in the [training]{.orange} set and
    $30$ in the [test]{.blue} set.

. . .

::: {.panel-tabset}
## Original dataset

```{r}
#| message: false
rm(list = ls())
library(tidyverse)
prostate <- read.table("../data/prostate_data.txt")
glimpse(prostate)
```

## Standardized dataset

```{r}
# Standardize the predictors, as in Tibshirani (1996)
which_vars <- which(colnames(prostate) %in% c("lpsa", "train"))
prostate[, -which_vars] <- apply(prostate[, -which_vars], 2, function(x) (x - mean(x)) / sd(x))

# Split in training and test
prostate_train <- filter(prostate, train) %>% select(-train)
prostate_test <- filter(prostate, train == FALSE) %>% select(-train)

glimpse(prostate)
```
:::

<!-- -   The variable `train` splits the data into a training and test set, -->
<!--     as in the textbook. -->



## Correlation matrix of `prostate`

```{r}
#| fig-width: 15
#| fig-height: 7
#| fig-align: center
library(ggcorrplot)
corr <- cor(subset(prostate_train, select = -lpsa)) # Remove the outcome lpsa
ggcorrplot(corr,
  hc.order = FALSE,
  outline.col = "white",
  ggtheme = ggplot2::theme_bw,
  colors = c("#fc7d0b", "white", "#1170aa")
)
```

## The variable selection problem

-   We consider a [linear model]{.orange} in which the relationship between the response variable
    $Y_i$ (`lpsa`) and the covariates is modelled through the function$$
      f(\bm{x}_i; \beta_0, \beta) = \beta_0+ \beta_1 x_{i1} + \cdots + \beta_p x_{ip} =\beta_0 + \bm{x}_i^T\beta.
      $$ In this unit the [intercept]{.blue} $\beta_0$ will play a
    special role, therefore we use this slightly different notation
    compared to [Unit A](unit_A.html).

. . .

- Including a lot of covariates into the model is not necessarily a good thing!

-   Indeed, some variables are likely to be [irrelevant]{.blue}:

    -   they might be [correlated]{.orange} with other covariates and
        therefore [redundant]{.orange};
    -   they could be uncorrelated with the response `lpsa`.

. . .
    
-   If we use all the $p = 8$ available covariates, the estimated
    $f(\bm{x}; \hat{\beta_0}, \hat{\beta})$ might have a [high
    variance]{.orange}, without important gain in term of bias, i.e. a
    [large mean squared error]{.blue}.

-   We are looking for a [simpler model]{.orange} having,
    hopefully, a lower mean squared error.

## A naïve approach: (ab)using p-values

::: {style="font-size: 75%;"}
```{r}
tab <- data.frame(broom::tidy(lm(lpsa ~ ., data = prostate_train), conf.int = FALSE))
rownames(tab) <- tab[, 1]
tab <- t(as.matrix(tab[, -1]))
knitr::kable(tab, digits = 2)
```
:::

::: incremental
-   It is common practice to use the [p-values]{.orange}, e.g. those
    obtained through the `summary` function, to perform [model
    selection]{.blue} in a stepwise fashion.
<!-- -   A typical procedure is to omit "non significant" coefficients, refit -->
<!--     the model, and repeat this scheme until we obtain only "significant" -->
<!--     coefficients. -->
-   This is [not a good idea]{.orange}, at least when done without appropriate [multiplicity corrections]{.blue}.

-   The above p-values are meant to be
    used in the context of a single hypothesis testing problem, [not]{.orange} to
    make [iterative choices]{.orange}.

<!-- -   Such an iterative usage of "univariate" p-values is formally [incorrect]{.orange} -->
<!--     because it leads to the well-known [multiple testing -->
<!--     problem]{.blue}. -->
:::

## To explain or to predict?

-   "*All models are approximations. Essentially, all models are wrong,
    but some are useful*." George E. P. Box

. . .

-   If the
    [focus]{.blue} is on [prediction]{.blue}, we do not necessarily
    care about selecting the "true" set of parameters.

-   In many data mining problems, the focus is on
    [minimizing]{.orange} the [prediction errors]{.orange}.

. . .

-   Hence, often times we may [accept some bias]{.blue} (i.e. we use a
    "wrong" but useful model), if this leads to a [reduction in
    variance]{.orange}.

. . .

-   Besides, in certain cases it does not even make much sense to speak
    about the "true parameters".

-   For example, what if the true $f(\bm{x})$ were not linear? In this
    context, a [linear model]{.blue} is simply an approximation of the
    unknown $f(\bm{x})$ and hypothesis testing procedures are ill-posed.

## Best subset selection

-   Let us get back to our [variable selection problem]{.blue}.

-   In principle, we could perform an [exhaustive search]{.orange}
    considering all the $2^p$ possible models and then selecting the one
    having the best out-of-sample predictive performance.

. . .

::: callout-note
#### Best subset procedure

1.  Let $\mathcal{M}_0$ be the [null model]{.blue}, which contains no
    predictors, i.e. set $\hat{y}_i = \hat{\beta}_0 = \bar{y}$.

2.  For $k =1,\dots,p$, do:

    i.  Estimate [all]{.orange} the $\binom{p}{k}$ models that contain
        exactly $k$ covariates;

    ii. Identify the "best" model with $k$ covariates
        having the smallest $\text{MSE}_{k, \text{train}}$; call it $\mathcal{M}_k$.
:::

-   A model with more variables has lower [training]{.orange} error, namely $\text{MSE}_{k + 1, \text{train}} \le \text{MSE}_{k, \text{train}}$ by construction. Hence, the optimal subset size
    $k$ must be chosen e.g. via [cross-validation]{.blue}.

## Step 1. and 2. of best subset selection

```{r}
# Here I compute some basic quantities
X <- model.matrix(lpsa ~ ., data = prostate_train)[, -1]
y <- prostate_train$lpsa
n <- nrow(X)
p <- ncol(X) # This does not include the intercept
```

```{r}
library(leaps)
fit_best <- regsubsets(lpsa ~ ., data = prostate_train, method = "exhaustive", nbest = 40, nvmax = p)
sum_best <- summary(fit_best)
sum_best$p <- rowSums(sum_best$which) - 1 # Does not include the intercept here
```

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

library(ggplot2)
library(ggthemes)
data_best_subset <- data.frame(p = sum_best$p, MSE = sum_best$rss / n)
data_best_subset <- reshape2::melt(data_best_subset, id = c("p"))
colnames(data_best_subset) <- c("p", "MSE", "value")

data_best_subset2 <- data.frame(p = unique(sum_best$p), MSE = tapply(sum_best$rss / n, sum_best$p, min))

ggplot(data = data_best_subset, aes(x = p, y = value)) +
  geom_point() +
  theme_light() +
  theme(legend.position = "top") +
  geom_line(data = data_best_subset2, aes(x = p, y = MSE), col = "#fc7d0b") +
  geom_point(data = data_best_subset2, aes(x = p, y = MSE), col = "#fc7d0b", size = 1.5) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of covariates") +
  ylab("Mean squared error (training)")
```

## The "best" models $\mathcal{M}_1,\dots, \mathcal{M}_p$

- The output of the [best subset selection]{.orange}, on the training set is:

```{r}
summary(regsubsets(lpsa ~ ., data = prostate_train, method = "exhaustive", nbest = 1, nvmax = p))$outmat
```

::: incremental

- The above table means that the best model with $k = 1$ uses the variable `lcavol`, whereas when $k = 2$ the selected variables are `lcavol` and `lweight`, and so on and so forth. 

- Note that, in general, these models are [not]{.orange} necessarily [nested]{.orange}, i.e. a variable selected at step $k$ is not necessarily included at step $k +1$.  Here they are, but it is a coincidence.
:::

. . .

- What is the [optimal subset size]{.orange} $k$ in terms on out-of-sample mean squared error?


## The wrong way of doing cross-validation

::: incremental

-   Consider a regression problem with a [large number of predictors]{.orange} (relative to $n$) such as the `prostate` dataset.

-   A typical strategy for analysis might be as follows:

    1.  Screen the predictors: find a subset of "good" predictors that show fairly strong correlation with the response;

    2.  Using this subset of predictors (e.g. `lcavol`, `lweight` and `svi`), build a regression model;

    3.  Use cross-validation to estimate the prediction error of the model of the step 2.

-   Is this a correct application of cross-validation?

-   If your reaction was "[this is absolutely wrong!]{.orange}", it means you correctly understood the principles of cross-validation.

-   If you though this was an ok-ish idea, you may want to read [Section 7.10.2]{.blue} of HTF (2009), called "the wrong way of doing cross-validation". 

:::


## Step 3. of best subset selection via cross-validation

```{r}
library(rsample)

set.seed(123)
cv_fold <- vfold_cv(prostate_train, v = 10)
resid_subs <- matrix(0, n, p + 1)

for (k in 1:10) {
  # Estimation of the null model
  fit_null <- lm(lpsa ~ 1, data = analysis(cv_fold$splits[[k]]))
  # Best subset using branch and bound
  fit <- regsubsets(lpsa ~ ., data = analysis(cv_fold$splits[[k]]), method = "exhaustive", nbest = 1, nvmax = p)
  sum <- summary(fit)

  # Hold-out quantities
  X_k <- as.matrix(cbind(1, assessment(cv_fold$splits[[k]]) %>% select(-lpsa)))
  y_k <- assessment(cv_fold$splits[[k]])$lpsa

  # MSE of the null model
  resid_subs[complement(cv_fold$splits[[k]]), 1] <- y_k - predict(fit_null, assessment(cv_fold$splits[[k]]))

  # MSE of the best models for different values of p
  for (j in 2:(p + 1)) {
    y_hat <- X_k[, sum$which[j - 1, ]] %*% coef(fit, j - 1)
    resid_subs[complement(cv_fold$splits[[k]]), j] <- y_k - y_hat
  }
}
```

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

data_cv <- data.frame(
  p = 0:p,
  MSE = apply(resid_subs^2, 2, mean),
  SE = apply(resid_subs^2, 2, function(x) sd(x) / sqrt(n))
)

se_rule <- data_cv$MSE[which.min(data_cv$MSE)] + data_cv$SE[which.min(data_cv$MSE)]
p_optimal <- which(data_cv$MSE < se_rule)[1] - 1

ggplot(data = data_cv, aes(x = p, y = MSE)) +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymax = MSE + SE, ymin = MSE - SE)) +
  geom_hline(yintercept = se_rule, linetype = "dotted") +
  geom_vline(xintercept = p_optimal, linetype = "dotted") +
  theme_light() +
  scale_x_continuous(breaks = 0:9) +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of covariates") +
  ylab("Mean squared error (10-fold cv)")
```

- By applying the "1 standard error rule", we select $k = 2$, i.e. `lcavol` and `lweight`. 


## Comments and computations

::: incremental

-   The correct way of doing cross-validation requires that the [best subset selection]{.blue} is performed on [every fold]{.orange}, possibly obtaining different "best" models with the same size. 

- Best subset selection is conceptually appealing, but it has a [major limitation]{.orange}. There are $$
\sum_{k=1}^p \binom{n}{k} = 2^p
$$
models to consider, which is [computationally prohibitive]{.orange}!

- There exists algorithms (i.e. [leaps and bounds]{.blue}) that makes this feasible for $p \approx 30$.  

- Recently, [Bertsimas et al., 2016](https://projecteuclid.org/journals/annals-of-statistics/volume-44/issue-2/Best-subset-selection-via-a-modern-optimization-lens/10.1214/15-AOS1388.full) proposed the usage of a mixed integer optimization formulation, allowing $p$ to be in the order of hundreds. 

- Despite these advances, this problem remains [computationally very expensive]{.orange}. See also the recent paper [Hastie et al. (2020)](https://projecteuclid.org/journals/statistical-science/volume-35/issue-4/Best-Subset-Forward-Stepwise-or-Lasso-Analysis-and-Recommendations-Based/10.1214/19-STS733.full) for additional considerations and comparisons.

:::

## Forward regression


- Forward regression is [greedy approximation]{.orange} of best subset selection, that produces a sequence of [nested]{.blue} models. It is computationally feasible and can be applied when $p > n$. 

. . .

::: callout-note
#### Forward regression

1.  Let $\mathcal{M}_0$ be the [null model]{.blue}, which contains no
    predictors, i.e. set $\hat{y}_i = \hat{\beta}_0 = \bar{y}$.

2.  For $k = 0,\dots, \min(n - 1, p - 1)$, do:

    i.  Consider the $p − k$ models that augment the predictors in $\mathcal{M}_k$ with [one additional covariate]{.orange}.

    ii. Identify the "best" model among the above $p - k$ competitors
        having the smallest $\text{MSE}_{k, \text{train}}$ and call it $\mathcal{M}_k$. 
:::

. . .


- It can be shown that the identification of the [optimal new predictor]{.blue} can be efficiently computed using the [QR decomposition]{.orange} (see Exercises).

## Backward regression

- When $p < n$, an alternative greedy approach is [backward regression]{.orange}, which also produces a sequence of [nested]{.blue} models. 

. . .

::: callout-note
#### Backward regression

1.  Let $\mathcal{M}_p$ be the [full model]{.blue}, which contains all the
    predictors.

2.  For $k = p, p - 1,\dots, 1$, do:

    i.  Consider the $k$ models that contain [all but one]{.orange} of the predictors in $\mathcal{M}_k$, for a total of $k − 1$ predictors.

    ii. Identify the "best" model $\mathcal{M}_k$ among these $k$ models
        having the smallest $\text{MSE}_{k, \text{train}}$. 
:::

. . .

- It can be shown that the [dropped predictor]{.blue} is the one with the lowest absolute $Z$-score or, equivalently, the [highest p-value]{.orange} (see Exercises).

## Forward, backward and best subset

```{r}
fit_forward <- regsubsets(lpsa ~ ., data = prostate_train, method = "forward", nbest = 1, nvmax = p)
sum_forward <- summary(fit_forward)
fit_backward <- regsubsets(lpsa ~ ., data = prostate_train, method = "backward", nbest = 1, nvmax = p)
sum_backward <- summary(fit_backward)
```

```{r}
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: center

# Organization of the results for graphical purposes
data_stepwise <- data.frame(
  p = c(1:p, 1:p, 1:p), MSE = c(
    sum_forward$rss,
    sum_backward$rss,
    tapply(sum_best$rss, sum_best$p, min)
  ) / n,
  Stepwise = rep(c("Forward", "Backward", "Best subset"), each = p)
)
data_stepwise <- reshape2::melt(data_stepwise, id = c("p", "Stepwise"))
colnames(data_stepwise) <- c("p", "Stepwise", "MSE", "value")

ggplot(data = data_stepwise, aes(x = p, y = value, col = Stepwise)) +
  geom_line() +
  geom_point() +
  facet_grid(. ~ Stepwise) +
  theme_light() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = 0:9) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of covariates") +
  ylab("MSE (training)")
```

- In the `prostate` dataset, forward, backward and best subset selection all gave exactly the [same path of solutions]{.orange} on the full training set.

## Extension to generalized linear models

# Principal components regression

## Data compression

::: columns
::: {.column width="30%"}
![](img/cowboy_pixel.jpg)
:::

::: {.column width="70%"}

- At this point we established that [many covariates = many problems]{.orange}.

- Instead of selecting the "best" variables, let us consider a different perspective.

- We consider a [compressed]{.blue} version of the covariates that has smaller dimension $k$ but retains most information.

- Intuitively, we want to [reduce the variance]{.orange} by finding a good compression, without sacrificing too much bias. 

- The main statistical tool, unsurprisingly, will be the celebrated [principal components analysis]{.blue} (PCA).

- You should already know PCA as a tool for explorative analysis... We will focus on its usage for regression purposes. 
:::
:::

## Centering the predictors I

::: incremental

- Let us consider a [reparametrization]{.blue} of the linear model using  [centered predictors]{.orange}
$$
\begin{aligned}
f(\bm{x}_i; \alpha, \beta) & = \alpha + \beta_1 (x_{i1} - \bar{x}_1) + \cdots + \beta_p (x_{ip} - \bar{x}_{ip}) = \alpha + (\bm{x}_i -\bar{\bm{x}})^T\beta \\
& = \alpha - \bar{\bm{x}}^T\beta + \bm{x}_i^T\beta = \beta_0 + \bm{x}_i^T\beta, \quad \text{where} \quad \beta_0 = \alpha - \bar{\bm{x}}^T\beta.
\end{aligned}
$$

- In this reparametrization the centered predictors are [orthogonal]{.blue} to the [intercept]{.blue}. Thus, the estimates for $(\alpha, \beta)$ can be computed [in two steps]{.orange}.

- The [estimate]{.orange} of the [intercept]{.orange} with centered predictors is $\hat{\alpha} = \bar{y}$. In fact: $$
\hat{\alpha} = \arg\min_{\alpha \in \mathbb{R}}\sum_{i=1}^n\{y_i - \alpha - (\bm{x}_i -\bar{\bm{x}})^T\beta\}^2 = \frac{1}{n}\sum_{i=1}^n\{y_i - (\bm{x}_i -\bar{\bm{x}})^T\beta\} = \frac{1}{n}\sum_{i=1}^ny_i.
$$

- Then, the [estimate of $\beta$]{.blue} can be obtained considering a linear model [without intercept]{.blue}:
$$
      f(\bm{x}_i; \beta) = (\bm{x}_i -\bar{\bm{x}})^T\beta,
$$
used to predict the [centered responses]{.blue} $y_i - \bar{y}$. 
:::

## Centering the predictors II

::: incremental

- From here on, and without loss of generality, we assume that everything is [centered]{.orange}:
$$
\sum_{i=1}^ny_i = 0, \qquad \sum_{i=1}^nx_{ij} = 0, \qquad j=1,\dots,p.
$$

- Hence, because of the previous results, we can focus on linear models [without intercept]{.blue}:
$$
f(\bm{x}_i; \beta) = x_{i1}\beta_1 + \cdots + x_{ip}\beta_p = \bm{x}_i^T\beta.
$$
- Under such an assumption the [covariance matrix]{.orange} of the data is simply
$$
S = \frac{1}{n} \bm{X}^T\bm{X}.
$$
- If in addition each variable has been [scaled]{.blue} by their standard deviations, then $n^{-1} \bm{X}^T\bm{X}$ corresponds to the [correlation]{.blue} matrix.

:::

## Singular value decomposition (SVD)

- Let $\bm{X}$ be a $n \times p$ matrix. Then, its full form [singular value decomposition]{.orange} is: $$
\bm{X} = \bm{U} \bm{D} \bm{V}^T = \sum_{j=1}^m d_j \tilde{\bm{u}}_j \tilde{\bm{v}}_j^T,
$$
with $m =\min\{n, p\}$ and where:
  - the $n \times n$ matrix $\bm{U} = (\tilde{\bm{u}}_1, \dots, \tilde{\bm{u}}_n)$ is [orthogonal]{.orange}, namely: $\bm{U}^T \bm{U} = \bm{U}\bm{U}^T= I_n$; 
  - the $p \times p$ matrix $\bm{V} = (\tilde{\bm{v}}_1,\dots,\tilde{\bm{v}}_p)$ is [orthogonal]{.orange}, namely: $\bm{V}^T \bm{V} = \bm{V}\bm{V}^T= I_p$;
  - the $n \times p$ matrix $\bm{D}$ has [diagonal]{.blue} entries $[\bm{D}]_{jj} = d_j$, for $j=1,\dots,m$, and zero entries elsewhere;

- The real numbers $d_1 \ge d_2 \ge \cdots \ge d_m \ge 0$ are called [singular values]{.blue}.

- If one or more $d_j = 0$, then the matrix $\bm{X}$ is singular.

## Principal component analysis I

::: incremental

- Le us assume that $p < n$ and that $\text{rk}(\bm{X}) = p$, recalling that $\bm{X}$ is a [centered]{.orange} matrix.  

- Using SVD, the matrix $\bm{X}^T\bm{X}$ can be expressed as
$$
\bm{X}^T\bm{X} = (\bm{U} \bm{D} \bm{V}^T)^T \bm{U} \bm{D} \bm{V}^T = \bm{V} \bm{D}^T \textcolor{red}{\bm{U}^T \bm{U}} \bm{D} \bm{V}^T = \bm{V} \bm{\Delta}^2 \bm{V},
$$
where $\bm{\Delta}^2 = \bm{D}^T\bm{D}$ is a $p \times p$ [diagonal]{.blue} matrix with entries $d_1^2,\dots,d_p^2$.

- This equation is at the heart of [principal component analysis]{.blue} (PCA). Define the matrix
$$
\bm{Z} = \bm{X}\bm{V} = \bm{U}\bm{D},
$$
whose columns $\tilde{\bm{z}}_1,\dots,\tilde{\bm{z}}_p$ are called [principal components]{.orange}.

- The matrix $\bm{Z}$ is orthogonal, because $\bm{Z}^T\bm{Z} = \bm{D}^T\textcolor{red}{\bm{U}^T \bm{U}} \bm{D} = \bm{\Delta}^2$, which is diagonal. 

- Moreover, by definition the entries of $\bm{Z}$ are linear combination of the original variables:
$$
z_{ij} = x_{i1}v_{i1} + \cdots + x_{ip} v_{ip} = \bm{x}_i^T\tilde{\bm{v}}_j.
$$
The columns $\tilde{\bm{v}}_1,\dots,\tilde{\bm{v}}_p$ of $\bm{V}$ are sometimes called [loadings]{.blue}.




:::

## Principal component analysis II

::: incremental

- Principal components form an orthogonal basis of $\bm{X}$, but they are not a "random" choice and they do [not]{.orange} coincide with them [Gram-Schmidt]{.orange} basis of [Unit A](un_A.html#the-qr-decomposition-i). 

- Indeed, the [first principal component]{.blue} is the linear combination having [maximal variance]{.orange}:
$$
\tilde{\bm{v}}_1 = \arg\max_{\bm{v} \in \mathbb{R}^p} \text{var}(\bm{X}\bm{v})= \arg\max_{\bm{v} \in \mathbb{R}^p} \frac{1}{n} \bm{v}^T\bm{X}^T\bm{X} \bm{v}, \quad \text{ subject to } \quad \bm{v}^T \bm{v} = 1.
$$

- The [second principal component]{.blue} maximizes the variance under the additional constraint of being [orthogonal]{.orange} to the former. And so on and so forth. 

- The values $d_1^2 \ge d_2^2 \ge \dots \ge d_p^2 > 0$ are the [eigenvalues]{.orange} of $\bm{X}^T\bm{X}$ and correspond to the rescaled [variances]{.blue} of each principal component, that is $\text{var}(\tilde{\bm{z}}_j) = \tilde{\bm{z}}_j^T \tilde{\bm{z}}_j/n = d^2_j / n$.

- Hence, the quantity $d_j^2 / \sum_{j'=1}^p d_{j'}^2$ measures the amount of total variance captured by principal components.  

:::

## Principal component analysis: `prostate` data

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center
pr <- princomp(prostate_train[, -9], cor = FALSE)
ggplot(data = data.frame(p = 1:p, vars = pr$sdev^2 / sum(pr$sdev^2)), aes(x = p, xmin = p, xmax = p, y = vars, ymax = vars, ymin = 0)) +
  geom_pointrange() +
  theme_light() +
  scale_x_continuous(breaks = 1:9) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of principal components") +
  ylab("Fraction of explained variance")
```


## Principal components regression (PCR)


::: incremental

- We use the first $k \le p$ [principal components]{.blue} to predict the responses $Y_i$ via 
$$
f(\bm{z}_i; \gamma) = \gamma_1 z_{i1} + \cdots + \gamma_kz_{ik}, \qquad i=1,\dots,n,
$$

- Because of orthogonality, the least squares solution is straightforward to compute:
$$
\hat{\gamma}_j = \frac{\tilde{\bm{z}}_j^T\bm{y}}{\tilde{\bm{z}}_j^T\tilde{\bm{z}}_j} = \frac{1}{d_j^2}\tilde{\bm{z}}_j^T\bm{y}, \qquad j=1,\dots,k.
$$

- The principal components are in [order of importance]{.blue} and effectively [compressing the information]{.blue} contained in $\bm{X}$ using only $k \le p$ variables.

- When $k = p$ we are simply rotating the original matrix $\bm{X} = \bm{Z}\bm{V}$, i.e. performing [no compression]{.orange}. The predicted values coincide with OLS. 

- The number $k$ is a [complexity parameter]{.blue} which should be chosen via information criteria or cross-validation.

:::

## Selection of $k$: cross-validation

```{r}
#| message: false
library(pls)
resid_pcr <- matrix(0, n, p)

for (k in 1:10) {
  # Hold-out dataset
  y_k <- assessment(cv_fold$splits[[k]])$lpsa
  # MSE of the null model
  resid_pcr[complement(cv_fold$splits[[k]]), 1] <- y_k - predict(fit_null, assessment(cv_fold$splits[[k]]))
  # Fitting PCR (all the components at once)
  fit_pcr <- pcr(lpsa ~ ., data = analysis(cv_fold$splits[[k]]), center = TRUE, scale = FALSE)

  for (j in 2:p) {
    # Predictions
    y_hat <- predict(fit_pcr, newdata = assessment(cv_fold$splits[[k]]))[, , j - 1]
    # MSE of the best models for different values of p
    resid_pcr[complement(cv_fold$splits[[k]]), j] <- y_k - y_hat
  }
}
```

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

data_cv <- data.frame(
  p = 1:p,
  MSE = apply(resid_pcr^2, 2, mean),
  SE = apply(resid_pcr^2, 2, function(x) sd(x) / sqrt(n))
)

se_rule <- data_cv$MSE[which.min(data_cv$MSE)] + data_cv$SE[which.min(data_cv$MSE)]
p_optimal <- which(data_cv$MSE < se_rule)[1]

ggplot(data = data_cv, aes(x = p, y = MSE)) +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymax = MSE + SE, ymin = MSE - SE)) +
  geom_hline(yintercept = se_rule, linetype = "dotted") +
  geom_vline(xintercept = p_optimal, linetype = "dotted") +
  theme_light() +
  scale_x_continuous(breaks = 1:9) +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of principal components") +
  ylab("Mean squared error (10-fold cv)")
```



## Shrinkage effect of principal components I

::: incremental

- A closer look to the PCR solution reveals some interesting aspects. Recall that:
$$
\tilde{\bm{z}}_j = \bm{X}\tilde{\bm{v}}_j = d_j \tilde{\bm{u}}_j,  \qquad j=1,\dots,p.
$$

- The [predicted values]{.orange} $\hat{\bm{y}} = (\hat{y}_1,\dots,\hat{y}_n)$ of the PCR with $k$ components are:
$$
\hat{\bm{y}} = \sum_{j=1}^k \tilde{\bm{z}}_j \hat{\gamma}_j = \bm{X} \sum_{j=1}^k \tilde{\bm{v}}_j \hat{\gamma}_j = \bm{X}\hat{\beta}_\text{pcr}, \quad \text{ where } \quad \hat{\beta}_\text{pcr} = \sum_{j=1}^k \tilde{\bm{v}}_j \hat{\gamma}_j.
$$

- This representation highlights two important aspects:
  - It is possible to express the PCR solution in the original scale, for better [interpretability]{.orange};
  - The vector $\hat{\beta}_\text{pcr}$ is a [constrained solution]{.blue}, being a combination of $k \le p$ coefficients, therefore [reducing]{.orange} the [complexity]{.orange} of the model and [shrinking]{.blue} the coefficients. 
  
  
- When $k = 1$, then the $\hat{\beta}_\text{pcr}$ estimate coincide with the scaled loading vector $\hat{\beta}_\text{pcr} = \hat{\gamma}_1 \tilde{\bm{v}}_1$;
- When $k = p$ then the $\hat{\beta}_\text{pcr}$ coincides with [ordinary least squares]{.blue} (see Exercises). 

:::
 
## Shrinkage effect of principal components II

::: incremental

- The [variance]{.orange} of $\hat{\beta}_\text{pcr}$, assuming iid errors $\epsilon_i$ and after some algebraic manipulation, results: $$
\text{var}(\hat{\beta}_\text{pcr}) = \sigma^2\sum_{j=1}^k \frac{1}{d_j^2} \tilde{\bm{v}}_j\tilde{\bm{v}}_j^T.
$$
- Thus, if a [multicollinearity]{.blue} exists, then it appears as a principal component with very small variance, i.e. a small $d_j^2$. Its removal therefore drastically [reduces]{.orange} the [variance]{.orange} of $\hat{\beta}_\text{pcr}$. 

- Furthermore, the predicted values can be expressed as
$$
\hat{\bm{y}} = \sum_{j=1}^k \tilde{\bm{z}}_j \hat{\gamma}_j = \sum_{j=1}^k \tilde{\bm{z}}_j  \frac{\tilde{\bm{z}}_j^T\bm{y}}{\tilde{\bm{z}}_j^T\tilde{\bm{z}}_j} = \sum_{j=1}^k \textcolor{darkblue}{d_j} \tilde{\bm{u}}_j  \frac{\textcolor{darkblue}{d_j}}{\textcolor{darkblue}{d_j^2}} \frac{\tilde{\bm{u}}_j^T\bm{y}}{\textcolor{red}{\tilde{\bm{u}}_j^T\tilde{\bm{u}}_j}} = \sum_{j=1}^k \tilde{\bm{u}}_j \tilde{\bm{u}}_j^T \bm{y}.
$$
- The columns of $\bm{U}$, namely the vectors $\tilde{\bm{u}}_j$ are the [normalized principal components]{.blue}.

- Hence, we are shrinking the predictions towards the main [principal directions]{.orange}.

:::

## Shrinkage effect of principal components III

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center
fit_pcr <- pcr(lpsa ~ ., data = prostate_train, center = TRUE, scale = FALSE)

data_pcr <- reshape2::melt(coef(fit_pcr, 1:8))
colnames(data_pcr) <- c("Covariate", "lpsa", "Components", "value")
data_pcr$Components <- as.numeric(data_pcr$Components)
data_pcr <- rbind(data_pcr, data.frame(Covariate = data_pcr$Covariate[data_pcr$Components == 1], lpsa = NA, Components = 0, value = 0))
ggplot(data = data_pcr, aes(x = Components, y = value, col = Covariate)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.position = "top") +
  scale_x_continuous(breaks = 0:9) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of principal components") +
  ylab("Regression coefficients")
```



## Summary and next steps

- We have see so far two "discrete" methods: best subset selection and PCR

- Best subset selection perform variable selection, whereas PCR reduces the variance of the coefficients.

|            | [Shrinkage]{.orange}                      | [Variable selection]{.orange}    |
|------------|--------------------------------|-----------------------|
| [Discrete]{.blue}   | Principal component regression | Best subset selection, stepwise |
| [Continuous]{.blue} | Ridge regression               | Relaxed Lasso           |


# Ridge regression


<!-- ## Ridge regression -->

<!-- ```{r} -->

<!-- df_ridge <- function(lambda, X) { -->

<!--   X_tilde <- scale(X, TRUE, FALSE) -->

<!--   d2 <- eigen(crossprod(X_tilde))$values -->

<!--   sum(d2 / (d2 + lambda)) -->

<!-- } -->

<!-- df_ridge <- Vectorize(df_ridge, vectorize.args = "lambda") -->

<!-- ``` -->

<!-- ```{r} -->

<!-- library(glmnet) -->

<!-- my_ridge <- function(X, y, lambda){ -->

<!--   n <- nrow(X) -->

<!--   p <- ncol(X) -->

<!--   y_mean <- mean(y) -->

<!--   y <- y - y_mean -->

<!--   X_mean <- colMeans(X) -->

<!--   X <- X - rep(1,n) %*% t(X_mean) -->

<!--   X_scale <- sqrt( diag( (1/n) * crossprod(X) ) ) -->

<!--   X <- X %*% diag( 1 / X_scale ) -->

<!--   beta_scaled <- solve(crossprod(X) + lambda*diag(rep(1,p)), t(X) %*% y)  -->

<!--   beta <- diag( 1 / X_scale ) %*% beta_scaled -->

<!--   beta0 <- y_mean - X_mean %*% beta -->

<!--   return(c(beta0, beta)) -->

<!-- } -->

<!-- l = 1 -->

<!-- my_ridge(X,y,lambda = l) -->

<!-- coef(glmnet(X, y, alpha=0, lambda = l/n, thresh = 1e-20)) -->

<!-- y_std <- scale(y, center=TRUE, scale=sd(y)*sqrt((n-1)/n) )[,] -->

<!-- my_ridge(X,y_std,lambda = l) -->

<!-- coef(glmnet(X, y_std, alpha=0, lambda = l/n, thresh = 1e-20)) -->

<!-- ``` -->

<!-- ```{r} -->

<!-- #  -->

<!-- #  -->

<!-- # lambda <- 100 -->

<!-- # XX <- X[,-1] #scale(X[, -1], TRUE, scale = apply(X[, -1], 2, function(x)  sqrt(mean(x^2) - mean(x)^2))) -->

<!-- # yy <- y #(y - mean(y)) / sqrt(mean(y^2) - mean(y)^2) -->

<!-- #  -->

<!-- # cv_ridge_fit <- cv.glmnet(XX, yy, family = "gaussian", standardize = FALSE, lambda = exp(seq(-10, 12, length = 500)), -->

<!-- #                     alpha = 0, thresh = 1e-16) -->

<!-- # plot(cv_ridge_fit) -->

<!-- #  -->

<!-- # c(solve((crossprod(XX) + lambda * diag(p-1)), crossprod(XX, yy))) -->

<!-- # c(coef(fit_ridge)[-1, ]) -->

<!-- ``` -->

<!-- ```{r} -->

<!-- # plot(log(cv_ridge_fit$lambda), cv_ridge_fit$cvm, type = "l") -->

<!-- # plot(1 + df_ridge(nrow(X[, -1]) * cv_ridge_fit$lambda, X[, -1]),  -->

<!-- #      cv_ridge_fit$cvm, type = "b", xlab = "Model complexity", ylab = "MSE") -->

<!-- # lines(1 + df_ridge(nrow(X[, -1]) * cv_ridge_fit$lambda, X[, -1]),  -->

<!-- #       cv_ridge_fit$cvup, type = "b", xlab = "Model complexity", ylab = "MSE", lty = "dashed", col = "red") -->

<!-- #  -->

<!-- # 1 + df_ridge(nrow(X[, -1]) * cv_ridge_fit$lambda.min, X[, -1]) -->

<!-- # 1 + df_ridge(nrow(X[, -1]) * cv_ridge_fit$lambda.1se, X[, -1]) -->

<!-- #  -->

<!-- # ridge_fit <- cv_ridge_fit$glmnet.fit -->

<!-- # coef(ridge_fit) -->

<!-- # plot(ridge_fit, , label = TRUE) -->

<!-- ``` -->



<!-- # Lasso, LARS, and elastic-net -->

<!-- ## Lasso -->

<!-- ::: columns -->

<!-- ::: {.column width="25%"} -->

<!-- ![](img/lasso.png) -->

<!-- ::: -->

<!-- ::: {.column width="75%"} -->

<!-- -   asdasd -->

<!-- ::: -->

<!-- ::: -->

<!-- ## Lasso -->

<!-- ```{r} -->

<!-- library(lars) -->

<!-- lambda <- 100 -->

<!-- XX <- scale(X[, -1], TRUE, scale = apply(X[, -1], 2, function(x)  sqrt(mean(x^2) - mean(x)^2))) -->

<!-- yy <- y#(y - mean(y)) / sqrt(mean(y^2) - mean(y)^2) -->

<!-- cv_lars <- cv.lars(x = XX, y = yy, K = 10, type = "lasso", mode = "step") -->

<!-- fit_lars <- lars(x = XX, y = yy, type ="lasso", normalize = FALSE) -->

<!-- cv_lasso_fit <- cv.glmnet(XX, yy, standardize = FALSE,  -->

<!--                           family = "gaussian", alpha = 1, nfolds = 10, -->

<!--                           lambda = 1 / n * fit_lars$lambda, thresh = 1e-16) -->

<!-- plot(cv_lasso_fit) -->

<!-- lasso_fit <- cv_lasso_fit$glmnet.fit -->

<!-- lambda_sel <- 4 -->

<!-- round(coef(fit_lars)[lambda_sel, ], 5) -->

<!-- round(coef(lasso_fit, mode = "lambda")[-1, lambda_sel], 5) -->

<!-- ``` -->

<!-- ## Summary of the estimated coefficients -->

<!-- ::: {style="font-size: 70%;"} -->

<!-- ```{r} -->

<!-- library(DT) -->

<!-- tab <- data.frame(OLS = rep(0, p), best_subset = rep(0, p), PCR = rep(0, p)) -->

<!-- rownames(tab) <- colnames(sum_best$which) -->

<!-- tab$OLS <- coef(lm(lpsa ~ ., data = prostate_train)) -->

<!-- tab$best_subset <- c(coef(lm(lpsa ~ lcavol + lweight, data = prostate_train)), rep(0, 6)) -->

<!-- # Principal components regression (PCR) -->

<!-- fit_pcr <- pcr(lpsa ~ ., data = prostate_train, center = TRUE, scale = FALSE) -->

<!-- beta <- c(coef(fit_pcr, 3)) -->

<!-- beta <- c(mean(prostate_train$lpsa) - colMeans(X[, -1]) %*% beta, beta) -->

<!-- tab$PCR <- beta -->

<!-- datatable(tab, colnames = c("OLS", "Best subset", "PCR"), options = list( -->

<!--   pageLength = 9, -->

<!--   dom = "t")) %>% -->

<!--   formatRound(columns = 1:3, digits = 3) %>% -->

<!--   formatStyle( -->

<!--     columns = 0, fontWeight = "bold" -->

<!--   ) %>% -->

<!--   formatStyle( -->

<!--     columns = 1:3, -->

<!--     backgroundColor = styleInterval(0, c("#FED8B1", "#DBE9FA")) -->

<!--   ) %>% -->

<!--   formatStyle( -->

<!--     columns = 1:3, -->

<!--     backgroundColor = styleEqual(0, c("white")) -->

<!--   ) -->

<!-- ``` -->

<!-- ::: -->

# Generalized linear models

## References
