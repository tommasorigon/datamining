---
title: "Methods for model selection"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Universit√† degli Studi di Milano-Bicocca_"
execute:
  cache: false
format:
  revealjs:
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_C_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 150
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
    highlight-style: github
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 150
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_C.qmd", output = "../code/un_C.R")
styler:::style_file("../code/un_C.R")
```

::: columns
::: {.column width="25%"}
![](img/lasso.png){}
:::

::: {.column width="75%"}
-   In this unit we will cover the following [topics]{.orange}:

    -   Best subset regression
    -   Principal component regression
    -   Ridge regression
    -   Lasso, LARS, elastic-net
    
- The common thread among these topics is the so-called [variable selection]{.blue} problem.

- In other words: what do we do when we have many [irrelevant]{.orange} variables?
:::
:::

## The `Hitters` dataset

::: incremental

- We consider the `Hitters` dataset, which contains information about $n = 263$ Major League [Baseball players]{.blue} from the 1986 and 1987 seasons. 

- We are interested in predicting the [Salary]{.orange} of each player, as a function of 

- The dataset is available in the `ISLR` R package.

:::

## A `glimpse` of the `Hitters` dataset

```{r}
#| message: false
rm(list = ls())
library(ISLR)
library(tidyverse)
data(Hitters)
Hitters <- na.omit(Hitters)
Hitters <- mutate(Hitters, logYears = log10(Years), logSalary = log10(Salary)) %>% select(-c(Salary, Years))
glimpse(Hitters)
```
    

## 3D plots



```{r}
#| message: false
library(plotly)
logYears <- seq(from = min(Hitters$logYears), to = max(Hitters$logYears), length = 100)
Hits <- seq(from = min(Hitters$Hits), to = max(Hitters$Hits), length = 100)
logSalary <- matrix(predict(lm(logSalary ~ Hits + logYears, data = Hitters),
  newdata = data.frame(expand.grid(logYears = logYears, Hits = Hits, logSalary = NA))
), ncol = length(logYears))
```


```{r}
#| fig-width: 6
#| fig-height: 4
#| fig-align: center
#| warning: false
#| message: false
plot_ly() %>%
  add_surface(
    x = ~logYears, y = ~Hits, z = ~logSalary, colors = "Reds",
    contours = list(
      x = list(show = TRUE, start = 0, end = 3, size = 0.3, color = "white"),
      y = list(show = TRUE, start = 0, end = 200, size = 25, color = "white")
    )
  ) %>%
  add_markers(x = ~ Hitters$logYears, y = ~ Hitters$Hits, z = ~ Hitters$logSalary, size = 0.15, marker = list(color = "black"), showlegend = FALSE) %>%
  layout(
    scene = list(
      camera = list(
        eye = list(x = 0.9, y = -2, z = 0.3)
      )
    )
  ) %>% hide_colorbar()
```


## asd


```{r}
library(leaps)
fit <- regsubsets(logSalary ~ ., data = Hitters, method = "exhaustive", nbest = 1, nvmax = 20)
sum1 <- summary(fit)

plot(rowSums(sum1$which), sum1$cp)

fit <- regsubsets(logSalary ~ ., data = Hitters, method = "backward", nbest = 1, nvmax = 20)
sum1 <- summary(fit)
# plot(rowSums(sum1$which), sum1$cp)
```



# Best subset regression

## The wrong way of doing cross-validation

::: incremental

- Consider a regression problem with a [large number of predictors]{.blue}, as may arise, for example, in genomic or proteomic applications. 

- A typical strategy for analysis might be as follows:

  1. Screen the predictors: find a subset of "good" predictors that show fairly strong (univariate) correlation with the class labels;
  2. Using just this subset of predictors, build a regression model;
  3. Use cross-validation to estimate the unknown tuning parameters (i.e. degree of polynomials) and to estimate the prediction error of the final model.

- Is this a correct application of cross-validation? 

- If your reaction was "[this is  absolutely wrong!]{.orange}", it means you correctly understood the principles of cross-validation. 

- If you though this was an ok-ish idea, please read [Section 7.10.2]{.blue} of HTF (2009).
:::


# Ridge regression

# Lasso, LARS, and elastic-net

## Lasso

::: columns
::: {.column width="25%"}
![](img/lasso.png){}
:::

::: {.column width="75%"}
-   asdasd
:::
:::

## References
