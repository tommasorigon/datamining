---
title: "Shrinkage and variable selection"
subtitle: "Data Mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
execute:
  cache: false
filters: 
  - remove-pause.lua
format:
  revealjs:
    df-print: tibble
    auto-stretch: true
    center: true
    html-math-method: katex
    transition: none
    output-file: un_C_slides.html
    slide-number: true
    callout-appearance: minimal
    code-line-numbers: true
    theme: [default, ../template.css] # alternative themes (subset): default, night, dark
    embed-resources: false
    echo: false
    fig-dpi: 200
    # incremental: true  # Remove comment if you like incremental bullet points
    logo: img/logoB.png
    footer: "[Home page](https://tommasorigon.github.io/datamining)"
    highlight-style: github
  html:
    html-math-method: katex
    echo: false
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    code-fold: false
    code-summary: "Show the code"
    fig-dpi: 200
    highlight-style: github
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

## [Homepage](../index.html)

```{r}
#| warning: false
#| echo: false
#| include: false
#| message: false
#| purl: false

knitr::purl("un_C.qmd", output = "../code/un_C.R")
styler:::style_file("../code/un_C.R")
```

::: columns
::: {.column width="30%"}
![](img/cowboy.jpg)
:::

::: {.column width="70%"}
-   In this unit we will cover the following [topics]{.orange}:

    -   Best subset regression
    -   Principal component regression
    -   Ridge regression
    -   Lasso, LARS, elastic-net

-   The common themes are called [variable
    selection]{.blue} and [shrinkage estimation]{.orange}.

-   The issue we face is the presence of a high number $p$ of covariates that are [potentially
    irrelevant]{.blue}.
    
- This problem is quite challenging when the [ratio]{.blue} $p / n$ is [large]{.blue}. 

- In the [extreme case]{.orange} $p > n$, is there any hope to fit a meaningful model?
:::
:::

## The `prostate` dataset

-   The `prostate` cancer data investigates the relationship between the
    prostate-specific [antigen]{.orange} and a number of clinical
    measures, in men about to receive a prostatectomy.
    
. . .
    
- This [dataset](https://hastie.su.domains/ElemStatLearn/datasets/prostate.data) has been used in the [original paper]{.orange} by Tibshirani (1996) to present the lasso. A description is given in [Section 3.2.1]{.blue} of HTF (2009).


. . .

-   We want to [predict]{.blue} the logarithm of a [prostate-specific
    antigen]{.orange} (`lpsa`) as a function of:

    -   logarithm of the cancer volume (`lcavol`);
    -   logarithm of the prostate weight (`lweight`);
    -   age each man (`age`);
    -   logarithm of the benign prostatic hyperplasia amount (`lbph`);
    -   seminal vesicle invasion (`svi`), a binary variable;
    -   logarithm of the capsular penetration (`lcp`);
    -   Gleason score (`gleason`), an ordered categorical variable;
    -   Percentage of Gleason scores $4$ and $5$ (`pgg45`).



## A `glimpse` of the `prostate` dataset

-   Summarizing, there are in total $8$ [variables]{.orange} that can be used to predict the antigen `lpsa`. 

- We [centered]{.orange} and [standardized]{.blue} all the covariates before the training/test split.

-   There are $n = 67$ observations in the [training]{.orange} set and
    $30$ in the [test]{.blue} set.

. . .

::: {.panel-tabset}
## Original dataset

```{r}
#| message: false
rm(list = ls())
library(tidyverse)
prostate <- read.table("../data/prostate_data.txt")
glimpse(prostate)
```

## Standardized dataset

```{r}
# Standardize the predictors, as in Tibshirani (1996)
which_vars <- which(colnames(prostate) %in% c("lpsa", "train"))
prostate[, -which_vars] <- apply(prostate[, -which_vars], 2, function(x) (x - mean(x)) / sd(x))

# Split in training and test
prostate_train <- filter(prostate, train) %>% select(-train)
prostate_test <- filter(prostate, train == FALSE) %>% select(-train)

glimpse(prostate)
```
:::

<!-- -   The variable `train` splits the data into a training and test set, -->
<!--     as in the textbook. -->



## Correlation matrix of `prostate`

```{r}
#| fig-width: 15
#| fig-height: 7
#| fig-align: center
library(ggcorrplot)
corr <- cor(subset(prostate_train, select = -lpsa)) # Remove the outcome lpsa
ggcorrplot(corr,
  hc.order = FALSE,
  outline.col = "white",
  ggtheme = ggplot2::theme_bw,
  colors = c("#fc7d0b", "white", "#1170aa")
)
```

## The regression framework

::: incremental

- In this unit we will assume that the response variables $Y_i$ (`lpsa`) are obtained as
$$
Y_i = f(\bm{x}_i) + \epsilon_i, \qquad
$$
where $\epsilon_i$ are [iid]{.orange} random variables with $\mathbb{E}(\epsilon_i) = 0$ and $\text{var}(\epsilon_i) = \sigma^2$.

- Unless specifically stated, we will [not]{.orange} assume the [Gaussianity]{.orange} of the errors $\epsilon_i$ nor make any specific assumption about $f(\bm{x})$, which could be [non-linear]{.blue}.

- In practice, we [approximate]{.orange} the true $f(\bm{x})$ using a [linear model]{.blue}, e.g. by considering the following function$$
      f(\bm{x}_i; \beta_0, \beta) = \beta_0+ \beta_1 x_{i1} + \cdots + \beta_p x_{ip} =\beta_0 + \bm{x}_i^T\beta,
      $$ 
      in which the regression coefficients must be estimated.
      
- In this unit the [intercept]{.blue} $\beta_0$ will play a special role, therefore we use a slightly different notation compared to [Unit A](unit_A.html).

:::

## The variable selection problem

- Including a lot of covariates into the model is not necessarily a good thing!

. . .

- Indeed, some variables are likely to be [irrelevant]{.blue}:

    -   they might be [correlated]{.orange} with other covariates and
        therefore [redundant]{.orange};
    -   they could be uncorrelated with the response `lpsa`.

. . .
    
-   If we use all the $p = 8$ available covariates, the estimated
    $f(\bm{x}; \hat{\beta_0}, \hat{\beta})$ might have a [high
    variance]{.orange}, without important gain in term of bias, i.e. a
    [large mean squared error]{.blue}.

-   We are looking for a [simpler model]{.orange} having,
    hopefully, a lower mean squared error.
    
. . .

- These considerations are particularly relevant in cases in which $p > n$!

## A naïve approach: (ab)using p-values

::: {style="font-size: 75%;"}
```{r}
tab <- data.frame(broom::tidy(lm(lpsa ~ ., data = prostate_train), conf.int = FALSE))
rownames(tab) <- tab[, 1]
tab <- t(as.matrix(tab[, -1]))
knitr::kable(tab, digits = 2)
```
:::

. . .

- It is common practice to use the [p-values]{.orange} to perform [model selection]{.blue} in a stepwise fashion.
    
- However, what if the true $f(\bm{x})$ were not linear? 

- In many data mining problems, a [linear model]{.blue} is simply an approximation of the
    unknown $f(\bm{x})$ and hypothesis testing procedures are ill-posed.

. . .

- Even if the true function were linear, using p-values would [not be a good idea]{.orange}, at least if done without appropriate [multiplicity corrections]{.blue}.

-   The above p-values are meant to be used in the context of a single hypothesis testing problem, [not]{.orange} to make [iterative choices]{.orange}.

## The predictive culture

::: columns
::: {.column width="25%"}
![George E. P. Box](img/box.jpg){fig-align="left"}
:::

::: {.column width="75%"}
-   "*All models are approximations. Essentially, all models are wrong,
    but some are useful*." 
    
    [George E. P. Box]{.grey}


-   If the
    [focus]{.blue} is on [prediction]{.blue}, we do not necessarily
    care about selecting the "true" set of parameters.

-   In many data mining problems, the focus is on
    [minimizing]{.orange} the [prediction errors]{.orange}.

-   Hence, often times we may [accept some bias]{.blue} (i.e. we use a
    "wrong" but useful model), if this leads to a [reduction in
    variance]{.orange}.

:::
:::




# Best subset selection

## Best subset selection

-   Let us get back to our [variable selection problem]{.blue}.

-   In principle, we could perform an [exhaustive search]{.orange}
    considering all the $2^p$ possible models and then selecting the one
    having the best out-of-sample predictive performance.

. . .

::: callout-note
#### Best subset procedure

1.  Let $\mathcal{M}_0$ be the [null model]{.blue}, which contains no
    predictors, i.e. set $\hat{y}_i = \hat{\beta}_0 = \bar{y}$.

2.  For $k =1,\dots,p$, do:

    i.  Estimate [all]{.orange} the $\binom{p}{k}$ models that contain
        exactly $k$ covariates;

    ii. Identify the "best" model with $k$ covariates
        having the smallest $\text{MSE}_{k, \text{train}}$; call it $\mathcal{M}_k$.
:::

-   A model with more variables has lower [training]{.orange} error, namely $\text{MSE}_{k + 1, \text{train}} \le \text{MSE}_{k, \text{train}}$ by construction. Hence, the optimal subset size
    $k$ must be chosen e.g. via [cross-validation]{.blue}.

## Step 1. and 2. of best subset selection

```{r}
# Here I compute some basic quantities
X <- model.matrix(lpsa ~ ., data = prostate_train)[, -1]
y <- prostate_train$lpsa
n <- nrow(X)
p <- ncol(X) # This does not include the intercept
```

```{r}
library(leaps)
fit_best <- regsubsets(lpsa ~ ., data = prostate_train, method = "exhaustive", nbest = 40, nvmax = p)
sum_best <- summary(fit_best)
sum_best$p <- rowSums(sum_best$which) - 1 # Does not include the intercept here
```

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

library(ggplot2)
library(ggthemes)
data_best_subset <- data.frame(p = sum_best$p, MSE = sum_best$rss / n)
data_best_subset <- reshape2::melt(data_best_subset, id = c("p"))
colnames(data_best_subset) <- c("p", "MSE", "value")

data_best_subset2 <- data.frame(p = unique(sum_best$p), MSE = tapply(sum_best$rss / n, sum_best$p, min))

ggplot(data = data_best_subset, aes(x = p, y = value)) +
  geom_point() +
  theme_light() +
  theme(legend.position = "top") +
  geom_line(data = data_best_subset2, aes(x = p, y = MSE), col = "#fc7d0b") +
  geom_point(data = data_best_subset2, aes(x = p, y = MSE), col = "#fc7d0b", size = 1.5) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of covariates") +
  ylab("Mean squared error (training)")
```

## The "best" models $\mathcal{M}_1,\dots, \mathcal{M}_p$

- The output of the [best subset selection]{.orange}, on the training set is:

```{r}
summary(regsubsets(lpsa ~ ., data = prostate_train, method = "exhaustive", nbest = 1, nvmax = p))$outmat
```

::: incremental

- The above table means that the best model with $k = 1$ uses the variable `lcavol`, whereas when $k = 2$ the selected variables are `lcavol` and `lweight`, and so on and so forth. 

- Note that, in general, these models are [not]{.orange} necessarily [nested]{.orange}, i.e. a variable selected at step $k$ is not necessarily included at step $k +1$.  Here they are, but it is a coincidence.
:::

. . .

- What is the [optimal subset size]{.orange} $k$ in terms on out-of-sample mean squared error?


## The wrong way of doing cross-validation

::: incremental

-   Consider a regression problem with a [large number of predictors]{.orange} (relative to $n$) such as the `prostate` dataset.

-   A typical strategy for analysis might be as follows:

    1.  Screen the predictors: find a subset of "good" predictors that show fairly strong correlation with the response;

    2.  Using this subset of predictors (e.g. `lcavol`, `lweight` and `svi`), build a regression model;

    3.  Use cross-validation to estimate the prediction error of the model of the step 2.

-   Is this a correct application of cross-validation?

-   If your reaction was "[this is absolutely wrong!]{.orange}", it means you correctly understood the principles of cross-validation.

-   If you though this was an ok-ish idea, you may want to read [Section 7.10.2]{.blue} of HTF (2009), called "the wrong way of doing cross-validation". 

:::


## Step 3. of best subset selection via cross-validation

```{r}
library(rsample)

set.seed(123)
cv_fold <- vfold_cv(prostate_train, v = 10)
resid_subs <- matrix(0, n, p + 1)

for (k in 1:10) {
  # Estimation of the null model
  fit_null <- lm(lpsa ~ 1, data = analysis(cv_fold$splits[[k]]))
  # Best subset using branch and bound
  fit <- regsubsets(lpsa ~ ., data = analysis(cv_fold$splits[[k]]), method = "exhaustive", nbest = 1, nvmax = p)
  sum <- summary(fit)

  # Hold-out quantities
  X_k <- as.matrix(cbind(1, assessment(cv_fold$splits[[k]]) %>% select(-lpsa)))
  y_k <- assessment(cv_fold$splits[[k]])$lpsa

  # MSE of the null model
  resid_subs[complement(cv_fold$splits[[k]]), 1] <- y_k - predict(fit_null, assessment(cv_fold$splits[[k]]))

  # MSE of the best models for different values of p
  for (j in 2:(p + 1)) {
    y_hat <- X_k[, sum$which[j - 1, ]] %*% coef(fit, j - 1)
    resid_subs[complement(cv_fold$splits[[k]]), j] <- y_k - y_hat
  }
}
```

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

data_cv <- data.frame(
  p = 0:p,
  MSE = apply(resid_subs^2, 2, mean),
  SE = apply(resid_subs^2, 2, function(x) sd(x) / sqrt(n))
)

se_rule <- data_cv$MSE[which.min(data_cv$MSE)] + data_cv$SE[which.min(data_cv$MSE)]
p_optimal <- which(data_cv$MSE < se_rule)[1] - 1

ggplot(data = data_cv, aes(x = p, y = MSE)) +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymax = MSE + SE, ymin = MSE - SE)) +
  geom_hline(yintercept = se_rule, linetype = "dotted") +
  geom_vline(xintercept = p_optimal, linetype = "dotted") +
  theme_light() +
  scale_x_continuous(breaks = 0:9) +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of covariates") +
  ylab("Mean squared error (10-fold cv)")
```

- By applying the "1 standard error rule", we select $k = 2$, i.e. `lcavol` and `lweight`. 


## Comments and computations

::: incremental

-   The correct way of doing cross-validation requires that the [best subset selection]{.blue} is performed on [every fold]{.orange}, possibly obtaining different "best" models with the same size. 

- Best subset selection is conceptually appealing, but it has a [major limitation]{.orange}. There are $$
\sum_{k=1}^p \binom{n}{k} = 2^p
$$
models to consider, which is [computationally prohibitive]{.orange}!

- There exists algorithms (i.e. [leaps and bounds]{.blue}) that makes this feasible for $p \approx 30$.  

- Recently, [Bertsimas et al., 2016](https://projecteuclid.org/journals/annals-of-statistics/volume-44/issue-2/Best-subset-selection-via-a-modern-optimization-lens/10.1214/15-AOS1388.full) proposed the usage of a mixed integer optimization formulation, allowing $p$ to be in the order of hundreds. 

- Despite these advances, this problem remains [computationally very expensive]{.orange}. See also the recent paper [Hastie et al. (2020)](https://projecteuclid.org/journals/statistical-science/volume-35/issue-4/Best-Subset-Forward-Stepwise-or-Lasso-Analysis-and-Recommendations-Based/10.1214/19-STS733.full) for additional considerations and comparisons.

:::

## Forward regression


- Forward regression is [greedy approximation]{.orange} of best subset selection, that produces a sequence of [nested]{.blue} models. It is computationally feasible and can be applied when $p > n$. 

. . .

::: callout-note
#### Forward regression

1.  Let $\mathcal{M}_0$ be the [null model]{.blue}, which contains no
    predictors, i.e. set $\hat{y}_i = \hat{\beta}_0 = \bar{y}$.

2.  For $k = 0,\dots, \min(n - 1, p - 1)$, do:

    i.  Consider the $p − k$ models that augment the predictors in $\mathcal{M}_k$ with [one additional covariate]{.orange}.

    ii. Identify the "best" model among the above $p - k$ competitors
        having the smallest $\text{MSE}_{k, \text{train}}$ and call it $\mathcal{M}_k$. 
:::

. . .


- It can be shown that the identification of the [optimal new predictor]{.blue} can be efficiently computed using the [QR decomposition]{.orange} (see Exercises).

## Backward regression

- When $p < n$, an alternative greedy approach is [backward regression]{.orange}, which also produces a sequence of [nested]{.blue} models. 

. . .

::: callout-note
#### Backward regression

1.  Let $\mathcal{M}_p$ be the [full model]{.blue}, which contains all the
    predictors.

2.  For $k = p, p - 1,\dots, 1$, do:

    i.  Consider the $k$ models that contain [all but one]{.orange} of the predictors in $\mathcal{M}_k$, for a total of $k − 1$ predictors.

    ii. Identify the "best" model $\mathcal{M}_k$ among these $k$ models
        having the smallest $\text{MSE}_{k, \text{train}}$. 
:::

. . .

- It can be shown that the [dropped predictor]{.blue} is the one with the lowest absolute $Z$-score or, equivalently, the [highest p-value]{.orange} (see Exercises).

## Forward, backward and best subset

```{r}
fit_forward <- regsubsets(lpsa ~ ., data = prostate_train, method = "forward", nbest = 1, nvmax = p)
sum_forward <- summary(fit_forward)
fit_backward <- regsubsets(lpsa ~ ., data = prostate_train, method = "backward", nbest = 1, nvmax = p)
sum_backward <- summary(fit_backward)
```

```{r}
#| fig-width: 10
#| fig-height: 3.5
#| fig-align: center

# Organization of the results for graphical purposes
data_stepwise <- data.frame(
  p = c(1:p, 1:p, 1:p), MSE = c(
    sum_forward$rss,
    sum_backward$rss,
    tapply(sum_best$rss, sum_best$p, min)
  ) / n,
  Stepwise = rep(c("Forward", "Backward", "Best subset"), each = p)
)
data_stepwise <- reshape2::melt(data_stepwise, id = c("p", "Stepwise"))
colnames(data_stepwise) <- c("p", "Stepwise", "MSE", "value")

ggplot(data = data_stepwise, aes(x = p, y = value, col = Stepwise)) +
  geom_line() +
  geom_point() +
  facet_grid(. ~ Stepwise) +
  theme_light() +
  theme(legend.position = "none") +
  scale_x_continuous(breaks = 0:9) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of covariates") +
  ylab("MSE (training)")
```

- In the `prostate` dataset, forward, backward and best subset selection all gave exactly the [same path of solutions]{.orange} on the full training set.

# Principal components regression

## Data compression

::: columns
::: {.column width="30%"}
![](img/cowboy_pixel.jpg)
:::

::: {.column width="70%"}

- At this point we established that [many covariates = many problems]{.orange}.

- Instead of selecting the "best" variables, let us consider a different perspective.

- We consider a [compressed]{.blue} version of the covariates that has smaller dimension $k$ but retains most information.

- Intuitively, we want to [reduce the variance]{.orange} by finding a good compression, without sacrificing too much bias. 

- The main statistical tool, unsurprisingly, will be the celebrated [principal components analysis]{.blue} (PCA).

- You should already know PCA as a tool for explorative analysis... We will focus on its usage for regression purposes. 
:::
:::

## Centering the predictors I

::: incremental

- Let us consider a [reparametrization]{.blue} of the linear model using  [centered predictors]{.orange}
$$
\begin{aligned}
f(\bm{x}_i; \alpha, \beta) & = \alpha + \beta_1 (x_{i1} - \bar{x}_1) + \cdots + \beta_p (x_{ip} - \bar{x}_{ip}) = \alpha + (\bm{x}_i -\bar{\bm{x}})^T\beta \\
& = \alpha - \bar{\bm{x}}^T\beta + \bm{x}_i^T\beta = \beta_0 + \bm{x}_i^T\beta, \quad \text{where} \quad \beta_0 = \alpha - \bar{\bm{x}}^T\beta.
\end{aligned}
$$

- In this reparametrization the centered predictors are [orthogonal]{.blue} to the [intercept]{.blue}. Thus, the estimates for $(\alpha, \beta)$ can be computed [in two steps]{.orange}.

- The [estimate]{.orange} of the [intercept]{.orange} with centered predictors is $\hat{\alpha} = \bar{y}$. In fact: $$
\hat{\alpha} = \arg\min_{\alpha \in \mathbb{R}}\sum_{i=1}^n\{y_i - \alpha - (\bm{x}_i -\bar{\bm{x}})^T\beta\}^2 = \frac{1}{n}\sum_{i=1}^n\{y_i - (\bm{x}_i -\bar{\bm{x}})^T\beta\} = \frac{1}{n}\sum_{i=1}^ny_i.
$$

- Then, the [estimate of $\beta$]{.blue} can be obtained considering a linear model [without intercept]{.blue}:
$$
      f(\bm{x}_i; \beta) = (\bm{x}_i -\bar{\bm{x}})^T\beta,
$$
used to predict the [centered responses]{.blue} $y_i - \bar{y}$. 
:::

## Centering the predictors II

::: incremental

- From here on, and without loss of generality, we assume that everything is [centered]{.orange}:
$$
\sum_{i=1}^ny_i = 0, \qquad \sum_{i=1}^nx_{ij} = 0, \qquad j=1,\dots,p.
$$

- Hence, because of the previous results, we can focus on linear models [without intercept]{.blue}:
$$
f(\bm{x}_i; \beta) = x_{i1}\beta_1 + \cdots + x_{ip}\beta_p = \bm{x}_i^T\beta.
$$
- Under such an assumption the [covariance matrix]{.orange} of the data is simply
$$
S = \frac{1}{n} \bm{X}^T\bm{X}.
$$
- If in addition each variable has been [scaled]{.blue} by their standard deviations, then $n^{-1} \bm{X}^T\bm{X}$ corresponds to the [correlation]{.blue} matrix.

:::

## Singular value decomposition (SVD)

- Let $\bm{X}$ be a $n \times p$ matrix. Then, its full form [singular value decomposition]{.orange} is: $$
\bm{X} = \bm{U} \bm{D} \bm{V}^T = \sum_{j=1}^m d_j \tilde{\bm{u}}_j \tilde{\bm{v}}_j^T,
$$
with $m =\min\{n, p\}$ and where:
  - the $n \times n$ matrix $\bm{U} = (\tilde{\bm{u}}_1, \dots, \tilde{\bm{u}}_n)$ is [orthogonal]{.orange}, namely: $\bm{U}^T \bm{U} = \bm{U}\bm{U}^T= I_n$; 
  - the $p \times p$ matrix $\bm{V} = (\tilde{\bm{v}}_1,\dots,\tilde{\bm{v}}_p)$ is [orthogonal]{.orange}, namely: $\bm{V}^T \bm{V} = \bm{V}\bm{V}^T= I_p$;
  - the $n \times p$ matrix $\bm{D}$ has [diagonal]{.blue} entries $[\bm{D}]_{jj} = d_j$, for $j=1,\dots,m$, and zero entries elsewhere;

- The real numbers $d_1 \ge d_2 \ge \cdots \ge d_m \ge 0$ are called [singular values]{.blue}.

- If one or more $d_j = 0$, then the matrix $\bm{X}$ is singular.

## Principal component analysis I

::: incremental

- Le us assume that $p < n$ and that $\text{rk}(\bm{X}) = p$, recalling that $\bm{X}$ is a [centered]{.orange} matrix.  

- Using SVD, the matrix $\bm{X}^T\bm{X}$ can be expressed as
$$
\bm{X}^T\bm{X} = (\bm{U} \bm{D} \bm{V}^T)^T \bm{U} \bm{D} \bm{V}^T = \bm{V} \bm{D}^T \textcolor{red}{\bm{U}^T \bm{U}} \bm{D} \bm{V}^T = \bm{V} \bm{\Delta}^2 \bm{V}^T,
$$
where $\bm{\Delta}^2 = \bm{D}^T\bm{D}$ is a $p \times p$ [diagonal]{.blue} matrix with entries $d_1^2,\dots,d_p^2$.

- This equation is at the heart of [principal component analysis]{.blue} (PCA). Define the matrix
$$
\bm{Z} = \bm{X}\bm{V} = \bm{U}\bm{D},
$$
whose columns $\tilde{\bm{z}}_1,\dots,\tilde{\bm{z}}_p$ are called [principal components]{.orange}.

- The matrix $\bm{Z}$ is orthogonal, because $\bm{Z}^T\bm{Z} = \bm{D}^T\textcolor{red}{\bm{U}^T \bm{U}} \bm{D} = \bm{\Delta}^2$, which is diagonal. 

- Moreover, by definition the entries of $\bm{Z}$ are linear combination of the original variables:
$$
z_{ij} = x_{i1}v_{i1} + \cdots + x_{ip} v_{ip} = \bm{x}_i^T\tilde{\bm{v}}_j.
$$
The columns $\tilde{\bm{v}}_1,\dots,\tilde{\bm{v}}_p$ of $\bm{V}$ are sometimes called [loadings]{.blue}.




:::

## Principal component analysis II

::: incremental

- Principal components form an orthogonal basis of $\bm{X}$, but they are not a "random" choice and they do [not]{.orange} coincide with them [Gram-Schmidt]{.orange} basis of [Unit A](un_A.html#the-qr-decomposition-i). 

- Indeed, the [first principal component]{.blue} is the linear combination having [maximal variance]{.orange}:
$$
\tilde{\bm{v}}_1 = \arg\max_{\bm{v} \in \mathbb{R}^p} \text{var}(\bm{X}\bm{v})= \arg\max_{\bm{v} \in \mathbb{R}^p} \frac{1}{n} \bm{v}^T\bm{X}^T\bm{X} \bm{v}, \quad \text{ subject to } \quad \bm{v}^T \bm{v} = 1.
$$

- The [second principal component]{.blue} maximizes the variance under the additional constraint of being [orthogonal]{.orange} to the former. And so on and so forth. 

- The values $d_1^2 \ge d_2^2 \ge \dots \ge d_p^2 > 0$ are the [eigenvalues]{.orange} of $\bm{X}^T\bm{X}$ and correspond to the rescaled [variances]{.blue} of each principal component, that is $\text{var}(\tilde{\bm{z}}_j) = \tilde{\bm{z}}_j^T \tilde{\bm{z}}_j/n = d^2_j / n$.

- Hence, the quantity $d_j^2 / \sum_{j'=1}^p d_{j'}^2$ measures the amount of total variance captured by principal components.  

:::

## Principal component analysis: `prostate` data

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center
pr <- princomp(prostate_train[, -9], cor = FALSE)
ggplot(data = data.frame(p = 1:p, vars = pr$sdev^2 / sum(pr$sdev^2)), aes(x = p, xmin = p, xmax = p, y = vars, ymax = vars, ymin = 0)) +
  geom_pointrange() +
  theme_light() +
  scale_x_continuous(breaks = 1:9) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of principal components") +
  ylab("Fraction of explained variance")
```


## Principal components regression (PCR)


::: incremental

- We use the first $k \le p$ [principal components]{.blue} to predict the responses $Y_i$ via 
$$
f(\bm{z}_i; \gamma) = \gamma_1 z_{i1} + \cdots + \gamma_kz_{ik}, \qquad i=1,\dots,n,
$$

- Because of orthogonality, the least squares solution is straightforward to compute:
$$
\hat{\gamma}_j = \frac{\tilde{\bm{z}}_j^T\bm{y}}{\tilde{\bm{z}}_j^T\tilde{\bm{z}}_j} = \frac{1}{d_j^2}\tilde{\bm{z}}_j^T\bm{y}, \qquad j=1,\dots,k.
$$

- The principal components are in [order of importance]{.blue} and effectively [compressing the information]{.blue} contained in $\bm{X}$ using only $k \le p$ variables.

- When $k = p$ we are simply rotating the original matrix $\bm{X} = \bm{Z}\bm{V}$, i.e. performing [no compression]{.orange}. The predicted values coincide with OLS. 

- The number $k$ is a [complexity parameter]{.blue} which should be chosen via information criteria or cross-validation.

:::

## Selection of $k$: cross-validation

```{r}
#| message: false
library(pls)
resid_pcr <- matrix(0, n, p)

for (k in 1:10) {
  # Hold-out dataset
  y_k <- assessment(cv_fold$splits[[k]])$lpsa
  # MSE of the null model
  resid_pcr[complement(cv_fold$splits[[k]]), 1] <- y_k - predict(fit_null, assessment(cv_fold$splits[[k]]))
  # Fitting PCR (all the components at once)
  fit_pcr <- pcr(lpsa ~ ., data = analysis(cv_fold$splits[[k]]), center = TRUE, scale = FALSE)

  for (j in 2:p) {
    # Predictions
    y_hat <- predict(fit_pcr, newdata = assessment(cv_fold$splits[[k]]))[, , j - 1]
    # MSE of the best models for different values of p
    resid_pcr[complement(cv_fold$splits[[k]]), j] <- y_k - y_hat
  }
}
```

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

data_cv <- data.frame(
  p = 1:p,
  MSE = apply(resid_pcr^2, 2, mean),
  SE = apply(resid_pcr^2, 2, function(x) sd(x) / sqrt(n))
)

se_rule <- data_cv$MSE[which.min(data_cv$MSE)] + data_cv$SE[which.min(data_cv$MSE)]
p_optimal <- which(data_cv$MSE < se_rule)[1]

ggplot(data = data_cv, aes(x = p, y = MSE)) +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymax = MSE + SE, ymin = MSE - SE)) +
  geom_hline(yintercept = se_rule, linetype = "dotted") +
  geom_vline(xintercept = p_optimal, linetype = "dotted") +
  theme_light() +
  scale_x_continuous(breaks = 1:9) +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of principal components") +
  ylab("Mean squared error (10-fold cv)")
```



## Shrinkage effect of principal components I

::: incremental

- A closer look to the PCR solution reveals some interesting aspects. Recall that:
$$
\tilde{\bm{z}}_j = \bm{X}\tilde{\bm{v}}_j = d_j \tilde{\bm{u}}_j,  \qquad j=1,\dots,p.
$$

- The [predicted values]{.orange} of the PCR with $k$ components are:
$$
\bm{X}\hat{\beta}_\text{pcr} = \sum_{j=1}^k \tilde{\bm{z}}_j \hat{\gamma}_j = \bm{X} \sum_{j=1}^k \tilde{\bm{v}}_j \hat{\gamma}_j, \qquad \text{ where } \qquad \hat{\beta}_\text{pcr} = \sum_{j=1}^k \tilde{\bm{v}}_j \hat{\gamma}_j.
$$

- This representation highlights two important aspects:
  - It is possible to express the PCR solution in the original scale, for better [interpretability]{.orange};
  - The vector $\hat{\beta}_\text{pcr}$ is a [constrained solution]{.blue}, being a combination of $k \le p$ coefficients, therefore [reducing]{.orange} the [complexity]{.orange} of the model and [shrinking]{.blue} the coefficients. 
  
  
- When $k = 1$, then the $\hat{\beta}_\text{pcr}$ estimate coincide with the scaled loading vector $\hat{\beta}_\text{pcr} = \hat{\gamma}_1 \tilde{\bm{v}}_1$;
- When $k = p$ then the $\hat{\beta}_\text{pcr}$ coincides with [ordinary least squares]{.blue} (see Exercises). 

:::
 
## Shrinkage effect of principal components II

::: incremental

- The [variance]{.orange} of $\hat{\beta}_\text{pcr}$, assuming iid errors $\epsilon_i$ and after some algebraic manipulation, results: $$
\text{var}(\hat{\beta}_\text{pcr}) = \sigma^2\sum_{j=1}^k \frac{1}{d_j^2} \tilde{\bm{v}}_j\tilde{\bm{v}}_j^T.
$$
- Thus, if a [multicollinearity]{.blue} exists, then it appears as a principal component with very small variance, i.e. a small $d_j^2$. Its removal therefore drastically [reduces]{.orange} the [variance]{.orange} of $\hat{\beta}_\text{pcr}$. 

- Furthermore, the predicted values can be expressed as
$$
\bm{X}\hat{\beta}_\text{pcr} = \sum_{j=1}^k \tilde{\bm{z}}_j \hat{\gamma}_j = \sum_{j=1}^k \tilde{\bm{z}}_j  \frac{\tilde{\bm{z}}_j^T\bm{y}}{\tilde{\bm{z}}_j^T\tilde{\bm{z}}_j} = \sum_{j=1}^k \textcolor{darkblue}{d_j} \tilde{\bm{u}}_j  \frac{\textcolor{darkblue}{d_j}}{\textcolor{darkblue}{d_j^2}} \frac{\tilde{\bm{u}}_j^T\bm{y}}{\textcolor{red}{\tilde{\bm{u}}_j^T\tilde{\bm{u}}_j}} = \sum_{j=1}^k \tilde{\bm{u}}_j \tilde{\bm{u}}_j^T \bm{y}.
$$
- The columns of $\bm{U}$, namely the vectors $\tilde{\bm{u}}_j$ are the [normalized principal components]{.blue}.

- Hence, we are shrinking the predictions towards the main [principal directions]{.orange}.

:::

## Shrinkage effect of principal components III

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center
fit_pcr <- pcr(lpsa ~ ., data = prostate_train, center = TRUE, scale = FALSE)

data_pcr <- reshape2::melt(coef(fit_pcr, 1:8))
colnames(data_pcr) <- c("Covariate", "lpsa", "Components", "value")
data_pcr$Components <- as.numeric(data_pcr$Components)
data_pcr <- rbind(data_pcr, data.frame(Covariate = data_pcr$Covariate[data_pcr$Components == 1], lpsa = NA, Components = 0, value = 0))
data_pcr$Covariate <- as.factor(as.character(data_pcr$Covariate))
ggplot(data = data_pcr, aes(x = Components, y = value, col = Covariate)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.position = "top") +
  scale_x_continuous(breaks = 0:9) +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Number of principal components") +
  ylab("Regression coefficients")
```



## Summary and next steps

- We have see so far two "discrete" methods: best subset selection and principal components regression.

- Best subset selection perform [variable selection]{.orange}, whereas principal components regression [reduces the variance]{.orange} of the coefficients.

- These "discrete" methods can be seen as the naïve counterpart of more advanced and [continuous]{.blue} ideas, that are presented in the second part of the Unit. 

. . .

|            | [Shrinkage]{.orange}                      | [Variable selection]{.orange}    |
|------------|--------------------------------|-----------------------|
| [Discrete]{.blue}   | Principal component regression | Best subset selection, stepwise |
| [Continuous]{.blue} | Ridge regression               | Relaxed Lasso           |

. . .

- Finally, the [lasso]{.blue} and the [elastic net]{.orange} perform both shrinkage and variable selection.

# Ridge regression

## The ridge regularization method


- The ridge estimator $\hat{\beta}_\text{ridge}$ is the most common [shrinkage method]{.blue} and is the [minimizer]{.orange} of $$
\sum_{i=1}^n(y_i - \beta_0 - \bm{x}_i^T\beta)^2 \qquad \text{subject to} \qquad \sum_{j=1}^p \beta_j^2 \le s.
$$

. . .

- When [complexity parameter]{.blue} $s$ is small, the coefficients of $\hat{\beta}_\text{ridge}$ are explicitly [shrinked]{.orange}, i.e. biased, [towards zero]{.orange}. 

- On the other hand, if $s$ is large enough, then $\hat{\beta}_\text{ridge}$ is equivalent to 
 $\hat{\beta}_\text{ols}$. 

. . .

- In ridge regression, the [variability]{.orange} of the estimator is explicitly [bounded]{.orange}, although this comes with some [bias]{.blue}. The parameter $s$ controls the bias-variance trade-off. 

. . .

- Note that the intercept term $\beta_0$ is [not penalized]{.orange}.

## Centering and scaling the predictors

::: incremental

- We can use the same [mathematical trick]{.blue} that we [exploited before](/#centering-the-predictors-i) to simplify computations.

- Under centered predictors, one can show that $\hat{\beta}_0 = \bar{y}$ even in the ridge regression case. The proof is almost identical as before (see Exercises).

- Hence, we can [omit the intercept]{.orange} term $\beta_0$ after [centering]{.orange} the inputs and the response: $$
\bar{y} = 0, \qquad \bar{x}_{j} = 0, \quad j=1,\dots,p.
$$

- The ridge solutions are [not equivariant]{.orange} under [scalings of the input]{.orange}, so one normally [standardizes]{.blue} the input to have unit variance, if they are not in the same scale.

- Standardization is often times performed under the hood in most statistical software. 

- Different R packages have [different defaults]{.blue}. For instance `glmnet` also [standardizes the response]{.blue} $y$ and then [transforms back]{.orange} the estimated coefficients into the [original scale]{.orange}.


:::

## Lagrange multipliers and ridge solution

- The ridge regression problem can be [equivalently expressed]{.orange} in its [Lagrangian form](https://en.wikipedia.org/wiki/Karush–Kuhn–Tucker_conditions), which greatly facilitates computations. The ridge estimator $\hat{\beta}_\text{ridge}$ is the [minimizer]{.orange} of
$$
\sum_{i=1}^n(y_i - \bm{x}_i^T\beta)^2 + \lambda \sum_{j=1}^p\beta_j^2 = \underbrace{||\bm{y} - \bm{X}\beta||^2}_{\text{least squares}} + \underbrace{\lambda ||\beta||^2}_{\text{\textcolor{red}{penalty}}},
$$
where $\lambda > 0$ is a [complexity parameter]{.blue} controlling the [penalty]{.orange}. It holds that $s = ||\hat{\beta}_\text{ridge} ||^2$.

. . .

- When $\lambda = 0$ then $\hat{\beta}_\text{ridge} = \hat{\beta}_\text{ols}$ whereas when $\lambda \rightarrow \infty$ we get $\hat{\beta}_\text{ridge} = 0$.

. . .

::: callout-note
#### Ridge regression estimator

For any $n\times p$ design matrix $\bm{X}$, not necessarily of full-rank, the ridge estimator is 
$$
\hat{\beta}_\text{ridge} = (\bm{X}^T\bm{X} + \lambda I_p)^{-1}\bm{X}^T\bm{y}.
$$
Such an estimator [always exists]{.blue} and is [unique]{.orange} (even when $p > n$).
:::

<!-- ## Mathematical details -->

<!-- ::: incremental -->

<!-- - The matrix $\bm{X}^T\bm{X} + \lambda I_p$ is [invertible]{.blue} for any $\lambda > 0$.  -->



<!-- - For any $\lambda >0$, the ridge solution is always at the [boundary]{.orange}, that is, is never an interior point. Indeed, if $\bm{X}$ is full rank, it can be shown that -->
<!-- $$ -->
<!-- ||\hat{\beta}_\text{ridge}||^2 \le ||\hat{\beta}||^2. -->
<!-- $$ -->

<!-- - The size of the spherical ridge parameter constraint [shrinks monotonously]{.orange} as $\lambda$ increases: -->
<!-- $$ -->
<!-- \frac{\partial}{\partial \lambda} ||\hat{\beta}_\text{ridge}||^2 < 0, -->
<!-- $$ -->
<!-- ::: -->

## The geometry of the ridge solution

![](img/ridge.png){fig-align="center"}

## The ridge path

```{r}
my_ridge <- function(X, y, lambda_tilde, standardize = FALSE) {
  n <- nrow(X)
  p <- ncol(X)
  lambda <- lambda_tilde * n
  y_mean <- mean(y) # Center the response
  y <- y - y_mean

  # Centering the covariates
  X_mean <- colMeans(X)
  if (standardize) {
    X_scale <- apply(X, 2, function(x) sqrt(mean(x^2) - mean(x)^2))
  } else {
    X_scale <- rep(1, p)
  }

  X <- scale(X, center = X_mean, scale = X_scale)

  # Ridge solution (scaled data)
  beta_scaled <- solve(crossprod(X) + lambda * diag(rep(1, p)), crossprod(X, y))

  # Transform back to the original scale
  beta <- beta_scaled / X_scale
  # Compute the intercept
  beta0 <- y_mean - X_mean %*% beta
  return(c(beta0, beta))
}
```

```{r}
df_ridge <- function(lambda_tilde, X, standardize = FALSE) {
  n <- nrow(X)
  lambda <- lambda_tilde * n
  # Rescale the predictors
  X_mean <- colMeans(X)
  if (standardize) {
    X_scale <- apply(X, 2, function(x) sqrt(mean(x^2) - mean(x)^2))
  } else {
    X_scale <- rep(1, p)
  }
  X <- scale(X, center = X_mean, scale = X_scale)
  d2 <- eigen(crossprod(X))$values
  sum(d2 / (d2 + lambda))
}

df_ridge <- Vectorize(df_ridge, vectorize.args = "lambda_tilde")
```

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center
lambda_tilde_seq <- exp(seq(from = -4, to = 5, length = 50))
data_ridge <- cbind(lambda_tilde_seq, matrix(0, length(lambda_tilde_seq), p))

mse_ridge <- data.frame(lambda_tilde_seq, Cp = NA, mse = NA, df = df_ridge(lambda_tilde_seq, X, standardize = FALSE), sigma2 = NA)

for (i in 1:length(lambda_tilde_seq)) {
  data_ridge[i, -1] <- my_ridge(X, y, lambda_tilde = lambda_tilde_seq[i], standardize = FALSE)[-1]
  mse_ridge$mse[i] <- mean((y - mean(y) - X %*% data_ridge[i, -1])^2)
  mse_ridge$sigma2[i] <- mse_ridge$mse[i] * n / (n - mse_ridge$df[i])
  mse_ridge$Cp[i] <- mse_ridge$mse[i] + 2 * mse_ridge$sigma2[i] / n * mse_ridge$df[i]
}

colnames(data_ridge)[-1] <- colnames(X)
data_ridge <- tidyr::gather(data.frame(data_ridge), lambda, value, lcavol:pgg45)
colnames(data_ridge) <- c("lambda_tilde", "Covariate", "value")

ggplot(data = data_ridge, aes(x = lambda_tilde, y = value, col = Covariate)) +
  geom_point() +
  geom_line() +
  theme_light() +
  theme(legend.position = "top") +
  scale_x_log10() +
  scale_color_tableau(palette = "Color Blind") +
  xlab(expression(lambda / n)) +
  ylab("Regression coefficients")
```


## Comments on the ridge path

- The values of $\lambda$ are in somewhat [arbitrary scale]{.orange}. The ridge penalty has a concrete effect starting from $\lambda / n > 0.1$ or so.

- The variable `lcavol` is arguably the most important, followed by `lweight` and `svi`, being the one that receive less shrinkage compared to the others.

. . .

- The coefficient of `age`, `gleason` and `lcp`, is negative at the beginning and then become positive for large values of $\lambda$.

- This indicate that their negative value in $\hat{\beta}_\text{ols}$ was probably a consequence of their [correlation]{.orange} with other variables. 

. . .

- There is an interesting similarity between this plot and the one of principal component regression... is it a coincidence? 

## Shrinkage effect of ridge regression I

- Considering, once again, the [singular value decomposition]{.orange}, we get: $$\begin{aligned}
\bm{X}\hat{\beta}_\text{ridge} &= \bm{X}(\bm{X}^T\bm{X} + \lambda I_p)^{-1}\bm{X}^T\bm{y} \\
& = \bm{U}\bm{D} \bm{V}^T[\bm{V}(\bm{D}^T\bm{D} + \lambda I_p)\bm{V}^T]^{-1}(\bm{U}\bm{D}\bm{V})^T\bm{y}  \\
& = \bm{U}\bm{D} \textcolor{red}{\bm{V}^T\bm{V}}(\bm{D}^T\bm{D} + \lambda I_p)^{-1} \textcolor{red}{\bm{V}^T \bm{V}} \bm{D}^T \bm{U} ^T\bm{y} \\
& = \bm{U}\bm{D}(\bm{D}^T\bm{D} + \lambda I_p)^{-1}\bm{D}^T\bm{U}^T\bm{y} \\
& = \bm{H}_\text{ridge}\bm{y} = \sum_{j=1}^p \tilde{\bm{u}}_j \frac{d_j^2}{d_j^2 + \lambda}\tilde{\bm{u}}_j^T \bm{y},
\end{aligned}
$$
where $\bm{H}_\text{ridge} = \bm{X}(\bm{X}^T\bm{X} + \lambda I_p)^{-1}\bm{X}^T$ is the so-called [hat matrix]{.blue} of ridge regression.

. . .

- This means that ridge regression [shrinks]{.blue} the [principal directions]{.blue} by an amount that depends on the [eigenvalues]{.orange} $d_j^2$. 

- In other words, it [smoothly reduces]{.orange} the impact of the [redundant]{.orange} information. 


## Shrinkage effect of ridge regression II

- A sharp [connection]{.blue} with [principal components regression]{.blue} is therefore revealed. 

- Compare the previous formula for $\bm{X}\hat{\beta}_\text{ridge}$ with [the one we previously obtained](#shrinkage-effect-of-principal-components-ii) for $\bm{X}\hat{\beta}_\text{pcr}$.

. . .

- More explicitly, for [ridge regression]{.orange} we will have that $$
\hat{\beta}_\text{ridge} = \bm{V}\text{diag}\left(\frac{d_1}{d_1^2 + \lambda}, \dots, \frac{d_p}{d_p^2 + \lambda}\right)\bm{U}^T\bm{y}.
$$
whereas for [principal components regression]{.blue} with $k$ components we get
$$
\hat{\beta}_\text{pcr} = \bm{V}\text{diag}\left(\frac{1}{d_1}, \dots, \frac{1}{d_k}, 0, \dots, 0\right)\bm{U}^T\bm{y}.
$$

. . .

- Both operate on the singular values, but where principal component regression [thresholds]{.blue} the singular values, [ridge regression]{.orange} shrinks them.

## Bias-variance trade-off

- The ridge regression [add some bias]{.orange} to the estimates, but it [reduces]{.blue} their [variance]{.blue}. 

. . .

- The [variance]{.orange} of $\hat{\beta}_\text{ridge}$, assuming iid errors $\epsilon_i$ with variance $\sigma^2$, results: $$
\text{var}(\hat{\beta}_\text{ridge}) = \sigma^2\sum_{j=1}^p \frac{d_j^2}{(d_j^2 + \lambda)^2} \tilde{\bm{v}}_j\tilde{\bm{v}}_j^T,
$$
whose diagonal elements are always smaller than those of $\text{var}(\hat{\beta}_\text{ols})$.

. . .

::: callout-note
#### Theorem (Theobald, 1974)
Let us [additionally]{.blue} assume that $y_i = \bm{x}_i^T\beta + \epsilon_i$, where $\epsilon_i$ are iid errors with variance $\sigma^2$. Then, there exist a $\lambda > 0$ such that $$
\mathbb{E}\left(||\hat{\beta}_\text{ridge} - \beta)||^2\right) < \mathbb{E}\left(||\hat{\beta}_\text{ols} - \beta||^2\right) = \text{var}(\hat{\beta}_\text{ols}).
$$
:::

## ☠️ - A historical perspective I

::: incremental

- The ridge regression estimator was originally proposed by Hoerl and Kennard (1970) with a quite different motivation in mind.

- In linear models, the estimate of $\beta$ is obtained by solving the [normal equations]{.blue} $$
(\bm{X}^T\bm{X})\beta = \bm{X}^T\bm{y},
$$
which could be [ill-conditioned]{.orange}.

- In other words, the [condition number]{.blue}
$$
\kappa(\bm{X}^T\bm{X}) = \frac{d_1^2}{d_p^2},
$$
might be very large, leading to [numerical inaccuracies]{.orange}, since the matrix $\bm{X}^T\bm{X}$ is [numerically singular]{.blue} and therefore not invertible in practice.

:::

## ☠️ - A historical perspective II

::: incremental
- Ridge provides a [remedy]{.blue} for [ill-conditioning]{.orange}, by adding a "ridge" to the diagonal of $\bm{X}^T\bm{X}$, obtaining the modified normal equations
$$
(\bm{X}^T\bm{X} + \lambda I_p)\beta = \bm{X}^T\bm{y}.
$$

- The [condition number]{.orange} of the modified $(\bm{X}^T\bm{X} + \lambda I_p)$ matrix becomes
$$
\kappa(\bm{X}^T\bm{X} + \lambda I_p) = \frac{\lambda + d_1^2}{\lambda + d_p^2}.
$$
- Notice that even if $d_p = 0$, i.e. the matrix $\bm{X}$ is [singular]{.blue}, then condition number will be finite as long as $\lambda > 0$. 

- This technique is known as [Tikhonov regularization]{.orange}, after the Russian mathematician Andrey Tikhonov.

:::

## ☠️ - A historical perspective III

![](img/ridge_paper.png){fig-align="center" width=40%}

- Figure 1 of the [original paper]{.orange} by Hoerl and Kennard (1970), displaying the bias-variance [trade-off]{.blue}.

## On the choice of $\lambda$

- The [penalty parameter]{.orange} $\lambda$ determines the amount of bias and variance of  $\hat{\beta}_\text{ridge}$ and therefore it must be carefully [estimated]{.blue}.

. . .

- [Minimizing]{.blue} $||\bm{y} - \bm{X}\hat{\beta}_\text{ridge}||^2$ over $\lambda$ is a [very bad idea]{.orange} and it would always choose $\lambda =0$, corresponding to $\hat{\beta}_\text{ridge} = \hat{\beta}_\text{ols}$.

- Indeed, $\lambda$ is a [complexity]{.blue} parameter and, like the number of covariates, should be selected using [information criteria]{.orange} or training/test and [cross-validation]{.orange}.

. . .

- Suppose we wish to use an [information criteria]{.blue} such as the AIC or BIC, of the form $$
    \text{IC}(p) = -2 \ell(\hat{\beta}_\text{ridge}) + \text{penalty}(\text{``degrees of freedom"}).
    $$
We need a careful definition of [degrees of freedom]{.blue} that is appropriate in this context.

- The current definition of degrees of freedom, i.e. the number of [non-zero coefficients]{.orange}, is [not appropriate]{.orange} for ridge regression, because it would be equal to $p$ for any value of $\lambda$. 

## Effective degrees of freedom I

- Let us recall that the [optimism]{.orange} for a generic estimator $\hat{f}(\bm{x})$ is defined as the following average of covariances
$$
\text{Opt} = \frac{2}{n}\sum_{i=1}^n\text{cov}(Y_i, \hat{f}(\bm{x}_i)),
$$
which is equal to $\text{Opt}_\text{ols} = (2\sigma^2p)/ n$ in [ordinary least squares]{.blue}.

. . .

::: callout-note
#### Effective degrees of freedom

Let $\hat{f}(\bm{x})$ be an estimate for the regression function $f(\bm{x})$ based on the data $Y_1,\dots,Y_n$. The [effective degrees of freedom]{.orange} are defined as $$
\text{df} = \frac{1}{\sigma^2}\sum_{i=1}^n\text{cov}(Y_i, \hat{f}(\bm{x}_i)).
$$

:::



## Effective degrees of freedom II

::: incremental

- The effective degrees of freedom of [ordinary least squares]{.blue} and [principal component regression]{.blue} are
$$
\text{df}_\text{ols} = p, \qquad \text{df}_\text{pcr} = k.
$$

- After some algebra, one finds that the effective degrees of freedom of [ridge regression]{.orange} are
$$
\text{df}_\text{ridge} = \text{tr}(\bm{H}_\text{ridge}) = \sum_{j=1}^p \frac{d_j^2}{d_j^2 + \lambda},
$$
where $\bm{H}_\text{ridge} = \bm{X}(\bm{X}^T\bm{X} + \lambda I_p)^{-1}\bm{X}^T$ is the [hat matrix]{.orange} of ridge regression.

- Using the above result, we can then [plug-in]{.orange} $\text{df}_\text{ridge}$ into the formula of the [$C_p$ of Mallows]{.blue}: $$
    \widehat{\mathrm{ErrF}} = \frac{1}{n}\sum_{i=1}^n(y_i - \bm{x}_i^T\hat{\beta}_\text{ridge})^2 + \frac{2 \hat{\sigma}^2}{n} \text{df}_\text{ridge}.
    $$
where the residual variance is estimated as $\hat{\sigma}^2 = (n - \text{df}_\text{ridge})^{-1} \sum_{i=1}^n(y_i - \bm{x}_i^T\hat{\beta}_\text{ridge})^2$.

:::

## Effective degrees of freedom III

::: columns
::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4
df_min_cp <- mse_ridge$df[which.min(mse_ridge$Cp)]
lambda_tilde_min_cp <- mse_ridge$lambda_tilde_seq[which.min(mse_ridge$Cp)]

ggplot(data = mse_ridge, aes(x = df, y = Cp)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = df_min_cp, linetype = "dotted") +
  theme_light() +
  scale_x_continuous(breaks = 0:8) +
  xlab("Effective degrees of freedom (df)") +
  ylab(expression(C[p]))
```
:::

::: {.column width="50%"}
```{r}
#| fig-width: 5
#| fig-height: 4
ggplot(data = mse_ridge, aes(x = lambda_tilde_seq, y = Cp)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = lambda_tilde_min_cp, linetype = "dotted") +
  theme_light() +
  scale_x_log10() +
  xlab(expression(lambda / n)) +
  ylab(expression(C[p]))
```
:::
:::


## Cross-validation for ridge regression I

- Training / test strategies and [cross-validation]{.blue} are also valid tools for selecting $\lambda$. 

- Most statistical software packages use a slightly [different parametrization]{.orange} for $\lambda$, as they minimize $$
\textcolor{red}{\frac{1}{n}}\sum_{i=1}^n(y_i - \bm{x}_i^T\beta)^2 + \tilde{\lambda} \sum_{j=1}^p\beta_j^2,
$$
where the penalty parameter $\tilde{\lambda} = \lambda / n$.



. . .


- This parametrization does not alter the estimate of $\hat{\beta}_\text{ridge}$ but is more amenable for [cross-validation]{.blue} as the values of $\tilde{\lambda}$ can be compared across dataset with [different sample sizes]{.orange}.

- The R package `glmnet` uses this convention.


## Cross-validation for ridge regression II

```{r}
#| message: false
resid_ridge <- matrix(0, n, length(lambda_tilde_seq))

for (k in 1:10) {
  # Hold-out dataset
  X_test_k <- as.matrix(subset(assessment(cv_fold$splits[[k]]), select = -c(lpsa)))
  y_test_k <- assessment(cv_fold$splits[[k]])$lpsa

  X_train_k <- as.matrix(subset(analysis(cv_fold$splits[[k]]), select = -c(lpsa)))
  y_train_k <- analysis(cv_fold$splits[[k]])$lpsa

  for (j in 1:length(lambda_tilde_seq)) {
    # Estimates
    beta_hat <- my_ridge(X_train_k, y_train_k, lambda_tilde = lambda_tilde_seq[j], standardize = FALSE)
    # Predictions
    y_hat <- cbind(1, X_test_k) %*% beta_hat
    # MSE of the best models for different values of lambda
    resid_ridge[complement(cv_fold$splits[[k]]), j] <- y_test_k - y_hat
  }
}
```

```{r}
#| fig-width: 10
#| fig-height: 5
#| fig-align: center

data_cv <- data.frame(
  lambda_tilde = lambda_tilde_seq,
  df = df_ridge(lambda_tilde_seq, X, standardize = FALSE),
  MSE = apply(resid_ridge^2, 2, mean),
  SE = apply(resid_ridge^2, 2, function(x) sd(x) / sqrt(n))
)

se_rule <- data_cv$MSE[which.min(data_cv$MSE)] + data_cv$SE[which.min(data_cv$MSE)]
lambda_tilde_min_cv <- lambda_tilde_seq[tail(which(data_cv$MSE < se_rule), 1)]
df_min_cv <- data_cv$df[tail(which(data_cv$MSE < se_rule), 1)]

ggplot(data = data_cv, aes(x = lambda_tilde, y = MSE)) +
  geom_point() +
  geom_line() +
  geom_linerange(aes(ymax = MSE + SE, ymin = MSE - SE)) +
  scale_x_log10() +
  geom_hline(yintercept = se_rule, linetype = "dotted") +
  geom_vline(xintercept = lambda_tilde_min_cv, linetype = "dotted") +
  theme_light() +
  xlab(expression(tilde(lambda))) +
  ylab("Mean squared error (10-fold cv)")
```

```{r}
#| include: false
#| execute: false
# Double checks
library(glmnet)
y_std <- prostate_train$lpsa / sqrt(mean(prostate_train$lpsa^2) - mean(prostate_train$lpsa)^2)

as.numeric(coef(glmnet(X, y_std, family = "gaussian", standardize = FALSE, alpha = 0, thresh = 1e-16, lambda = lambda_tilde_min_cv)))

my_ridge(X, y_std, standardize = FALSE, lambda_tilde = lambda_tilde_min_cv)
```


<!-- ## Leave-one-out cross-validation -->


<!-- -   Ridge regression is, like ordinary least squares, a [linear smoother]{.blue}, namely: -->
<!-- $$ -->
<!-- \bm{X}\hat{\beta}_\text{ridge} = \bm{X}(\bm{X}^T\bm{X} + \lambda I_p)^{-1}\bm{X}^T\bm{y} = \bm{H}_\text{ridge}\bm{y}, -->
<!-- $$ -->
<!-- meaning that a [computational shortcut]{.orange} for LOO-CV can be exploited. -->

<!-- . . . -->

<!-- ::: callout-note -->
<!-- #### LOO-CV (Ridge regression) -->

<!-- Let $\hat{y}_{-i, \text{ridge}} = \bm{x}_i^T\hat{\beta}_{-i, \text{ridge}}$ be the leave-one-out -->
<!-- predictions of a [linear model]{.blue} with a [ridge penalty]{.orange} and let $h_{i, \text{ridge}} = [\bm{H}_\text{ridge}]_{ii}$ and -->
<!-- $\hat{y}_i$ be the leverages and the predictions. Then: $$ -->
<!-- y_i - \hat{y}_{-i, \text{ridge}} = \frac{y_i - \hat{y}_{i, \text{ridge}}}{1 - h_{i, \text{ridge}}}, \qquad i=1,\dots,n, -->
<!-- $$ -->
<!-- recalling that $\bar{y} = 0$. Therefore, the leave-one-out mean squared error is -->
<!-- $$\widehat{\mathrm{Err}} = \frac{1}{n}\sum_{i=1}^n \left(\frac{y_i - \hat{y}_{i, \text{ridge}}}{1 - h_{i, \text{ridge}}}\right)^2.$$ -->
<!-- ::: -->

## The ridge estimate

```{r}
#| fig-width: 9
#| fig-height: 5
#| fig-align: center
ggplot(data = data_ridge, aes(x = df_ridge(lambda_tilde, X), y = value, col = Covariate)) +
  geom_point() +
  geom_line() +
  theme_light() +
  geom_vline(xintercept = df_min_cv, linetype = "dashed") +
  geom_vline(xintercept = df_min_cp, linetype = "dotted") +
  scale_x_continuous(breaks = 0:8) +
  theme(legend.position = "top") +
  scale_color_tableau(palette = "Color Blind") +
  xlab("Effective degrees of freedom (df)") +
  ylab("Regression coefficients")
```

## Summary of ridge regression

- Trade-off bias variance, variables more correlated are shrinked

- All variables are used

- Solution always exists, even when $p > n$ or in presence of perfect collinearity.

- Mention to efficient computations

# The lasso

## Looking for sparsity

## The lasso regularization method

## Lagrange multipliers and lasso solution

## The geometry of the lasso solution

## Single predictor I

## Single predictor II

## The lasso path

## Least angle regression I

## Least angle regression II

## Least angle regression: lasso modification

## The maximum $\lambda$

## Uniqueness of the lasso solution

## The degrees of freedom of the lasso

## ☠️ - Effective degrees of freedom of best subset


## A comparison between pcr, ridge, and lasso

## The estimated coefficients

## Summary of LARS and lasso

# Elastic net

# Computations

## Computations for ridge, lasso and elastic net

## Convex optimization

## Coordinate descent

## Pathwise coordinate optimization I

## Pathwise coordinate optimization II

## Pathwise coordinate optimization III

# Generalized linear models

## Best subset for GLMs

## Principal components regression for GLMs

## Shrinkage methods for GLMs

## Quadratic approximations

## Main references

-   [Main references]{.blue}
    -   **Chapter 3** of Azzalini, A. and Scarpa, B. (2011), [*Data
        Analysis and Data
        Mining*](http://azzalini.stat.unipd.it/Book-DM/), Oxford
        University Press.
    -   **Chapter 7** of Hastie, T., Tibshirani, R. and Friedman, J.
        (2009), [*The Elements of Statistical
        Learning*](https://hastie.su.domains/ElemStatLearn/), Second
        Edition, Springer.
        
## Ridge references

-   [Specialized references]{.orange}
    - Hastie, T. (2020). Ridge regularization: an essential concept in data science. *Technometrics*, 62(4), 426-433.

## Lasso and LARS references

## Other references



<!-- # Lasso, LARS, and elastic-net -->

<!-- ## Lasso -->

<!-- ::: columns -->

<!-- ::: {.column width="25%"} -->

<!-- ![](img/lasso.png) -->

<!-- ::: -->

<!-- ::: {.column width="75%"} -->

<!-- -   asdasd -->

<!-- ::: -->

<!-- ::: -->

<!-- ## Lasso -->

<!-- ```{r} -->

<!-- library(lars) -->

<!-- lambda <- 100 -->

<!-- XX <- scale(X[, -1], TRUE, scale = apply(X[, -1], 2, function(x)  sqrt(mean(x^2) - mean(x)^2))) -->

<!-- yy <- y#(y - mean(y)) / sqrt(mean(y^2) - mean(y)^2) -->

<!-- cv_lars <- cv.lars(x = XX, y = yy, K = 10, type = "lasso", mode = "step") -->

<!-- fit_lars <- lars(x = XX, y = yy, type ="lasso", normalize = FALSE) -->

<!-- cv_lasso_fit <- cv.glmnet(XX, yy, standardize = FALSE,  -->

<!--                           family = "gaussian", alpha = 1, nfolds = 10, -->

<!--                           lambda = 1 / n * fit_lars$lambda, thresh = 1e-16) -->

<!-- plot(cv_lasso_fit) -->

<!-- lasso_fit <- cv_lasso_fit$glmnet.fit -->

<!-- lambda_sel <- 4 -->

<!-- round(coef(fit_lars)[lambda_sel, ], 5) -->

<!-- round(coef(lasso_fit, mode = "lambda")[-1, lambda_sel], 5) -->

<!-- ``` -->

<!-- ## Summary of the estimated coefficients -->

<!-- ::: {style="font-size: 70%;"} -->

<!-- ```{r} -->

<!-- library(DT) -->

<!-- tab <- data.frame(OLS = rep(0, p), best_subset = rep(0, p), PCR = rep(0, p)) -->

<!-- rownames(tab) <- colnames(sum_best$which) -->

<!-- tab$OLS <- coef(lm(lpsa ~ ., data = prostate_train)) -->

<!-- tab$best_subset <- c(coef(lm(lpsa ~ lcavol + lweight, data = prostate_train)), rep(0, 6)) -->

<!-- # Principal components regression (PCR) -->

<!-- fit_pcr <- pcr(lpsa ~ ., data = prostate_train, center = TRUE, scale = FALSE) -->

<!-- beta <- c(coef(fit_pcr, 3)) -->

<!-- beta <- c(mean(prostate_train$lpsa) - colMeans(X[, -1]) %*% beta, beta) -->

<!-- tab$PCR <- beta -->

<!-- datatable(tab, colnames = c("OLS", "Best subset", "PCR"), options = list( -->

<!--   pageLength = 9, -->

<!--   dom = "t")) %>% -->

<!--   formatRound(columns = 1:3, digits = 3) %>% -->

<!--   formatStyle( -->

<!--     columns = 0, fontWeight = "bold" -->

<!--   ) %>% -->

<!--   formatStyle( -->

<!--     columns = 1:3, -->

<!--     backgroundColor = styleInterval(0, c("#FED8B1", "#DBE9FA")) -->

<!--   ) %>% -->

<!--   formatStyle( -->

<!--     columns = 1:3, -->

<!--     backgroundColor = styleEqual(0, c("white")) -->

<!--   ) -->

<!-- ``` -->

<!-- ::: -->

