---
title: "Exercises C"
subtitle: "Data mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Universit√† degli Studi di Milano-Bicocca_"
lang: en
execute:
  cache: true
format:
  html:
    html-math-method: katex
    echo: true
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: true
    code-line-numbers: true
    smooth-scroll: true
    fig-dpi: 250
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

# [Homepage](../index.html)

## Theoretical exercises

#### Exercise C.1 - Equivalence between PCR and OLS when $k = p$

[Show]{.orange} that when the number of principal components $k = p$, then the predicted values of PCR and ordinary least squares coincide, namely
$$
\bm{X}\hat{\beta}_\text{ols} = \bm{Z}\hat{\gamma}_\text{pcr},
$$
where the definition of $\bm{Z}$ and $\hat{\gamma} = (\hat{\gamma}_1,\dots, \hat{\gamma}_p)$ is given in [this slide](../slides/un_C.html#principal-components-regression-pcr). Moreover, show that when $k = p$
$$
\hat{\beta}_\text{ols} = \hat{\beta}_\text{pcr},
$$
where $\hat{\beta}_\text{pcr}$ has been defined in [this slide](https://tommasorigon.github.io/datamining/slides/un_C.html#shrinkage-effect-of-principal-components-i).

#### Exercise C.2 - Centering and scaling the predictors

#### Exercise C.3 - Ridge estimator

[Verify]{.orange} the correctness of the statement of [this slide](../slides/un_C.html#lagrange-multipliers-and-ridge-solution), namely that the [ridge estimator]{.blue}
$$
\hat{\beta}_\text{ridge} = (\bm{X}^T\bm{X} + \lambda I_p)^{-1}\bm{X}^T\bm{y}.
$$
is the [minimizer]{.orange} of the penalized loss function
$$
\sum_{i=1}^n(y_{i} - \bm{x}_{i}^T\beta)^2 + \lambda \sum_{j=1}^p\beta_j^2.
$$
[Hint]{.blue}: the proof is a simple adaptation of the one for ordinary least squares. 

#### Exercise C.4 - Ridge data-augmentation (3.12 HTF)

#### Exercise C.5 - Ridge regression with identical variables (3.29 HTF)

#### Exercise C.6 - Degrees of freedom of ridge regression

#### Exercise C.7 - Lasso and elastic-net solution with a single predictor

## Practical exercises

<!-- #### Exercise P.1 -->

<!-- ```{r} -->
<!-- set.seed(123) -->
<!-- x <- seq(from = 0, to = 4, length = 100) -->
<!-- y <- 2 + x + rnorm(100, 0, 1) -->
<!-- x_center <- x - mean(x) -->
<!-- y_center <- y - mean(y) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- mean(y) -->
<!-- ``` -->


<!-- - To fix the ideas, the OLS estimed with [uncentered predictors]{.blue} is -->

<!-- ```{r} -->
<!-- #| echo: true -->
<!-- coef(lm(y ~ x)) -->
<!-- ``` -->

<!-- - After centering, we obtain the  -->


<!-- ```{r} -->
<!-- #| echo: true -->
<!-- round(coef(lm(y_center ~ x_center)), 6) -->
<!-- ``` -->

<!-- - asd -->

<!-- ```{r} -->
<!-- #| echo: true -->
<!-- coef(lm(y ~ x_center)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #| echo: true -->
<!-- coef(lm(y ~ 0 + x_center)) -->
<!-- ``` -->

