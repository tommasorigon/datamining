---
title: "Exercises C"
subtitle: "Data mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_UniversitÃ  degli Studi di Milano-Bicocca_"
lang: en
execute:
  cache: true
format:
  html:
    html-math-method: katex
    echo: true
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: true
    code-line-numbers: true
    smooth-scroll: true
    fig-dpi: 250
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

# [Homepage](../index.html)

## Theoretical exercises

#### Exercise C.1 - Equivalence between PCR and OLS when $k = p$

[Show]{.orange} that when the number of principal components $k = p$, then the predicted values of PCR and ordinary least squares coincide, namely
$$
\bm{X}\hat{\beta}_\text{ols} = \bm{Z}\hat{\gamma}_\text{pcr},
$$
where the definition of $\bm{Z}$ and $\hat{\gamma} = (\hat{\gamma}_1,\dots, \hat{\gamma}_p)$ is given in [this slide](../slides/un_C.html#principal-components-regression-pcr). Moreover, show that when $k = p$
$$
\hat{\beta}_\text{ols} = \hat{\beta}_\text{pcr},
$$
where $\hat{\beta}_\text{pcr}$ has been defined in [this slide](https://tommasorigon.github.io/datamining/slides/un_C.html#shrinkage-effect-of-principal-components-i).

#### Exercise C.2 - Centering and scaling the predictors

**Part I**

Suppose the covariates $\tilde{\bm{x}}_j$ were [scaled]{.blue} (same variance) but [not centered]{.orange} (different means). Moreover, suppose the response $y$ was [not centered]{.orange}. Consider the following estimator
$$
(\hat{\beta}_0, \hat{\beta}_\text{ridge}) = \arg\min_{\beta_0,\beta} \sum_{i=1}^n(y_{i} - \beta_0 - \bm{x}_{i}^T\beta)^2 + \lambda \sum_{j=1}^p\beta_j^2.
$$
[Show]{.orange} that $\hat{\beta}_\text{ridge}$ can be equivalently obtained using the centered data, that is
$$
\hat{\beta}_\text{ridge} = \arg\min_{\beta} \sum_{i=1}^n\{(y_{i} - \bar{y}) - (\bm{x}_{i} - \bar{\bm{x}})^T\beta\}^2 + \lambda \sum_{j=1}^p\beta_j^2.
$$
and that $\hat{\beta}_0 = \bar{y} - \bar{\bm{x}}\hat{\beta}_\text{ridge}$. 



**Part II**

Suppose the covariates $\tilde{\bm{x}}_j$ were [not scaled]{.orange} (different variances $s_j$) and [not centered]{.orange} (different means). Moreover, suppose the response $y$ was [not centered]{.orange}. Consider the following [scaled-ridge estimator]{.blue}
$$
(\hat{\beta}_0, \hat{\beta}_\text{scaled-ridge}) = \arg\min_{\beta_0,\beta} \sum_{i=1}^n\left(y_{i} - \beta_0 - \bm{x}_i^T\beta\right)^2 + \lambda \sum_{j=1}^p s^2_j \beta_j^2.
$$
Consider now the following estimator
$$
\hat{\beta}_\text{ridge} = \arg\min_{\beta} \sum_{i=1}^n\left\{(y_{i} - \bar{y}) - \sum_{j=1}^p\left(\frac{x_{ij} - \bar{x}_j}{s_j}\right)\beta_j\right\}^2 + \lambda \sum_{j=1}^p\beta_j^2.
$$

As mentioned in [this slide](https://tommasorigon.github.io/datamining/slides/un_C.html#centering-and-scaling-the-predictors-ii), [show]{.orange} that the coefficients of [ridge regression]{.orange}, expressed in the [original scale]{.blue}, are $$
    \hat{\beta}_0 = \bar{y} - \bar{\bm{x}}\hat{\beta}_\text{scaled-ridge}, \qquad \hat{\beta}_\text{scaled-ridge} = \text{diag}(1 / s_1,\dots, 1/s_p) \hat{\beta}_\text{ridge}.
    $$ 

#### Exercise C.3 - Ridge estimator

[Verify]{.orange} the correctness of the statement of [this slide](../slides/un_C.html#lagrange-multipliers-and-ridge-solution). In other words, show that the [ridge estimator]{.blue}, under standard centering and scaling assumptions and defined as
$$
\hat{\beta}_\text{ridge} = (\bm{X}^T\bm{X} + \lambda I_p)^{-1}\bm{X}^T\bm{y},
$$
is the [minimizer]{.orange} of the penalized loss function
$$
\sum_{i=1}^n(y_{i} - \bm{x}_{i}^T\beta)^2 + \lambda \sum_{j=1}^p\beta_j^2.
$$
[Hint]{.blue}: the proof is a simple adaptation of the one for ordinary least squares. 

#### Exercise C.4 - Ridge data-augmentation

Show that the ridge regression estimator $\hat{\beta}_\text{ridge}$ can be obtained by [ordinary least squares]{.blue} regression on an [augmented data set]{.orange}. 

We augment the centered matrix $\bm{X}$ with $p$ additional rows $\sqrt{\lambda} I_p$ and augment $\bm{y}$ with $p$ zeros, namely we consider the augmented dataset
$$
\tilde{\bm{X}} = \begin{pmatrix}
\bm{X}\\
\sqrt{\lambda}I_p\\
\end{pmatrix}, \qquad \tilde{\bm{y}} = \begin{pmatrix}\bm{y} \\ 0_p\end{pmatrix}.
$$
By introducing artificial data having response value zero, the fitting procedure is forced to shrink the coefficients towards zero. [Show]{.orange} that 
$$
\hat{\beta}_\text{ridge} = (\tilde{\bm{X}}^T\tilde{\bm{X}})^{-1} \tilde{\bm{X}}^T\tilde{\bm{y}}.
$$

#### Exercise C.5 - Ridge regression with identical variables

Suppose we run a ridge regression with parameter $\lambda$ on a single variable $\tilde{\bm{x}}_1$ (centered and scaled) and get a coefficient $\hat{\beta}_1$. We now include an exact copy $\tilde{\bm{x}}_2 = \tilde{\bm{x}}_1$ and refit our ridge regression. 

[Show]{.orange} that both coefficients are identical and derive their value. 

#### Exercise C.6 - Lasso solution with a single predictor


Consider thea lasso problem with a    [single-predictor]{.blue} $$
     \hat{\beta}_\text{lasso} = \arg\min_{\beta}\frac{1}{2n}\sum_{i=1}^n(y_{i} - x_{i}\beta)^2 + \lambda |\beta|.
      $$
[Show]{.orange} that $\hat{\beta}_\text{lasso}$ has an [explicit expression]{.orange}, which is $$
    \hat{\beta}_\text{lasso} = \begin{cases} \text{cov}(x,y) - \lambda, \qquad &\text{if} \quad \text{cov}(x,y) > \lambda \\
    0 \qquad &\text{if} \quad \text{cov}(x,y) \le \lambda\\
    \text{cov}(x,y) + \lambda, \qquad &\text{if} \quad \text{cov}(x,y) < -\lambda \\
    \end{cases}
    $$
[Show]{.orange} in addition that $$
    \hat{\beta}_\text{ridge} = \frac{1}{\lambda + 1}\text{cov}(x,y) =\frac{1}{\lambda + 1}\hat{\beta}_\text{ols} = \frac{1}{\lambda + 1}\frac{1}{n}\sum_{i=1}^n x_{i}y_{i}.
    $$
    
## Practical exercises

#### Exercise C.7 - The `Hitters` dataset

Consider the `Hitters` dataset which is available in the `ISLR` R library. Having removed the missing values, consider a regression models to predict the `Salary` as a function of the available covariates.

Use best subset, principal components, ridge regression, and lasso to identify the handle the presence of potentially irrelevant variables. 

#### Exercise C.8 - Implementation of pathwise coordinate optimization

Implement the pathwise coordinate optimization algorithm that is described in [this slide](../slides/un_C.html#coordinate-descent). Use to predict the `Salary` of the baseball players on the `Hitters` dataset.

<!-- #### Exercise P.1 -->

<!-- ```{r} -->
<!-- set.seed(123) -->
<!-- x <- seq(from = 0, to = 4, length = 100) -->
<!-- y <- 2 + x + rnorm(100, 0, 1) -->
<!-- x_center <- x - mean(x) -->
<!-- y_center <- y - mean(y) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- mean(y) -->
<!-- ``` -->


<!-- - To fix the ideas, the OLS estimed with [uncentered predictors]{.blue} is -->

<!-- ```{r} -->
<!-- #| echo: true -->
<!-- coef(lm(y ~ x)) -->
<!-- ``` -->

<!-- - After centering, we obtain the  -->


<!-- ```{r} -->
<!-- #| echo: true -->
<!-- round(coef(lm(y_center ~ x_center)), 6) -->
<!-- ``` -->

<!-- - asd -->

<!-- ```{r} -->
<!-- #| echo: true -->
<!-- coef(lm(y ~ x_center)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #| echo: true -->
<!-- coef(lm(y ~ 0 + x_center)) -->
<!-- ``` -->

