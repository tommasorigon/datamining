---
title: "Exercises B"
subtitle: "Data mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Universit√† degli Studi di Milano-Bicocca_"
lang: en
execute:
  cache: true
format:
  html:
    html-math-method: katex
    echo: true
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: true
    code-line-numbers: true
    smooth-scroll: true
    fig-dpi: 250
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

# [Homepage](../index.html)


## Theoretical exercises

#### B.1 - Bias-variance decomposition

[Prove]{.orange} the bias-variance decomposition stated in [this slide](../slides/un_B.html#bias-variance-trade-off).

#### B.2 - Optimism

[Prove]{.orange} that, as stated in [this slide](../slides/un_B.html#optimism-ii), the optimism, defined as $$
    \text{Opt} = \mathbb{E}(\text{MSE}_\text{test}) - \mathbb{E}(\text{MSE}_\text{train}),
    $$
    can be equivalently expressed as $$
    \text{Opt} = \frac{2}{n}\sum_{i=1}^n\text{cov}(Y_i, \hat{f}(\bm{x}_i)).
    $$
Refer also Exercise 3.3 of the textbook A&S (2011), which is implicitly computing similar quantities.

#### B.3 - Leave-one-out

[Prove]{.orange} the leave-one-out formula for linear models stated in [this slide](../slides/un_B.html#leave-one-out-cross-validation). [Hint]{.blue}: most of the steps are similar to those needed for computing recursive least squares.


