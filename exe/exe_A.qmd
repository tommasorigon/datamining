---
title: "Exercises A"
subtitle: "Data mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Universit√† degli Studi di Milano-Bicocca_"
lang: en
execute:
  cache: true
format:
  html:
    html-math-method: katex
    echo: true
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: true
    code-line-numbers: true
    smooth-scroll: true
    fig-dpi: 250
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

# [Homepage](../index.html)

## Theoretical exercises

#### A.1 - Properties of the projection matrix $\bm{H}$

Suppose the $n\times p$ design matrix $\bm{X}$ has [full rank]{.blue}, that is $\text{rk}(\bm{X}) = p$, with $p < n$. [Prove]{.orange} that 
$$
\text{rk}(\bm{H}) = \text{tr}(\bm{H}) = p.
$$
Moreover, show that $\bm{H} = \bm{H}^T$ and that $\bm{H}^2 = \bm{H}$. 

[Hint]{.blue}. You may want to look for the properties of [projection matrices]{.orange} on your favorite linear algebra textbook, otherwise this exercise becomes quite hard. 

#### A.2 - Positive definiveness of $\bm{X}^T\bm{X}$

[Prove]{.orange} the statement of [Proposition A.1](../slides/un_A.html#cholesky-factorization). In other words, suppose the  $n\times p$  matrix $\bm{X}$ has [full rank]{.blue}, that is $\text{rk}(\bm{X}) = p$, with $p < n$. Then, show that $\bm{X}^T\bm{X}$ is [positive definite]{.blue}.

#### A.3 - OLS with orthogonal predictors

[Prove]{.orange} the statement of [Proposition A.2](../slides/un_A.html#orthogonal-predictors). In other words, show that when the predictors are orthogonal the least square estimate is $$
\hat{\beta}_j = \frac{\tilde{\bm{z}}_j^T\bm{y}}{\tilde{\bm{z}}_j^T\tilde{\bm{z}}_j}, \qquad j=1,\dots,p.
$$

Moreover, obtain the covariance matrix of $\hat{\beta}$ and conclude that the estimators $\hat{\beta}_j$ and $\hat{\beta}_{j'}$ are independent, assuming Gaussian errors.

#### A.4 - Final step of the Gram-Schidmt algorithm

Consider the last equation of [this slide](../slides/un_A.html#the-qr-decomposition-and-least-squares), that is
$$
\hat{\beta}_p = (\tilde{\bm{z}}_p^T\bm{y}) / (\tilde{\bm{z}}_p^T \tilde{\bm{z}}_p).
$$
Realize that this estimate can be regarded as the [final]{.orange} (additional) [step]{.orange} of the [Gram-Schmidt]{.blue} algorithm, as mentioned in Algorithm 2.1 of the textbook Azzalini \& Scarpa (2011).

#### A.5 - Recursive least squares

[Verify]{.orange} the correctness of the [recursive least square]{.blue} equations described in [this slide](../slides/un_A.html#the-recursive-least-squares-algorithm-i). 

The proof is already concisely written in the slides, you just need to convince yourself of the correctness of every step and add the [missing details]{.orange}. 

The second part of this exercise is quite [hard]{.blue} and [optional]{.blue}, because it involves a lot of algebraic steps. [Verify]{.orange} the correctness of the deviance formula:
$$
||\bm{y}_{(n + 1)} - \bm{X}_{(n+1)}\hat{\beta}_{(n+1)}||^2 = ||\bm{y}_{(n)} - \bm{X}_{(n)}\hat{\beta}_{(n)}||^2 + v_{(n)} e_{n+1}^2,
$$
which is mentioned [here](../slides/un_A.html#the-recursive-least-squares-algorithm-ii) without proof.

#### A.6 - Standardization of the input variables

I have read, on LinkedIn, a sentence about standardization, that can be loosely translated as follows:

  *"Standardization should be used when the data follow a Gaussian distribution and the algorithm presumes that the data comes from a Gaussian law, as in linear regression."*
    
Unfortunately, there are multiple mistakes within this single sentence. For instance, linear models do [not]{.orange} assume the Gaussianity of the input variables $\bm{x}_1,\dots,\bm{x}_n$. Sometimes (but not necessarily!) the errors $\epsilon_1,\dots,\epsilon_n$ are assumed to be Gaussian.

But there is an even more striking mistake. Suppose the original input values $x_{ij}$ are standardized, namely the transformed variables are such that $z_{ij}$
$$
z_{ij} = \frac{x_{ij} - \bar{x}_j}{s_j},
$$
where $\bar{x}_j$ and $s_j$ are the mean and the standard deviation of the $j$th variable. We denote with $\hat{\beta}$ the OLS estimate based on the original data and with $\hat{\gamma}$ the OLS estimate based on the standardized data. 

As an exercise, [prove]{.orange} that the predicted values coincide, that is:
$$
\hat{y}_i = \bm{x}_i^T\hat{\beta} = \bm{z}_i^T\hat{\gamma}, \qquad i=1,\dots,n.
$$
Hence, standardization of the inputs in ordinary least squares has [no effect]{.blue} on the [predictions]{.orange}. Similar linear operations, such as normalization, would also lead to the same conclusion. 

There are good reasons to standardize the variables, for instance, in ridge/lasso regression models. Moreover, centering the predictors may be helpful to get a matrix which is better conditioned... But these motivations are not related to Gaussianity nor mentioned in the post. 

This is, unfortunately, a clear example of the [button pressing](../slides/un_intro.html#press-the-button) culture. 

## Practical exercises

#### A.7 - Recursive least squares

[Implement]{.orange} a function `ols_function(X, y)` that computes the least squares estimate using the recursive least squares algorithm, described in [this slide](../slides/un_A.html#the-recursive-least-squares-algorithm-i).

A detailed description of this procedure is also described in [Algorithm 2.2]{.blue} of Azzalini \& Scarpa, 2011

#### A.8 - Iteratively re-weighted least squares for logistic regression

Implement a function called `logistic_mle(X, y)` which computes the maximum likelihood estimate for a logistic regression model using the [iteratively re-weighted least squares]{.blue}, as described [here](../slides/un_A.html#iteratively-re-weighted-least-squares-ii). 

Verify that the output of `logistic_mle` and the built-in `glm` function coincide, using the [hearth dataset](../slides/un_A.html#the-heart-dataset).
