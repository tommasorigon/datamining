---
title: "Exercises A"
subtitle: "Data mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Università degli Studi di Milano-Bicocca_"
lang: en
execute:
  cache: true
format:
  html:
    html-math-method: katex
    echo: true
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: true
    code-line-numbers: true
    smooth-scroll: true
    fig-dpi: 250
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---


# [Homepage](../index.html)

## Theoretical exercises

#### A.1 - Properties of the projection matrix $\bm{H}$

Suppose the $n\times p$ design matrix $\bm{X}$ has [full rank]{.blue}, that is $\text{rk}(\bm{X}) = p$, with $p < n$. [Prove]{.orange} that 
$$
\text{rk}(\bm{H}) = \text{tr}(\bm{H}) = p.
$$
Moreover, show that $\bm{H} = \bm{H}^T$ and that $\bm{H}^2 = \bm{H}$. Finally, show that if the [intercept]{.blue} is included into the model, then the sum of the elements of each row of $\bm{H}$ (and hence each column, because of symmetry) equals 1, that is
$$
\sum_{j=1}^n[\bm{H}]_{ij} = 1, \qquad i=1,\dots,n.
$$

[Hint]{.blue}. You may want to look for the properties of [projection matrices]{.orange} on your favorite linear algebra textbook, otherwise this exercise becomes quite hard. 


::: {.callout-warning collapse=true}
#### ✏️ - *Solution (Try it yourself before looking at it!)*
Suppose $\bm{X}$ is of full rank, that is $\text{rk}(\bm{X}) = p$. Then the following QR decomposition holds true
$$
\bm{X} = \bm{Q}\bm{R},
$$
with $\bm{Q} \in \mathbb{R}^{n \times p}$ and such that $\text{rk}(\bm{Q}) = p$ and $\bm{Q}^T\bm{Q} = I_p$. 

---------------------------------------------------------

We firstly show that $\bm{H}$ is [symmetric]{.orange}, i.e. $\bm{H}^T = \bm{H}$. In fact:
$$
\bm{H}^T = \left[\bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T\right]^T = (\bm{X}^T)^T\left[(\bm{X}^T\bm{X})^{-1}\right]^T\bm{X}^T = \bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T = \bm{H},
$$
where has been used the fact that $\left[(\bm{X}^T\bm{X})^{-1}\right]^T =(\bm{X}^T\bm{X})^{-1}$ because of symmetry. 

---------------------------------------------------------

We now offer [two alternative proofs]{.blue} to show that $\text{tr}(\bm{H}) = p$. The first proof is based on the  following property of the trace operator: $\text{tr}(\bm{A}\bm{B}) = \text{tr}(\bm{B}\bm{A})$. Hence,
$$
\text{tr}(\bm{H}) = \text{tr}(\bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T) = \text{tr}\left(\bm{X}^T\bm{X}(\bm{X}^T\bm{X})^{-1}\right) = \text{tr}(I_p) = p.
$$
[Alternatively]{.orange}, using the QR decomposition:
$$
\text{tr}(\bm{H}) = \text{tr}(\bm{Q}\bm{Q}^T) = \text{tr}(\bm{Q}^T\bm{Q}) = \text{tr}(I_p) = p.
$$

---------------------------------------------------------

The matrix $\bm{H}$ is [idempotent]{.blue}, meaning that $\bm{H}^2 = \bm{H}$. In fact,
$$
\bm{H}^2 = \bm{H}\bm{H} = \bm{X}(\bm{X}^T\bm{X})^{-1}\underbrace{\bm{X}^T \bm{X}(\bm{X}^T\bm{X})^{-1}}_{=I_p}\bm{X}^T = \bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T = \bm{H}
$$
[Alternatively]{.orange}, using the QR decomposition:
$$
\bm{H}^2 = \bm{H}\bm{H} = \bm{Q}\underbrace{\bm{Q}^T\bm{Q}}_{=I_p}\bm{Q}^T = \bm{Q}\bm{Q}^T = \bm{H}.
$$

---------------------------------------------------------

We now show that the rank of $\bm{H}$ is $p$. In order to do so, we consider the [spectral decomposition]{.blue} of $\bm{H}$. Suppose $\lambda$ is an [eigenvalue]{.orange} of the symmetric matrix $\bm{H}$, which means there is an (eigen-)vector $\bm{x} \in \mathbb{R}^n$ such that $\bm{x} \neq 0$
$$
\bm{H}\bm{x} = \lambda \bm{x}.
$$
We now exploit the fact that $\bm{H}$ is [idempotent]{.blue}:
$$
\lambda \bm{x} = \bm{H}\bm{x} = \bm{H} \underbrace{\bm{H} \bm{x}}_{=\lambda \bm{x}} = \lambda \underbrace{\bm{H} \bm{x}}_{=\lambda \bm{x}}  =  \lambda^2 \bm{x}.
$$
We have shown that $\lambda \bm{x} = \lambda^2 \bm{x}$ for $\bm{x} \neq 0$. This equality is true if and only if
$$
\lambda(\lambda - 1)\bm{x} = \bm{0}_n,
$$
which implies that the [eigevalues]{.orange} of $\bm{H}$ are [necessarily]{.orange} $\lambda \in \{0, 1\}$. Incidentally, this proves that $\bm{H}$ is positive semi-definite (not requested by the exercise). 

Let $\lambda_1,\dots,\lambda_n$ be the eigenvalues of $\bm{H}$. We proved before that $\text{tr}(\bm{H}) = p$ and note in addition that 
$$
\text{tr}(\bm{H}) = \sum_{i=1}^n \lambda_i = p.
$$
Thus, there are necessarily $p$ eigenvalues equal to $1$ and $n - p$ eigenvalues to $0$. In particular, there are $p$ positive eigenvalues, implying that $\text{rk}(\bm{H}) = p$.

---------------------------------------------------------

Finally, we show that the sums of the rows (and the columns) of $\bm{H}$ is $1$, if the intercept is included. This follows from the fact that $\bm{H}$ is [projecting]{.blue} the values of $\bm{y}$ into the space of columns of $\bm{X}$. 

Hence, if the intercept is included in the model, we obtain that
$$
\bm{H} \bm{1}_n = \bm{1}_n.
$$

:::

#### A.2 - Positive definiveness of $\bm{X}^T\bm{X}$

[Prove]{.orange} the statement of [Proposition A.1](../slides/un_A.html#cholesky-factorization). In other words, suppose the  $n\times p$  matrix $\bm{X}$ has [full rank]{.blue}, that is $\text{rk}(\bm{X}) = p$, with $p < n$. Then, show that $\bm{X}^T\bm{X}$ is [positive definite]{.blue}.

#### A.3 - OLS with orthogonal predictors

[Prove]{.orange} the statement of [Proposition A.2](../slides/un_A.html#orthogonal-predictors). In other words, show that when the predictors are orthogonal the least square estimate is $$
\hat{\beta}_j = \frac{\tilde{\bm{z}}_j^T\bm{y}}{\tilde{\bm{z}}_j^T\tilde{\bm{z}}_j}, \qquad j=1,\dots,p.
$$

Assume, in addition, that $\bm{Y} = \bm{Z}\beta + \bm{\epsilon}$ and that the errors $\epsilon_i \overset{\text{iid}}{\sim} \text{N}(0, \sigma^2)$. Then, obtain the covariance matrix of $\hat{\beta}$ and conclude that the estimators $\hat{\beta}_j$ and $\hat{\beta}_{j'}$ are independent, .

#### A.4 - Final step of the Gram-Schidmt algorithm

Consider the last equation of [this slide](../slides/un_A.html#the-qr-decomposition-and-least-squares), that is
$$
\hat{\beta}_p = (\tilde{\bm{z}}_p^T\bm{y}) / (\tilde{\bm{z}}_p^T \tilde{\bm{z}}_p).
$$
Realize that this estimate can be regarded as the [final]{.orange} (additional) [step]{.orange} of the [Gram-Schmidt]{.blue} algorithm, as mentioned in Algorithm 2.1 of the textbook Azzalini \& Scarpa (2011).

#### A.5 - Recursive least squares

[Verify]{.orange} the correctness of the [recursive least square]{.blue} equations described in [this slide](../slides/un_A.html#the-recursive-least-squares-algorithm-i). 

The proof is already concisely written in the slides, you just need to convince yourself of the correctness of every step and add the [missing details]{.orange}. 

The second part of this exercise is quite [hard]{.blue} and [optional]{.blue}, because it involves a lot of algebraic steps. [Verify]{.orange} the correctness of the deviance formula:
$$
||\bm{y}_{(n + 1)} - \bm{X}_{(n+1)}\hat{\beta}_{(n+1)}||^2 = ||\bm{y}_{(n)} - \bm{X}_{(n)}\hat{\beta}_{(n)}||^2 + v_{(n)} e_{n+1}^2,
$$
which is mentioned [here](../slides/un_A.html#the-recursive-least-squares-algorithm-ii) without proof.

#### A.6 - Separability in logistic regression

Consider a logistic regression model for binary data with a single predictor $x_i \in \mathbb{R}$, so that $f(x_i) = \beta_0 + \beta_1 x_i$ and $\mathbb{P}(Y_i = 1) = \pi(x_i) = 1/[1 + \exp\{-f(x_i)\}]$. Suppose there exists a point $x_0 \in \mathbb{R}$ that [perfectly separates]{.blue} the two binary outcomes. 

[Investigate]{.orange} the behavior of the maximum likelihood estimate $\hat{\beta}_0, \hat{\beta}$ in this scenario. Then, generalize this result to the multivariate case, when $\bm{x}_i \in \mathbb{R}^p$. That is, suppose there exists a vector $\bm{x}_0 \in \mathbb{R}^p$ that perfectly separates the binary outcomes.

A detailed description of this [separability issue]{.blue} is provided in the *Biometrika* paper by [Albert and Anderson (1984)](https://academic.oup.com/biomet/article/71/1/1/349338). See also the paper by [Rigon and Aliverti (2023)](../extra/RigonAliverti2023.pdf) for a simple correction. 

#### A.7 - Standardization of the input variables

<!-- I have read, on a popular social media, a sentence about standardization, that can be loosely translated as follows: -->

<!--   *"Standardization should be used when the data follow a Gaussian distribution and the algorithm presumes that the data comes from a Gaussian law, as in linear regression."* -->

<!-- There are multiple confusing statements. For instance, linear models do [not]{.orange} assume the Gaussianity of the input variables $x_{ij}$. Sometimes (but not necessarily!) the errors $\epsilon_1,\dots,\epsilon_n$ are assumed to be Gaussian. -->

<!-- But there is an even more striking mistake.  -->

In a [linear model]{.blue}, suppose the original input values $x_{ij}$ are standardized, namely the transformed variables $z_{ij}$ are such that 
$$
z_{ij} = \frac{x_{ij} - \bar{x}_j}{s_j},
$$
where $\bar{x}_j$ and $s_j$ are the mean and the standard deviation of the $j$th variable. We denote with $\hat{\beta}$ the OLS estimate based on the original data and with $\hat{\gamma}$ the OLS estimate based on the standardized data. 

[Prove]{.orange} that the predicted values coincide, that is:
$$
\hat{y}_i = \bm{x}_i^T\hat{\beta} = \bm{z}_i^T\hat{\gamma}, \qquad i=1,\dots,n,
$$
where $\bm{x}_i = (x_{i1},\dots,x_{ip})^T$ and $\bm{z}_i = (z_{i1},\dots,z_{ip})^T$. Hence, standardization of the inputs in ordinary least squares has [no effect]{.blue} on the [predictions]{.orange}. Similar linear operations on the inputs, such as [normalization]{.blue}, would lead to the same conclusion. 

<!-- There are good reasons to standardize the variables, for instance, in ridge/lasso regression models. Moreover, centering the predictors may be helpful to get a design matrix which is better conditioned... But these motivations are not related to Gaussianity.  -->

::: {.callout-note collapse=true}
#### Solution

To be added. The exercise has been discussed in-class.
:::

## Practical exercises

#### A.8 - Recursive least squares

[Implement]{.orange} a function `ols_function(X, y)` that computes the least squares estimate using the recursive least squares algorithm, described in [this slide](../slides/un_A.html#the-recursive-least-squares-algorithm-i).

A detailed description of this procedure is also described in [Algorithm 2.2]{.blue} of Azzalini \& Scarpa, 2011

#### A.9 - Iteratively re-weighted least squares for logistic regression

Implement a function called `logistic_mle(X, y)` which computes the maximum likelihood estimate for a logistic regression model using the [iteratively re-weighted least squares]{.blue}, as described [here](../slides/un_A.html#iteratively-re-weighted-least-squares-ii). 

Verify that the output of `logistic_mle` and the built-in `glm` function coincide, using the [hearth dataset](../slides/un_A.html#the-heart-dataset).
