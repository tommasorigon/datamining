---
title: "Exercises A"
subtitle: "Data mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_Universit√† degli Studi di Milano-Bicocca_"
lang: en
execute:
  cache: true
format:
  html:
    html-math-method: katex
    echo: true
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: true
    code-line-numbers: true
    smooth-scroll: true
    fig-dpi: 250
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

# [Homepage](../index.html)

## Theoretical exercises

#### A.1 - Properties of the projection matrix $\bm{H}$

Suppose the $n\times p$ design matrix $\bm{X}$ has [full rank]{.blue}, that is $\text{rk}(\bm{X}) = p$, with $p < n$. [Prove]{.orange} that 
$$
\text{rk}(\bm{H}) = \text{tr}(\bm{H}) = p.
$$
Moreover, show that $\bm{H}^2 = \bm{H}$. You may want to look for the properties of [projection matrices]{.orange} on your favorite linear algebra textbook, otherwise this exercise is quite hard. 

#### A.2 - Positive definiveness of $\bm{X}^T\bm{X}$

[Prove]{.orange} the statement of [Proposition A.1](../slides/un_A.html#cholesky-factorization). In other words, suppose the  $n\times p$  matrix $\bm{X}$ has [full rank]{.blue}, that is $\text{rk}(\bm{X}) = p$, with $p < n$. Then, show that $\bm{X}^T\bm{X}$ is [positive definite]{.blue}.

#### A.3 - OLS with orthogonal predictors

[Prove]{.orange} the statement of [Proposition A.2](../slides/un_A.html#orthogonal-predictors). In other words, show that when the predictors are orthogonal the least square estimate is $$
\hat{\beta}_j = \frac{\tilde{\bm{z}}_j^T\bm{y}}{\tilde{\bm{z}}_j^T\tilde{\bm{z}}_j}, \qquad j=1,\dots,p.
$$

Moreover, obtain the covariance matrix of $\hat{\beta}$ and conclude that the estimators $\hat{\beta}_j$ and $\hat{\beta}_{j'}$ are independent, assuming Gaussian errors.

#### A.4 - Final step of the Gram-Schidmt algorithm

Consider the last equation of [this slide](../slides/un_A.html#the-qr-decomposition-and-least-squares), that is
$$
\hat{\beta}_p = (\tilde{\bm{z}}_p^T\bm{y}) / (\tilde{\bm{z}}_p^T \tilde{\bm{z}}_p).
$$
Realize that this estimate can be regarded as the [final]{.orange} (additional) [step]{.orange} of the [Gram-Schmidt]{.blue} algorithm, as mentioned in Algorithm 2.1 of the textbook Azzalini \& Scarpa (2011).

#### A.5 - Recursive least squares

[Verify]{.orange} the correctness of the [recursive least square]{.blue} equations described in [this slide](../slides/un_A.html#the-recursive-least-squares-algorithm-i). You can (and should) use the Sherman-Morrison lemma. Moreover, verify the correctness for the deviance formula:
$$
||\bm{y}_{(n + 1)} - \bm{X}_{(n+1)}\hat{\beta}_{(n+1)}||^2 = ||\bm{y}_{(n + 1)} - \bm{X}_{(n)}\hat{\beta}_{(n)}||^2 + v_{(n)} e_{n+1}^2,
$$
which is mentioned [here](../slides/un_A.html#the-recursive-least-squares-algorithm-ii).


## Practical exercises

#### A.6 - Recursive least squares

[Implement]{.orange} a function `ols_function(X, y)` that computes the least squares estimate using therecursive least squares algorithm, described in [this slide](../slides/un_A.html#the-recursive-least-squares-algorithm-i).

A detailed description of this procedure is also described in [Algorithm 2.2]{.blue} of Azzalini \& Scarpa, 2011

#### A.7 - Iteratively re-weighted least squares for logistic regression

Implement a function called `logistic_mle(X, y)` which computes the maximum likelihood estimate for a logistic regression model using the [iteratively re-weighted least squares]{.blue}, as described [here](../slides/un_A.html#iteratively-re-weighted-least-squares-ii). 

Verify that the output of `logistic_mle` and the built-in `glm` function coincide, using the [hearth dataset](../slides/un_A.html#the-heart-dataset).

#### A.8 - Additional exercises

Solve the exercises 2.1 - 2.7 of the [textbook]{.orange} Azzalini \& Scarpa (2011).