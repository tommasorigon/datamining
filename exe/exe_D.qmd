---
title: "Exercises D"
subtitle: "Data mining - CdL CLAMSES"
author: "[Tommaso Rigon]{.orange}"
institute: "_UniversitÃ  degli Studi di Milano-Bicocca_"
lang: en
execute:
  cache: true
format:
  html:
    html-math-method: katex
    echo: true
    callout-appearance: minimal
    theme: [cosmo, ../template.css]
    toc: true
    toc-title: Table of contents
    embed-resources: true
    code-line-numbers: true
    smooth-scroll: true
    fig-dpi: 250
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---

# [Homepage](../index.html)

## Theoretical exercises

#### Exercise D.1 - Differentiability of Nadaraya-Watson

[Show]{.orange} that the Nadaraya-Watson estimator with fixed bandwidth $h$ and a Gaussian kernel is [differentiable]{.blue}. 

What can be said about the Epanechnikov kernel? 

#### Exercise D.2 - Local linear regression (explicit formula)

Prove the Theorem that is stated in [this slide](../slides/un_D.html#local-linear-regression-ii) about the explicit formula for [local linear regression estimator]{.blue}. 

#### Exercise D.3 - Preservation of linear trends

[Show]{.orange} that local linear regression applied to $(x_i, y_i)$ [preserves]{.orange} the [linear part of the fit]{.blue}. In other words, let us decompose $y_i = \hat{y}_{i,\text{ols}} + r_i$, where $\hat{y}_{i,\text{ols}} = \hat{\beta}_0 + \hat{\beta}_1 x_i$ represents the linear regression estimate, and $\bm{S}$ is the smoothing matrix, then
$$
\bm{S}\bm{y} = \bm{S}\hat{\bm{y}}_\text{ols} + \bm{S}\bm{r} = \hat{\bm{y}}_\text{ols} + \bm{S}\bm{r}.
$$
Another way of looking at this property is the following: if the points $(x_i, y_i)$ [belong to a line]{.blue}, then the fitted values of a local linear regression coincide with $y_i$. More formally, [show]{.orange} that if $y_i = \alpha + \beta x_i$, then
$$
\hat{f}(x) = \sum_{i=1}^ns_i(x)y_i = \alpha + \beta x.
$$

Does the same property hold for the Nadaraya-Watson estimator? 

#### Exercise D.4 - Conceptual exercise

Every nonparametric regression model involves a [smoothing parameter]{.blue}. For example, consider the parameter $\lambda$ of smoothing splines or $h$ of local linear regression. 

Can we estimate $h$ and $\lambda$ using a standard method such as the [maximum likelihood]{.orange}?

#### Exercise D.5 - Leave-one-out cross-validation

[Prove]{.orange} the Theorem of [this slide](../slides/un_D.html#choice-of-the-bandwidth-ii), about the leave-one-out cross validation for projective linear smoothers. 

Show in particular that the Theorem is correct for the: i. Nadaraya-Watson estimator, ii. local linear regression, iii. regression splines, iv. smoothing splines. 

#### Exercise D.6 - Truncated power basis

A cubic spline with one knots $\xi$ can be obtained using a basis of the form $x$, $x^2$, $x^3$ and ($x - \xi)_+$, as a consequence of the truncated power basis Theorem of [this slide](../slides/un_D.html#truncated-power-basis).

In this exercise we will show that a function 
$$
f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 (x - \xi)_+^3
$$
is a cubic spline, regardless of the values of $\beta_0,\dots,\beta_4$. 

i. Find a cubic polynomial 
$$
f_1(x) = a_1 + b_1 x + c_1 x^2 + d_1 x^3,
$$
such that $f(x) = f_1(x)$ for all $x \le \xi$. Express $a_1, b_1, c_1, d_1$ in terms of $\beta_0,\dots,\beta_4$.

ii. Find another cubic polynomial 
$$
f_2(x) = a_2 + b_2 x + c_2 x^2 + d_3 x^3,
$$
such that $f(x) = f_2(x)$ for all $x > \xi$. Express $a_2, b_2, c_2, d_2$ in terms of $\beta_0,\dots,\beta_4$. This establishes that $f(x)$ is piecewise polynomial. 

iii. Show that $f_1(\xi) = f_2(\xi)$. That is, $f(x)$ is continuous at $\xi$.

iv. Show that $f'_1(\xi) = f'_2(\xi)$. That is, the first derivative $f'(x)$ is continuous at $\xi$.

v. Show that $f''_1(\xi) = f''_2(\xi)$. That is, the second derivative $f''(x)$ is continuous at $\xi$.

Conclude that $f(x)$ [is]{.blue} indeed a [cubic spline]{.blue}. 

## Practical exercises

#### Exercise D.6 - Implementation of local linear regression

Write a function called `loclin(x, y, h)` that implements local linear regression using the formula of [this slide](../slides/un_D.html#local-linear-regression-ii). You can use any kernel function of your choice.

Compare the results of your function `loclin` on the `auto` dataset with those of the libraries `KernSmooth` and `sm`, along the lines of what has been done in class. 
 
#### Exercise D.7 - Implementation of leave-one-out cross-validation

Write a function called `loo_cv(X, y, h)` that computes the leave-one-out cross-validation for the `loclin(x, y, h)` function, using the result of [this slide](../slides/un_D.html#choice-of-the-bandwidth-ii).

Identify the optimal bandwidth for the `loclin` local linear regression, using the `auto` dataset.
